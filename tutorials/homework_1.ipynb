{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "- Last name: \n",
    "- First Name:\n",
    "\n",
    "Homework must be sent to `pwinant@escp.eu` by email before May 5th.\n",
    "\n",
    "You are welcome to install any library, take any initiative you deem appropriate and encourage to help each other (but not copy/paste code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eda1ec-a7b7-4d72-843a-9fb06a875ac8",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1: Linear Regression\n",
    "\n",
    "1. __For $N=100$, compute a sample $(x_i, y_i)_{i=[1:N]}$ satisfying $$y_i=0.4+2.5 x_i + \\epsilon_i$$ where $x_i$ uniformly distributed between 0 and 1 and $\\epsilon_i$ is drawn from a normal distribution with standard deviation 0.5.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb895840-2022-4f0d-a40c-028e67def4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54233088-7390-495a-ab6a-0395d38f2bd9",
   "metadata": {},
   "source": [
    "2. __Define the model $f(x;a,b)=a+b x$. Find the parameters $a$ and $b$ which minimize the objective $\\xi(a,b)=\\sum_i (f(x_i;a,b)-y_i)^2$ by using a numerical optimization algorithm (not the formula for the regression). Plot.__\n",
    "\n",
    "<mark>Hint</mark>: you can write your own gradient descent algorithm or use an optimization library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d42c0-d3dd-4cb3-965e-4bff806284c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: constrained optimization\n",
    "\n",
    "Consider the function $f(x,y) = 1-(x-0.5)^2 -(y-0.3)^2$.\n",
    "\n",
    "__Use Optim.jl to minimize $f$ without constraint. Check you understand diagnostic information returned by the optimizer.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now, consider the constraint $x<0.3$ and maximize $f$ under this new constraint.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reformulate the problem as a root finding with lagrangians. Write the complementarity conditions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solve using NLSolve.jl__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-coordinate",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 3: Learning simple decision rules in the Consumption-Savings model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-thursday",
   "metadata": {},
   "source": [
    "This exercise is inspired from *Individual learning about consumption* by Todd Allen and Chris Carroll [link](https://www.econstor.eu/bitstream/10419/72016/1/328292125.pdf).\n",
    "\n",
    "\n",
    "We consider the following consumption saving problem. An agent receives random income $y_t = \\exp(\\epsilon_t)$ where $\\epsilon_t\\sim \\mathcal{N}(\\sigma)$ ($\\sigma$ is the standard deviation.)\n",
    "\n",
    "Consumer starts the period with available income $w_t$. The law of motion for available income is:\n",
    "\n",
    "$$w_t = \\exp(\\epsilon_t) + (w_{t-1}-c_{t-1}) r$$\n",
    "\n",
    "where consumption $c_t \\in ]0,w_t]$ is chosen in each period in order to maximize:\n",
    "\n",
    "$$E_t \\sum_{t=0}^T \\beta^t U(c_t)$$\n",
    "\n",
    "given initial available income $w_0$.\n",
    "\n",
    "In the questions below, we will use the following calibration:\n",
    "- $\\beta = 0.9$\n",
    "- $\\sigma = 0.1$\n",
    "- $T=100$\n",
    "- $U(x) = \\frac{x^{1-\\gamma}}{1-\\gamma}$ with $\\gamma=2$\n",
    "- $w_0 = 1.1$ (alternatively, consider values 0.5 and 1)\n",
    "\n",
    "The theoretical solution to this problem is a concave function $\\varphi$ such that $\\varphi(x)\\in ]0,x]$ and $\\forall t,  c_t=\\varphi(w_t)$. Qualitatively, agents accumulate savings, up to a certain point (a buffer stock), beyond which wealth is not increasing any more (in expectation).\n",
    "\n",
    "Carroll and Allen have noticed that the true solution can be approximated very well by a simple rule:\n",
    "\n",
    "$\\psi(x) = \\min(x, \\theta_0 + \\theta_1 (x - \\theta_0) )$\n",
    "\n",
    "The main question they ask in the aforementioned paper is whether it is realistic that agents would learn good values of $\\theta_0$ and $\\theta_1$ by observing past experiences.\n",
    "\n",
    "We would like to examine this result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-bikini",
   "metadata": {},
   "source": [
    "### Lifetime reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-stereo",
   "metadata": {},
   "source": [
    "__Define a NamedTuple to hold the parameter values__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "supported-practitioner",
   "metadata": {},
   "source": [
    "__Define simple rule fonction `consumption(w::Number, Î¸_0::Number, Î¸_1::Number, p::NamedTuple)`\n",
    "which compute consumption using a simple rule. What is the meaning of $\\theta_0$ and $\\theta_1$? Make a plot in the space $w,c$, including consumption rule and the line where $w_{t+1} = w_t$.__\n",
    "\n",
    "(remark for later: `Number` type is compatible with ForwardDiff.jl ðŸ˜‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-cover",
   "metadata": {},
   "source": [
    "__Write a function `lifetime_reward(w_0::Number, Î¸_0::Number, Î¸_1::Number, p::NamedTuple)` which computes one realization of $\\sum \\beta^t U(c_t)$ for initial wealth `w_0` and simple rule `Î¸_0`, `Î¸_1`. Mathematically, we denote it by $\\xi(\\omega; \\theta_0, \\theta_1)$, where $\\omega$ represents the succession of random income draws.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-needle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "convertible-blink",
   "metadata": {},
   "source": [
    "__Write a function `expected_lifetime_reward(w_0::Number, Î¸_0::Number, Î¸_1::Number,  p::NamedTuple; N=1000)` which computes expected lifetime reward using `N` Monte-Carlo draws. Mathematically, we write it $\\Xi^{N}(\\theta_0, \\theta_1) =\\frac{1}{N} \\sum_1^N {\\xi(\\omega_N; \\theta_0, \\theta_1)}$. Check empirically that standard deviation of these draws decrease proportionally to $\\frac{1}{\\sqrt{N}}$ .__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-chicago",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "frank-armenia",
   "metadata": {},
   "source": [
    "__Using a high enough number for `N`, compute optimal values for $\\theta_0$ and $\\theta_1$. What is the matching value for the objective function converted into an equivalent stream of determinstic consumption ? That is if `V` is the approximated value computed above, what is $\\bar{c}\\in \\R$ such that $ V= \\sum_{t=0}^T \\beta^t U(\\bar{c})$ ?__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-representative",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "great-nickel",
   "metadata": {},
   "source": [
    "__Using a high enough number for `N`, make contour plots of lifetime rewards as a function of `Î¸_0` and `Î¸_1`. Ideally, represent lines with $1\\%$ consumption loss, $5\\%$ and $10\\%$ deterministic consumption loss w.r.t. to maximum.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-hawaii",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "personalized-louisiana",
   "metadata": {},
   "source": [
    "### Learning to save\n",
    "\n",
    "__We now focus on the number of steps it takes to optimize $\\theta_0$, $\\theta_1$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-director",
   "metadata": {},
   "source": [
    "__Implement a function `âˆ‡(Î¸::Vector; N=1000)::Vector` which computes the gradient of the objective w.r.t. `Î¸==[Î¸_0,Î¸_1]`. (You need to use automatic differentiation, otherwise you might get incorrect results).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-country",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prescription-friday",
   "metadata": {},
   "source": [
    "__Implement a gradient descent algorithm to maximize $\\Xi^N(\\theta_0, \\theta_1)$ using learning rate $\\lambda \\in ]0,1]$. Stop after a predefined number of iterations. Compare convergence speed for different values of $\\lambda$ and plot them on the $\\theta_0, \\theta_1$ plan. How many steps does it take to enter the `1%` error zone? The `5%` and the `10%` error zone?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-locking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prime-exhibit",
   "metadata": {},
   "source": [
    "__Even for big N, the evaluated value of âˆ‡ are stochastic, and always slightly inaccurate. In average, they are non-biased and the algorithm converges in expectation (it fluctuates around the maximum). This is called the stochastic gradient method.__\n",
    "\n",
    "__What are the values of $N$ and $\\lambda$ which minimize the number of iterations before reaching the target zones (at 1%, 2%, etc...)? How many simulations periods does it correspond to? Would you say it is realistic that consumers learn from their own experience?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-blair",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
