[
  {
    "objectID": "new.html",
    "href": "new.html",
    "title": "Introduction to Computational Economics",
    "section": "",
    "text": "Table SAM(u,v) social accounting matrix\n\n\n\nSAM\nBRD\nMLK\nCAP\nLAB\nHOH\n\n\n\n\nBRD\n-\n-\n-\n-\n15\n\n\nMLK\n\n-\n-\n-\n35\n\n\nCAP\n5\n-\n-\n-\n20\n\n\nLAB\n10\n15\n-\n-\n-\n\n\nHOH\n-\n-\n25\n25\n-\n\n\n\n\n\n\nSAM\nBRD\nMLK\nCAP\nLAB\nHOH\n\n\n\n\nJanuary\n$250\n\n\n\n15\n\n\nJanuary\n$250\n\n\n\n15\n\n\nJanuary\n$250\n\n\n\n15"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Economics - ECO309",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nMar 20, 2025\n\n\nIntroduction\n\n\n\n\nMar 27, 2025\n\n\nConvergence of Sequences\n\n\n\n\nApr 3, 2025\n\n\nOptimization\n\n\n\n\nApr 10, 2025\n\n\nPerturbation\n\n\n\n\nApr 17, 2025\n\n\nDiscrete Dynamic Programming\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#indicative-schedule",
    "href": "index.html#indicative-schedule",
    "title": "Computational Economics - ECO309",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nMar 20, 2025\n\n\nIntroduction\n\n\n\n\nMar 27, 2025\n\n\nConvergence of Sequences\n\n\n\n\nApr 3, 2025\n\n\nOptimization\n\n\n\n\nApr 10, 2025\n\n\nPerturbation\n\n\n\n\nApr 17, 2025\n\n\nDiscrete Dynamic Programming\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#communication",
    "href": "index.html#communication",
    "title": "Computational Economics - ECO309",
    "section": "Communication",
    "text": "Communication\n\nZulip (best)\nEmail to pablo.winant@polytechnique.edu or pwinant@escp.eu\n\nsubject starting with [eco309]\n\nGithub issues (PR also welcome)"
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "Computational Economics - ECO309",
    "section": "Assessment",
    "text": "Assessment\n\n\n\n\n\n\nMandatory Coursework\n\n\n\nChoose one of the two coursework subject and return your work by June 22.\n\n\n\npushups (optional)\ncoursework (mandatory)\npresentations (optional)"
  },
  {
    "objectID": "slides/session_optimization/index.html#introduction-1",
    "href": "slides/session_optimization/index.html#introduction-1",
    "title": "Optimization",
    "section": "Introduction",
    "text": "Introduction\nOptimization is everywhere in economics:\n\nto model agent’s behaviour: what would a rational agent do?\n\nconsumer maximizes utility from consumption\nfirm maximizes profit\n\nan economist tries to solve a model:\n\nfind prices that clear the market",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#two-kinds-of-optimization-problem",
    "href": "slides/session_optimization/index.html#two-kinds-of-optimization-problem",
    "title": "Optimization",
    "section": "Two kinds of optimization problem:",
    "text": "Two kinds of optimization problem:\n\nroot finding: \\(\\text{find  $x$ in $X$ such that $f(x)=0$}\\)\nminimization/maximization \\(\\min_{x\\in X} f(x)\\) or \\(\\max_{x\\in X} f(x)\\)\noften a minimization problem can be reformulated as a root-finding problem\n\\[x_0 = {argmin}_{x\\in X} f(x) \\overbrace{\\iff}^{??} f^{\\prime} (x_0) = 0\\]",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#plan",
    "href": "slides/session_optimization/index.html#plan",
    "title": "Optimization",
    "section": "Plan",
    "text": "Plan\n\ngeneral consideration about optimization problems\none-dimensional root-finding\none-dimensional optimization\nlocal root-finding\nlocal optimization\nconstrained optimization\nconstrained root-finding",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#optimization-tasks-come-in-many-flavours",
    "href": "slides/session_optimization/index.html#optimization-tasks-come-in-many-flavours",
    "title": "Optimization",
    "section": "Optimization tasks come in many flavours",
    "text": "Optimization tasks come in many flavours\n\ncontinuous versus discrete optimization\nconstrained and unconstrained optimization\nglobal and local\nstochastic and deterministic optimization\nconvexity",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#continuous-versus-discrete-optimization",
    "href": "slides/session_optimization/index.html#continuous-versus-discrete-optimization",
    "title": "Optimization",
    "section": "Continuous versus discrete optimization",
    "text": "Continuous versus discrete optimization\n\nChoice is picked from a given set (\\(x\\in X\\)) which can be:\n\ncontinuous: choose amount of debt \\(b_t \\in [0,\\overline{b}]\\), of capital \\(k_t \\in R^{+}\\)\ndiscrete: choose whether to repay or default \\(\\delta\\in{0,1}\\), how many machines to buy (\\(\\in N\\)), at which age to retire…\na combination of both: mixed integer programming",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#continuous-versus-discrete-optimization-2",
    "href": "slides/session_optimization/index.html#continuous-versus-discrete-optimization-2",
    "title": "Optimization",
    "section": "Continuous versus discrete optimization (2)",
    "text": "Continuous versus discrete optimization (2)\n\nDiscrete optimization requires a lot of combinatorial thinking\n\nWe don’t cover it today.\n…if needed, we just test all choices until we find the best one\n\nSometimes a discrete choice can be approximated by a mixed strategy (i.e. a random strategy).\n\nInstead of \\(\\delta\\in{0,1}\\) we choose \\(x\\) in \\(prob(\\delta=1)=\\sigma(x)\\)\nwith \\(\\sigma(x)=\\frac{2}{1+\\exp(-x)}\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#constrained-and-unconstrained-optimization",
    "href": "slides/session_optimization/index.html#constrained-and-unconstrained-optimization",
    "title": "Optimization",
    "section": "Constrained and Unconstrained optimization",
    "text": "Constrained and Unconstrained optimization\n\nUnconstrained optimization: \\(x\\in R\\)\nConstrained optimization: \\(x\\in X\\)\n\nbudget set: \\(p_1 c_1 + p_2 c_2 \\leq I\\)\npositivity of consumption: \\(c \\geq 0\\).\n\nIn good cases, the optimization set is convex…\n\npretty much always in this course",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#stochastic-vs-deterministic",
    "href": "slides/session_optimization/index.html#stochastic-vs-deterministic",
    "title": "Optimization",
    "section": "Stochastic vs Deterministic",
    "text": "Stochastic vs Deterministic\n\nCommon case, especially in machine learning \\[f(x) = E_{\\epsilon}[ \\xi (\\epsilon, x)]\\]\nOne wants to maximize (resp solve) w.r.t. \\(x\\) but it is costly to compute expectation precisely using Monte-Carlo draws (there are other methods).\nA stochastic optimization method allows to use noisy estimates of the expectation, and will still converge in expectation.\nFor now we focus on deterministic methods. Maybe later…",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#local-vs-global-algorithms",
    "href": "slides/session_optimization/index.html#local-vs-global-algorithms",
    "title": "Optimization",
    "section": "Local vs global Algorithms",
    "text": "Local vs global Algorithms\n\nIn principle, there can be many roots (resp maxima) within the optimization set.\nAlgorithms that find them all are called “global”. For instance:\n\ngrid search\nsimulated annealing\n\nWe will deal only with local algorithms, and consider local convergence properties.\n\n-&gt;then it might work or not\nto perform global optimization just restart from different points.",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#math-vs-practice",
    "href": "slides/session_optimization/index.html#math-vs-practice",
    "title": "Optimization",
    "section": "Math vs practice",
    "text": "Math vs practice\n\nThe full mathematical treatment will typically assume that \\(f\\) is smooth (\\(\\mathcal{C}_1\\) or \\(\\mathcal{C}_2\\) depending on the algorithm).\nIn practice we often don’t know about these properties\n\nwe still try and check thqt we have a local optimal\n\nSo: fingers crossed",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#math-vs-practice-1",
    "href": "slides/session_optimization/index.html#math-vs-practice-1",
    "title": "Optimization",
    "section": "Math vs practice",
    "text": "Math vs practice\nHere is the surface representing the objective that a deep neural network training algorithm tries to minimize.\n\nAnd yet, neural networks do great things!",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#what-do-you-need-to-know",
    "href": "slides/session_optimization/index.html#what-do-you-need-to-know",
    "title": "Optimization",
    "section": "What do you need to know?",
    "text": "What do you need to know?\n\nbe able to handcode simple algos (Newton, Gradient Descent)\nunderstand the general principle of the various algorithms to compare them in terms of\n\nrobustness\nefficiency\naccuracy\n\nthen you can just switch the various options, when you use a library…",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#bisection",
    "href": "slides/session_optimization/index.html#bisection",
    "title": "Optimization",
    "section": "Bisection",
    "text": "Bisection\n\nFind \\(x \\in [a,b]\\) such that \\(f(x) = 0\\). Assume \\(f(a)f(b) &lt;0\\).\nAlgorithm\n\nStart with \\(a_n, b_n\\). Set \\(c_n=(a_n+b_n)/2\\)\nCompute \\(f(c_n)\\)\n\n\nif \\(f(c_n)f(a_n)&lt;0\\) then set \\((a_{n+1},b_{n+1})=(a_n,c_n)\\)\nelse set \\((a_{n+1},b_{n+1})=(c_n,b_n)\\)\n\n\nIf \\(|f(c_n)|&lt;\\epsilon\\) and/or \\(\\frac{b-a}{2^n}&lt;\\delta\\) stop. Otherwise go back to 1.",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#bisection-2",
    "href": "slides/session_optimization/index.html#bisection-2",
    "title": "Optimization",
    "section": "Bisection (2)",
    "text": "Bisection (2)\n\nNo need for initial guess: globally convergent algorithm\n\nnot a global algorithm…\n… in the sense that it doesn’t find all solutions\n\n\\(\\delta\\) is a guaranteed accuracy on \\(x\\)\n\\(\\epsilon\\) is a measure of how good the solution is\nthink about your tradeoff: (\\(\\delta\\) or \\(\\epsilon\\) ?)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#newton-algorithm",
    "href": "slides/session_optimization/index.html#newton-algorithm",
    "title": "Optimization",
    "section": "Newton algorithm",
    "text": "Newton algorithm\n\nFind \\(x\\) such that \\(f(x) = 0\\). Use \\(x_0\\) as initial guess.\n\\(f\\) must be \\(\\mathcal{C_1}\\) and we assume we can compute its derivative \\(f^{\\prime}\\)\nGeneral idea:\n\nobserve that the zero \\(x^{\\star}\\) must satisfy \\[f(x^{\\star})=0=f(x_0)+f^{\\prime}(x_0)(x^{\\star}-x_0) + o(x-x_0)\\]\nHence a good approximation should be \\[x^{\\star}\\approx = x_0- f(x_0)/f^{\\prime}(x_0)\\]\nCheck it is good. otherwise, replace \\(x_0\\) by \\(x^{\\star}\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#newton-algorithm-2",
    "href": "slides/session_optimization/index.html#newton-algorithm-2",
    "title": "Optimization",
    "section": "Newton algorithm (2)",
    "text": "Newton algorithm (2)\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n- \\frac{f(x_n)}{f^{\\prime}(x_n)}=f^{\\text{newton}}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#quasi-newton",
    "href": "slides/session_optimization/index.html#quasi-newton",
    "title": "Optimization",
    "section": "Quasi-Newton",
    "text": "Quasi-Newton\n\nWhat if we can’t compute \\(f^{\\prime}\\) or it is expensive to do so?\n\nIdea: try to approximate \\(f^{\\prime}(x_n)\\) from the last iterates\n\nSecant method: \\[f^{\\prime}(x_n)\\approx \\frac{f(x_n)-f(x_{n-1})}{x_n-x_{n-1}}\\] \\[x_{n+1} = x_n- f(x_n)\\frac{x_n-x_{n-1}}{f(x_n)-f(x_{n-1})}\\]\n\nrequires two initial guesses: \\(x_1\\) and \\(x_0\\)\nsuperlinear convergence: \\(\\lim \\frac{x_t-x^{\\star}}{x_{t-1}-x^{\\star}}\\rightarrow 0\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#limits-of-newtons-method",
    "href": "slides/session_optimization/index.html#limits-of-newtons-method",
    "title": "Optimization",
    "section": "Limits of Newton’s method",
    "text": "Limits of Newton’s method\n\nHow could Newton method fail?\n\nbad guess\n\n-&gt; start with a better guess\n\novershoot\n\n-&gt; dampen the update (problem: much slower)\n-&gt; backtrack\n\nstationary point\n\n-&gt; if root of multiplicity \\(m\\) try \\(x_{n+1} = x_n- m \\frac{f(x_n)}{f^{\\prime}(x_n)}\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#backtracking",
    "href": "slides/session_optimization/index.html#backtracking",
    "title": "Optimization",
    "section": "Backtracking",
    "text": "Backtracking\n\nSimple idea:\n\nat stage \\(n\\) given \\(f(x_n)\\) compute Newton step \\(\\Delta_n=-\\frac{f(x_n)}{f^{\\prime}(x_n)}\\)\nfind the smallest \\(k\\) such that \\(|f(x_n-\\Delta/2^k)|&lt;|f(x_n)|\\)\nset \\(x_{n+1}=x_n-\\Delta/2^k\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#golden-section-search",
    "href": "slides/session_optimization/index.html#golden-section-search",
    "title": "Optimization",
    "section": "Golden section search",
    "text": "Golden section search\n\nMinimize \\(f(x)\\) for \\(x \\in [a,b]\\)\nChoose \\(\\Phi \\in [0,0.5]\\)\nAlgorithm:\n\nstart with \\(a_n &lt; b_n\\) (initially equal to \\(a\\) and \\(b\\))\ndefine \\(c_n = a_n+\\Phi(b_n-a_n)\\) and \\(d_n = a_n+(1-\\Phi)(b_n-a_n)\\)\n\nif \\(f(c_n)&lt;f(d_n)\\) set \\(a_{n+1},b_{n+1}=a_n, d_n\\)\nelse set \\(a_{n+1}, b_{n+1}= c_n, b_n\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#golden-section-search-2",
    "href": "slides/session_optimization/index.html#golden-section-search-2",
    "title": "Optimization",
    "section": "Golden section search (2)",
    "text": "Golden section search (2)\n\nThis is guaranteed to converge to a local minimum\nIn each step, the size of the interval is reduced by a factor \\(\\Phi\\)\nBy choosing \\(\\Phi=\\frac{\\sqrt{5}-1}{2}\\) one can save one evaluation by iteration.\n\nyou can check that either \\(c_{n+1} = d_n\\) or \\(d_{n+1} = c_n\\)\n\nRemark that bisection is not enough",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#gradient-descent",
    "href": "slides/session_optimization/index.html#gradient-descent",
    "title": "Optimization",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nMinimize \\(f(x)\\) for \\(x \\in R\\) given initial guess \\(x_0\\)\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n (1-\\lambda)- \\lambda f^{\\prime}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f^{\\prime}(x_n)| &lt; \\epsilon\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#gradient-descent-2",
    "href": "slides/session_optimization/index.html#gradient-descent-2",
    "title": "Optimization",
    "section": "Gradient Descent (2)",
    "text": "Gradient Descent (2)\n\nUses local information\n\none needs to compute the gradient\nnote that gradient at \\(x_n\\) does not provide a better guess for the minimum than \\(x_n\\) itself\nlearning speed is crucial\n\nConvergence speed: linear\n\nrate depend on the learning speed\noptimal learning speed? the fastest for which there is convergence",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#newton-raphson-method",
    "href": "slides/session_optimization/index.html#newton-raphson-method",
    "title": "Optimization",
    "section": "Newton-Raphson method",
    "text": "Newton-Raphson method\n\nMinimize \\(f(x)\\) for \\(x \\in R\\) given initial guess \\(x_0\\)\nBuild a local model of \\(f\\) around \\(x_0\\) \\[f(x) = f(x_0) + f^{\\prime}(x_0)(x-x_0) + f^{\\prime\\prime}(x_0)\\frac{(x-x_0)^2}{2} + o(x-x_0)^2\\]\nAccording to this model, \\[ f(x{\\star}) = min_x f(x)\\iff \\frac{d}{d x} \\left[ f(x_0) + f^{\\prime}(x_0)(x-x_0) + f^{\\prime\\prime}(x_0)\\frac{(x-x_0)^2}{2} \\right] = 0\\] which yields: \\(x^{\\star} = x_0 - \\frac{f^{\\prime}(x_0)}{f^{\\prime\\prime}(x_0)}\\)\nthis is Newton applied to \\(f^{\\prime}(x)=0\\)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#newton-raphson-algorithm-2",
    "href": "slides/session_optimization/index.html#newton-raphson-algorithm-2",
    "title": "Optimization",
    "section": "Newton-Raphson Algorithm (2)",
    "text": "Newton-Raphson Algorithm (2)\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n-\\frac{f^{\\prime}(x_0)}{f^{\\prime\\prime}(x_0)}\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f^{\\prime}(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#unconstrained-problems",
    "href": "slides/session_optimization/index.html#unconstrained-problems",
    "title": "Optimization",
    "section": "Unconstrained problems",
    "text": "Unconstrained problems\n\nMinimize \\(f(x)\\) for \\(x \\in R^n\\) given initial guess \\(x_0 \\in R^n\\)\nMany intuitions from the 1d case, still apply\n\nreplace derivatives by gradient, jacobian and hessian\nrecall that matrix multiplication is not commutative\n\nSome specific problems:\n\nupdate speed can be specific to each dimension\nsaddle-point issues (for minimization)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#quick-terminology",
    "href": "slides/session_optimization/index.html#quick-terminology",
    "title": "Optimization",
    "section": "Quick terminology",
    "text": "Quick terminology\nFunction \\(f: R^p \\rightarrow R^q\\)\n\nJacobian: \\(J(x)\\) or \\(f^{\\prime}_x(x)\\), \\(p\\times q\\) matrix such that: \\[J(x)_{ij} = \\frac{\\partial f(x)_i}{\\partial x_j}\\]\nGradient: \\(\\nabla f(x) = J(x)\\), gradient when \\(q=1\\)\nHessian: denoted by \\(H(x)\\) or \\(f^{\\prime\\prime}_{xx}(x)\\) when \\(q=1\\): \\[H(x)_{jk} = \\frac{\\partial f(x)}{\\partial x_j\\partial x_k}\\]\nIn the following explanations, \\(|x|\\) denotes the supremum norm, but most of the following explanations also work with other norms.",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#multidimensional-newton-raphson",
    "href": "slides/session_optimization/index.html#multidimensional-newton-raphson",
    "title": "Optimization",
    "section": "Multidimensional Newton-Raphson",
    "text": "Multidimensional Newton-Raphson\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n- J(x_{n})^{-1}f(x_n)=f^{\\text{newton}}(x_n)\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#multidimensional-newton-root-finding-2",
    "href": "slides/session_optimization/index.html#multidimensional-newton-root-finding-2",
    "title": "Optimization",
    "section": "Multidimensional Newton root-finding (2)",
    "text": "Multidimensional Newton root-finding (2)\n\nwhat matters is the computation of the step \\(\\Delta_n = {\\color{\\red}{J(x_{n})^{-1}}} f(x_n)\\)\ndon’t compute \\(J(x_n)^{-1}\\)\n\nit takes less operations to compute \\(X\\) in \\(AX=Y\\) than \\(A^{-1}\\) then \\(A^{-1}Y\\)\nin Julia: X = A \\ Y\n\nstrategies to improve convergence:\n\ndampening: \\(x_n = (1-\\lambda)x_{n-1} - \\lambda \\Delta_n\\)\nbacktracking: choose \\(k\\) such that \\(|f(x_n-2^{-k}\\Delta_n)|\\)&lt;\\(|f(x_{n-1})|\\)\nlinesearch: choose \\(\\lambda\\in[0,1]\\) so that \\(|f(x_n-\\lambda\\Delta_n)|\\) is minimal",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#multidimensional-gradient-descent",
    "href": "slides/session_optimization/index.html#multidimensional-gradient-descent",
    "title": "Optimization",
    "section": "Multidimensional Gradient Descent",
    "text": "Multidimensional Gradient Descent\n\nMinimize \\(f(x) \\in R\\) for \\(x \\in R^n\\) given \\(x_0 \\in R^n\\)\nAlgorithm\n\nstart with \\(x_n\\) \\[x_{n+1} = (1-\\lambda) x_n - \\lambda \\nabla f(x_n)\\]\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nComments:\n\nlots of variants\nautomatic differentiation software makes gradient easy to compute\nconvergence is typically linear",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#gradient-descent-variants",
    "href": "slides/session_optimization/index.html#gradient-descent-variants",
    "title": "Optimization",
    "section": "Gradient descent variants",
    "text": "Gradient descent variants",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#multidimensional-newton-minimization",
    "href": "slides/session_optimization/index.html#multidimensional-newton-minimization",
    "title": "Optimization",
    "section": "Multidimensional Newton Minimization",
    "text": "Multidimensional Newton Minimization\n\nAlgorithm:\n\nstart with \\(x_n\\)\ncompute \\(x_{n+1} = x_n-{\\color{\\red}{H(x_{n})^{-1}}}\\color{\\green}{ J(x_n)'}\\)\nstop if \\(|x_{n+1}-x_n|&lt;\\eta\\) or \\(|f(x_n)| &lt; \\epsilon\\)\n\nConvergence: quadratic\nProblem:\n\n\\(H(x_{n})\\) hard to compute efficiently\nrather unstable",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#quasi-newton-method-for-multidimensional-minimization",
    "href": "slides/session_optimization/index.html#quasi-newton-method-for-multidimensional-minimization",
    "title": "Optimization",
    "section": "Quasi-Newton method for multidimensional minimization",
    "text": "Quasi-Newton method for multidimensional minimization\n\nRecall the secant method:\n\n\\(f(x_{n-1})\\) and \\(f(x_{n-2})\\) are used to approximate \\(f^{\\prime}(x_{n-2})\\).\nIntuitively, \\(n\\) iterates would be needed to approximate a hessian of size \\(n\\)….\n\nBroyden method: takes \\(2 n\\) steps to solve a linear problem of size \\(n\\)\n\nuses past information incrementally",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#quasi-newton-method-for-multidimensional-minimization-1",
    "href": "slides/session_optimization/index.html#quasi-newton-method-for-multidimensional-minimization-1",
    "title": "Optimization",
    "section": "Quasi-Newton method for multidimensional minimization",
    "text": "Quasi-Newton method for multidimensional minimization\n\nConsider the approximation: \\[f(x_n)-f(x_{n-1}) \\approx J(x_n) (x_n - x_{n-1})\\]\n\n\\(J(x_n)\\) is unknown and cannot be determined directly as in the secant method.\nidea: \\(J(x_n)\\) as close as possible to \\(J(x_{n-1})\\) while solving the secant equation\nformula: \\[J_n = J_{n-1} + \\frac{(f(x_n)-f(x_{n-1})) - J_{n-1}(x_n-x_{n-1})}{||x_n-x_{n-1}||^2}(x_n-x_{n-1})^{\\prime}\\]",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#gauss-newton-minimization",
    "href": "slides/session_optimization/index.html#gauss-newton-minimization",
    "title": "Optimization",
    "section": "Gauss-Newton Minimization",
    "text": "Gauss-Newton Minimization\n\nRestrict to least-square minimization: $min_x _i f(x)_i^2 R $\nThen up to first order, \\(H(x_n)\\approx J(x_n)^{\\prime}J(x_n)\\)\nUse the step: \\(({J(x_n)^{\\prime}J(x_n)})^{-1}\\color{\\green}{ J(x_n)}\\)\nConvergence:\n\ncan be quadratic at best\nlinear in general",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#levenberg-marquardt",
    "href": "slides/session_optimization/index.html#levenberg-marquardt",
    "title": "Optimization",
    "section": "Levenberg-Marquardt",
    "text": "Levenberg-Marquardt\n\nLeast-square minimization: $min_x _i f(x)_i^2 R $\nreplace \\({J(x_n)^{\\prime}J(x_n)}^{-1}\\) by \\({J(x_n)^{\\prime}J(x_n)}^{-1} +\\mu I\\)\n\nadjust \\(\\lambda\\) depending on progress\n\nuses only gradient information like Gauss-Newton\nequivalent to Gauss-Newton close to the solution (\\(\\mu\\) small)\nequivalent to Gradient far from solution (\\(\\mu\\) high)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#consumption-optimization",
    "href": "slides/session_optimization/index.html#consumption-optimization",
    "title": "Optimization",
    "section": "Consumption optimization",
    "text": "Consumption optimization\nConsider the optimization problem: \\[\\max U(x_1, x_2)\\]\nunder the constraint \\(p_1 x_1 + p_2 x_2 \\leq B\\)\nwhere \\(U(.)\\), \\(p_1\\), \\(p_2\\) and \\(B\\) are given.\nHow do you find a solution by hand?",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#consumption-optimization-1",
    "href": "slides/session_optimization/index.html#consumption-optimization-1",
    "title": "Optimization",
    "section": "Consumption optimization (1)",
    "text": "Consumption optimization (1)\n\nCompute by hand\nEasy:\n\nsince the budget constraint must be binding, get rid of it by stating \\(x_2 = B - p_1 x_1\\)\nthen maximize in \\(x_1\\), \\(U(x_1, B - p_1 x_1)\\) using the first order conditions.\n\nIt works but:\n\nbreaks symmetry between the two goods\nwhat if there are other constraints: \\(x_1\\geq \\underline{x}\\)?\nwhat if constraints are not binding?\nis there a better way to solve this problem?",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#consumption-optimization-2",
    "href": "slides/session_optimization/index.html#consumption-optimization-2",
    "title": "Optimization",
    "section": "Consumption optimization (2)",
    "text": "Consumption optimization (2)\n\nAnother method, which keeps the symmetry. Constraint is binding, trying to minimize along the budget line yields an implicit relation between \\(d x_1\\) and \\(d x_2\\) \\[p_1 d {x_1} + p_2 d {x_2} = 0\\]\nAt the optimal: \\(U^{\\prime}_{x_1}(x_1, x_2)d {x_1} + U^{\\prime}_{x_2}(x_1, x_2)d {x_2} = 0\\)\nEliminate \\(d {x_1}\\) and \\(d {x_2}\\) to get one condition which characterizes optimal choices for all possible budgets. Combine with the budget constraint to get a second condition.",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#penalty-function",
    "href": "slides/session_optimization/index.html#penalty-function",
    "title": "Optimization",
    "section": "Penalty function",
    "text": "Penalty function\n\nTake a penalty function \\(p(x)\\) such that \\(p(x)=K&gt;0\\) if \\(x&gt;0\\) and \\(p(x)=0\\) if \\(x \\leq 0\\). Maximize: \\(V(x_1, x_2) = U(x_1, x_2) - p( p_1 x_1 + p_2 x_2 - B)\\)\nClearly, \\(\\min U \\iff \\min V\\)\nProblem: \\(\\nabla V\\) is always equal to \\(\\nabla U\\).\nSolution: use a smooth solution function like \\(p(x) = x^2\\)\nProblem: distorts optimization\n\nSolution: adjust weight of barrier and minimize \\(U(x_1, x_2) - \\kappa p(x)\\)\n\nPossible but hard to choose the weights/constraints.",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#penalty-function-1",
    "href": "slides/session_optimization/index.html#penalty-function-1",
    "title": "Optimization",
    "section": "Penalty function",
    "text": "Penalty function\n\nAnother idea: is there a canonical way to choose \\(\\lambda\\) such that at the minimum it is equivalent to minimize the original problem under constraint or to minimize \\[V(x_1, x_2) = U(x_1, x_2) - \\lambda (p_1 x_1 + p_2 x_2 - B)\\]\nClearly, when the constraint is not binding we must have \\(\\lambda=0\\). What should be the value of \\(\\lambda\\) when the constraint is binding ?",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#karush-kuhn-tucker-conditions",
    "href": "slides/session_optimization/index.html#karush-kuhn-tucker-conditions",
    "title": "Optimization",
    "section": "Karush-Kuhn-Tucker conditions",
    "text": "Karush-Kuhn-Tucker conditions\n\nIf \\((x^{\\star},y^{\\star})\\) is optimal there exists \\(\\lambda\\) such that:\n\n\\((x^{\\star},y^{\\star})\\) maximizes lagrangian \\(\\mathcal{L} = U(x_1, x_2) + \\lambda (B- p_1 x_1 - p_2 x_2)\\)\n\\(\\lambda \\geq 0\\)\n\\(B- p_1 x_1 - p_2 x_2 \\geq 0\\)\n\\(\\lambda  (B - p_1 x_1 - p_2 x_2 ) = 0\\)\n\nThe three latest conditions are called “complementarity” or “slackness” conditions\n\nthey are equivalent to \\(\\min(\\lambda, B - p_1 x_1 - p_2 x_2)=0\\)\nwe denote \\(\\lambda \\geq 0 \\perp B- p_1 x_1 + p_2 x_2  \\geq 0\\)\n\n\\(\\lambda\\) can be interpreted as the welfare gain of relaxing the constraint.",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#karush-kuhn-tucker-conditions-1",
    "href": "slides/session_optimization/index.html#karush-kuhn-tucker-conditions-1",
    "title": "Optimization",
    "section": "Karush-Kuhn-Tucker conditions",
    "text": "Karush-Kuhn-Tucker conditions\n\nWe can get first order conditions that factor in the constraints:\n\n\\(U^{\\prime}_x - \\lambda p_1 = 0\\)\n\\(U^{\\prime}_y - \\lambda p_2 = 0\\)\n\\(\\lambda \\geq 0 \\perp B-p_1 x_1 -p_2 x_2 \\geq 0\\)\n\nIt is now a nonlinear system of equations with complementarities (NCP)\n\nthere are specific solution methods to deal with it",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#solution-strategies-for-ncp-problems",
    "href": "slides/session_optimization/index.html#solution-strategies-for-ncp-problems",
    "title": "Optimization",
    "section": "Solution strategies for NCP problems",
    "text": "Solution strategies for NCP problems\n\nGeneral formulation for vector-valued functions \\[f(x)\\geq 0 \\perp g(x)\\geq 0\\] means \\[\\forall i, f_i(x)\\geq 0 \\perp g_i(x)\\geq 0\\]\n\nNCP do not necessarily arise from a single optimization problem\n\nThere are robust (commercial) solvers for NCP problems (PATH, Knitro) for that\nHow do we solve it numerically?\n\nassume constraint is binding then non-binding then check which one is good\n\nOK if not too many constraints\n\nreformulate it as a smooth problem\napproximate the system by a series of linear complementarities problems (LCP)",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#smooth-method",
    "href": "slides/session_optimization/index.html#smooth-method",
    "title": "Optimization",
    "section": "Smooth method",
    "text": "Smooth method\n\nConsider the Fisher-Burmeister function \\[\\phi(a,b) = a+b-\\sqrt{a^2+b^2}\\]\nIt is infinitely differentiable, except at \\((0,0)\\)\nShow that \\(\\phi(a,b) = 0 \\iff \\min(a,b)=0 \\iff a\\geq 0 \\perp b \\geq 0\\)\nAfter substitution in the original system one can use regular non-linear solver\n\nfun fact: the formulation with a \\(\\min\\) is nonsmooth but also works quite often",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "slides/session_optimization/index.html#optimization-libraries",
    "href": "slides/session_optimization/index.html#optimization-libraries",
    "title": "Optimization",
    "section": "Optimization libraries",
    "text": "Optimization libraries\n\nRobust optimization code is contained in the following libraries:\n\nRoots.jl: one-dimensional root finding\nNLSolve.jl: multidimensional root finding (+complementarities)\nOptim.jl: minimization\n\nThe two latter libraries have a somewhat peculiar API, but it’s worth absorbing it.\n\nin particular they provide non-allocating algorithms for functions that modify arguments in place\nthey are compatible with automatic differentiation\n\n\njulia&gt; f(x) = [x[1] - x[2] - 1, x[1] + x[2]]\nf (generic function with 1 method)\n\njulia&gt; NLsolve.nlsolve(f, [0., 0.0])\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.0, 0.0]\n * Zero: [0.5000000000009869, -0.5000000000009869]\n * Inf-norm of residuals: 0.000000       \n * Iterations: 1                       \n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true                           \n * Function Calls (f): 2\n * Jacobian Calls (df/dx): 2",
    "crumbs": [
      "Slides",
      "Optimization"
    ]
  },
  {
    "objectID": "coursework/coursework_2.html",
    "href": "coursework/coursework_2.html",
    "title": "Coursework 2025 - Cake Eating Problem",
    "section": "",
    "text": "Name:\nSurname:\nAfter completing the following questions, send the edited notebook, to pablo.winant@ensae.fr\nYou are allowed to use any online available resource, even to install Julia packages, but not to copy/paste any code.\nAlso, don’t forget to comment your code and take any initiative you find relevant.",
    "crumbs": [
      "Coursework",
      "Coursework 2025 - Cake Eating Problem"
    ]
  },
  {
    "objectID": "coursework/coursework_2.html#part-ι-useful-skills",
    "href": "coursework/coursework_2.html#part-ι-useful-skills",
    "title": "Coursework 2025 - Cake Eating Problem",
    "section": "Part Ι: Useful Skills",
    "text": "Part Ι: Useful Skills\n\nCreate a random 10x10 matrix R using rand(10,10). Normalize each row in R, in order to create a stochastic matrix (the sum of each row is equal to 1, and all coefficients are positive). Compute the only vector v with positive coefficients summing to 1, such that v' R == v'\n\n# your code...\n# R = ...\n# v = ...\n# check\n@assert maximum(abs, v' R - v')&lt;1e-8\n\nFind \\(u\\) such that \\(tanh(u)=0.5\\) by writing your own solver (no solver library but any algorithm you want).\n\n# your code...\n# u = \n# check\n@assert maximum(tanh(u) - 0.5)&lt;1e-8\n@assert maximum(u-tan(0.5))&lt;1e-8",
    "crumbs": [
      "Coursework",
      "Coursework 2025 - Cake Eating Problem"
    ]
  },
  {
    "objectID": "coursework/coursework_2.html#part-ii-cake-eating-problem-with-regeneration",
    "href": "coursework/coursework_2.html#part-ii-cake-eating-problem-with-regeneration",
    "title": "Coursework 2025 - Cake Eating Problem",
    "section": "Part II: Cake-Eating Problem with Regeneration",
    "text": "Part II: Cake-Eating Problem with Regeneration\n\nWe consider the following variant of the cake-eating problem. It can be interpreted as a model of sustainable consumption for a renewable resource.\nAn agent has a finite amount of cake \\(k_t\\) and must decide in each period how much of it to consume: \\(c_t\\in[0,k_t]\\).\nThe part of the cake that is not consumed degenerates at rate \\(\\delta\\) but new cake is also regenerated at rate \\(\\rho\\) so that the law of motion for the cake is:\n\\[k_{t+1}=\\delta (k_{t}-c_{t}) + \\rho\\]\nwith \\(\\delta\\in]0,1]\\) and \\(\\rho&gt;0\\)..\nThe goal is to maximize the present discounted value of utility over an infinite horizon:\n\\[\\sum_{t\\geq0} \\beta^t u(c_t)\\]\nwith \\(u(x) = \\frac{x^{1-\\sigma}}{1-\\sigma}\\)\nWe can use the parameters \\(\\beta=0.95\\), \\(\\sigma=2\\), \\(\\delta=0.9\\).\n\nWhat is the maximal value \\(\\overline{K}\\) for the cake that can be obtained using natural regeneration? Choose \\(N&gt;0\\) and discretize the values for the cake into a vector \\(K=(K_1, ... ,K_N)\\) of linearly spaced values.\n\n# your code\n\nCreate a named tuple (or any other Julia construct) to store all parameters of the (discretized) model. In what follows, all Julia functions will take an additional parameter m carrying all these informations.\n\n# your code\nSince the transition for the cake is deterministic, we can identify the choice of consumption with the choice of next period cake. Here we will make the assumption that next period level must stay on the discretized grid.\nThis yields the following, discretized, Bellman equation \\[V(K_i) = \\max_{j \\in A(i)} U(K_i,K_j) + \\beta V(K_j)\\] where \\(A(i)\\) is the set of all possible cake indices that can be chosen for tomorrow given today’s index and where \\(U(K_i,K_j)\\) is utility obtained from consumption today.\n\nWhat is \\(U(K_i,K_j)\\)? Write a function U(a,b,m)::Float64 to represent it in Julia.\n\n# your code\n\nWrite a function A(i,m)::Tuple{Int, Int} which maps the index \\(i\\) of today’s cake \\(K_i\\) with a tuple containing the lowest and highest possible indices for the cake tomorrow.\n\n# your code\n\nMake an initial guess v0 for the value function represented (a vector of Float64 of length N) and an admissible guess x0 for the optimal choices (a vector of Int64 of length N).\n\n# your code\n\nFor the initial guess, plot the values and the consumptions against the size of the cake\n\n# your code\n\nWrite a function bellman_step(v::Vector{Float64}, m)::Tuple{Vector{Float64}, Vector{Int64}} which associates \\(\\tilde{V}(K_1), ..., \\tilde{V}(K_N)\\) to \\(V(K_1), ... V(K_N)\\) in \\[V(K_i) = \\max_{j \\in A(i)} U(K_i,K_j) + \\beta \\tilde{V}(K_j)\\] This function returns the new value and the controls resulting from the optimization.\n\n# your code\n\nImplement the value function algorithm to find the optimal consumption policy by iterating on Bellman step until convergence.\n\n# your code\n\nPlot the optimal consumption rule.\n\n# your code\n\nImplement the policy iteration algorithm.\n\n# your code\n\n\n\n\n\n\nNote\n\n\n\nThe policy iteration algorithm can be obtained from the value function algorithm: after each bellman step which yields a new policy rule \\(p(i)\\), compute the value of this policy, i.e. the value \\(V\\) such that:\n\\[V(K_i) = \\left. \\left\\{U(K_i,K_j) + \\beta V(K_j)\\right\\} \\right|_{j=p(i)}\\]\n\n\n\nComment on the results (interpret, vary the parameters, …)\n\n# your code",
    "crumbs": [
      "Coursework",
      "Coursework 2025 - Cake Eating Problem"
    ]
  },
  {
    "objectID": "homework_ba.html",
    "href": "homework_ba.html",
    "title": "Homework",
    "section": "",
    "text": "The goal of this exercise consists in solving a simple CGE model using Julia.\nModel is taken from Handbook of Computible General Equilibrium Modeling. 1"
  },
  {
    "objectID": "homework_ba.html#a-simple-cge-model",
    "href": "homework_ba.html#a-simple-cge-model",
    "title": "Homework",
    "section": "",
    "text": "The goal of this exercise consists in solving a simple CGE model using Julia.\nModel is taken from Handbook of Computible General Equilibrium Modeling. 1"
  },
  {
    "objectID": "homework_ba.html#the-data-and-the-social-accounting-mmatrix",
    "href": "homework_ba.html#the-data-and-the-social-accounting-mmatrix",
    "title": "Homework",
    "section": "The data and the social accounting mmatrix",
    "text": "The data and the social accounting mmatrix\nIn a CGE, the data about the economy’s exchanges is compiled in one single “Social Accounting Matrix”. It is essentially a matrix representation of the national accounts that can be arbitrary refined based on data availability.\nIn this course work we consider the following simplified version. All items are in million euros.\n\n\n\nSAM\nBRD\nMLK\nCAP\nLAB\nHOH\n\n\n\n\nBRD\n\n\n\n\n15\n\n\nMLK\n\n\n\n\n35\n\n\nCAP\n5\n20\n\n\n\n\n\nLAB\n10\n15\n\n\n\n\n\nHOH\n\n\n25\n25\n\n\n\n\nIn this table, there are:\n\nTwo Goods: BRD and MLK\nTwo Factors: CAP and LAB (capital and labour)\nOne Agent: HOH (Households)\n\nTo read the matrix, we consider that rows decomposes the inputs for each column (measured by their cost).\nFor instance, the value of the production in the BRD-producing sector is decomposed between the payment to capital (5) and the payment to labor (10).\nWe will assume there are fixed endowments in capital and labour and that they are owned by the household.\nWe use the following notations:\n\n\\(X_i\\) household consumption of good i\n\\(F_{h,j}\\) the h-th factor input by firm j\n\\(Z_{j}\\) output of the j-th good\n\\(px_{i}\\) demand price of the i-th good\n\\(pz_{j}\\) supply price of the i-th good\n\\(pf_{h}\\) the h-th factor price}\n\nIn these definitions the indices are conventionally: \\(i \\in \\text{BRD,MLK}\\), \\(j\\in \\text{BRD,MLK}\\), \\(h \\in \\text{CAP,LAB}\\)).\nWe also denote by a zero exponent, the variables at the steady-state. For instance, \\(X_{BRD}^0\\) is the household consumption of good \\(BRD\\).\nWe assume that the SAM corresponds to values observed at the steady-state."
  },
  {
    "objectID": "homework_ba.html#consumers",
    "href": "homework_ba.html#consumers",
    "title": "Homework",
    "section": "Consumers",
    "text": "Consumers\nGiven prices \\(px_i\\), households maximize utility of consumption \\[U = \\prod_i X_i^{\\alpha_i}\\] where \\(\\alpha_i\\) is the share parameter in the utility function (with \\(\\sum_i \\alpha_i=1\\))\n\nWrite down the budget constraint of households. How can we calibrate \\(\\alpha_i\\) from the data?\n\n\n\\[\\alpha_i  = \\frac{X^0_i}{\\sum_j X^0_{j}}\\]\n\n\nShow that the household demand function satisfies \\[X_i  = \\alpha_i \\sum_h \\frac{pf_h FF_h}{px_i}\\]."
  },
  {
    "objectID": "homework_ba.html#producers",
    "href": "homework_ba.html#producers",
    "title": "Homework",
    "section": "Producers",
    "text": "Producers\nWe assume that producers of final goods, use the following production function:\n\\[Z_j   = b_j \\prod_h F_{h,j}^{\\beta_{h,j}}\\]\nwhere \\(\\beta_{h,j}\\) is the share parameter in production function (\\(\\sum_h \\beta_{h,j}=1\\)) and \\(b_j\\) a scale parameter.\n\nShow that factor demand function can be written as: \\[ F_{h,j} = \\beta_{h,j} \\frac{pz_j Z_j }{pf_h} \\]\nHow can we calibrate \\(\\beta_{h,j}\\) from the data? How can we recover \\(b_j\\)?\n\n\n\n\n\n\n\nNote\n\n\n\n\\[\\beta_{h,j} = \\frac{F^0_{h,j}}{\\sum_k F^0_{k,j}}\\] \\[b_j      = \\frac{Z^0_j}{ \\prod_h  (F^0_{h,j})^{\\beta_{h,j}} }\\]"
  },
  {
    "objectID": "homework_ba.html#equilibrium",
    "href": "homework_ba.html#equilibrium",
    "title": "Homework",
    "section": "Equilibrium",
    "text": "Equilibrium\nIn equilibrium we have the following conditions:\n\ndemand prices of the i-th good must match supply price of the same good: \\[px_{i} = pz_{i}\\]\ngood market must clear: \\[ X_i   = Z_i \\]\nfactor market must clear: \\[ \\sum_j F_{h,j} = FF_h \\]\n\n\nJustify the steady-state identity: \\(Z^0_j   = \\sum_h F^0_{h,j}\\)\n\nCalibration\n\\(i\\in I\\):goods \\(h\\in H\\): factor\n\\(X^0_i   = SAM(i,\"HOH\")\\)\n\\(F^0_{h,j} = SAM(h,j)\\)\n\\(FF_h   = SAM(\"HOH\",h)\\)\nDefinitions\n\\(X_i\\) household consumption of the i-th good\n\\(F_{h,j}\\) the h-th factor input by the j-th firm\n\\(Z_{j}\\) output of the j-th good\n\\(px_{i}\\) demand price of the i-th good\n\\(pz_{j}\\) supply price of the i-th good\n\\(pf_{h}\\) the h-th factor price\n\\(UU\\) utility [fictitious]\ngood market clearing condition \\[ X_i   = Z_i \\]\nfactor market clearing condition \\[ \\sum_j F_{h,j} = FF_h \\]\nprice equation \\[ px_i  = pz_i \\]\nutility function [fictitious] $$ UU = _i X_i^{_i}"
  },
  {
    "objectID": "homework_ba.html#section",
    "href": "homework_ba.html#section",
    "title": "Homework",
    "section": "",
    "text": "Consumer problem"
  },
  {
    "objectID": "homework_ba.html#footnotes",
    "href": "homework_ba.html#footnotes",
    "title": "Homework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGAMS is a modeling software, routinely used for CGEs. You can checkout the code for the current model here↩︎"
  },
  {
    "objectID": "pushups/1_epidemiology.html",
    "href": "pushups/1_epidemiology.html",
    "title": "Epidemiology Models",
    "section": "",
    "text": "H.W. Heathcote: epidemiologic models are deterministic models for infectious diseases which are spread by direct person-to-person contact in a population.\nThis kind of models has been used since by a few decades by economist, for matters that have nothing to do with health.\n\ndiffusion of information, or opinions on social medias\nasset prices and fads\nexpectation formation for macroeconomic outcomes (ex: The Epidemiology of Macroeconomic Expectations by Chris Carroll)\n\nEpidemiologic models have two features that depart from standard economic models:\n\nAgent’s behaviour does not take the full system into account, an is irrational in the sense that it isn’t forward looking. Instead, an agent is given a behavioural heuristic.\nThe transitions of the whole system can be determined without solving for complex interactions first.\n\nUnderstanding why these two assumptions are very costly for economists will keep us busy during a big part of the course. Here we just consider two simple models as small programming exercises.\n\nSimple SIR model\nThere is a continuum of agents of mass \\(1\\). Each agent can be either “Susceptible” (S), “Infected” (I) or “Recovered” (R). In each period, one agent meets another agent drawn randomly. During a meeting Susceptible agents who meet an infected agent, will always catch the disease (or the fad) but are not contagious. Infected agents, have a probability \\(\\pi\\) of being recovered. Nothing happens to Recovered agents who meet other people. No distinction is made between recovering as a healthy or a dead person.\nWe’re interested in the evolution in the number infected persons, both the speed of infection and the total amount of infected people in every period.\nWrite down the transition equations for \\(n_I\\), the number of infected people, for \\(n_R\\) the number of recovered people and \\(n_S\\) the number of susceptible people.\nCompute the transition function f for the vector state \\(s_t\\) returning \\(s_{t+1}\\).\nCompute the transitions over \\(T\\) periods. Plot the result using Plots.jl. (bonus: check against closed form solution)\nWe now assume a Susceptible person who meets an infected one has a probability \\(\\mu\\) of catching the disease. Update the transition function \\(f\\) and write a function simulate of \\(\\mu\\), \\(\\pi\\) which returns the simulation. Compare different values of the parameters. How would you interpret “social distancing”? How would you choose parameters \\(\\Pi\\) and \\(\\mu\\)\n\n\nA Spatial SIR model\nWe now consider another version of the model where agents evolve in the space \\(\\mathcal{S}=[0,1]\\times[0,1]\\). There are \\(N\\in\\mathbb{N}\\) agent. At any date, each agent \\(n \\in [0,1]\\) is located at \\((x_n,y_n)\\in \\mathcal{S}\\).\nEach agent moves follows a random walk bounded by \\(\\mathcal{S}\\): \\[x_t = \\min(\\max( x_{t-1} + \\epsilon_t, 0), 1)\\] \\[y_t = \\min(\\max( y_{t-1} + \\eta_t, 0), 1)\\] where \\(\\epsilon_t\\) and \\(\\eta_t\\) are both normally distributed with standard deviation \\(\\sigma\\).\nAt any date, the individual state of an agent is \\(s_t=(x_t, y_t, h_t)\\) where \\(h_t\\) is either “S”, “I” or “R”. \\(v_t\\) denotes the states of all agents (for instance \\(v_t=(s_{n,t})_n\\). The health status of each agent is updated in the following way:\n\nAgents \\(R\\) stay \\(R\\).\nAgents \\(I\\) have probability \\(\\pi\\) to become \\(R\\). They stay \\(I\\) otherwise.\nAn agent of type \\(S\\) in position \\((x,y)\\) has a probability \\(prob(x,y,S)\\) to be infected that is \\(\\mu\\) if there is another infected agent within a radius \\(r&gt;0\\).\n\nDefine a type Agent, which holds the type of an agent. The state of the whole system will be held in a Vector[Agent] type.\nWrite a function spatial_transition(S::Vector{Agent})::Vector{Agent} to compute the transition of the positions. Write another function random_guess(T=100) which simulates for \\(T\\) periods in order to find a good initial guess.\nWrite a function show_population to plot all agents with different colors for different health status.\nWrite a function evolve(S::Vector[Agent])::Vector[Agent] which takes the population in a given state and returns the same population with updated health status.\nWrite a function simulate(S0::Vector[Agent], k=1) to simulate the economy starting from an initially random position with k infected individuals. The returned object should be of type Vector[Vector[Agent]].\nCompute statistics along a simulated path for \\(n_I\\), \\(n_S\\), \\(n_R\\). Plot and compare with the basic SIR model\n\n\nAdditional questions\nHave fun by trying to answer one of these questions:\n\nchange probability of infection so that it depends on the number of infected people in the vincinity (with some suitable functional formulation for)\ncompute an animation of the transition\ncompute an interactive visualisation (with Interact.jl if available)",
    "crumbs": [
      "Pushups",
      "Epidemiology Models"
    ]
  },
  {
    "objectID": "homework_correction.html",
    "href": "homework_correction.html",
    "title": "Homework",
    "section": "",
    "text": "Name:\nSurname:\nAfter completing the following questions, send the edited notebook to pablo.winant@polytechnique.fr\nYou are allowed to use any online available resource, and to you use any julia package, but not to copy/paste any code.\nAlso, don’t forget to comment your code and take any initiative you find useful and/or interesting. Also don’t wait to be prompted to interpret your own results (or lack thereof)."
  },
  {
    "objectID": "homework_correction.html#part-i-a-simple-cge-model",
    "href": "homework_correction.html#part-i-a-simple-cge-model",
    "title": "Homework",
    "section": "Part I : A simple CGE model",
    "text": "Part I : A simple CGE model\nThe goal of this exercise consists in solving a simple CGE model using Julia.\nModel is taken from Handbook of Computible General Equilibrium Modeling. 1\n\nThe data and the social accounting matrix\nIn a CGE, the data about the economy’s exchanges is compiled in one single “Social Accounting Matrix”. It is essentially a matrix representation of the national accounts that can be arbitrary refined based on data availability.\nIn this course work we consider the following simplified version. All items are in million euros.\n\n\n\nSAM\nBRD\nMLK\nCAP\nLAB\nHOH\n\n\n\n\nBRD\n\n\n\n\n15\n\n\nMLK\n\n\n\n\n35\n\n\nCAP\n5\n20\n\n\n\n\n\nLAB\n10\n15\n\n\n\n\n\nHOH\n\n\n25\n25\n\n\n\n\nIn this table, there are 5 sectors:\n\nTwo Goods: BRD and MLK\nTwo Factors: CAP and LAB (capital and labour)\nOne Agent: HOH (Households)\n\nEach of the rows feature a decomposition of the spending associated with each column.\nFor instance, the value of the production in the BRD-producing sector (column BRD) is decomposed between the payment to capital (5) and the payment to labor (10). The full capital income goes to households (25). Households spend their income between BRD (15) and MLK (35.)\n\nCreate a DataFrame representing the SAM matrix.\n\n\nusing DataFrames\n\nSAM = ...\n\n\n\nNotations\nTo shorten notations while preserving readibility we associate conventional indices to each sector: \\(i \\in \\text{BRD,MLK}\\), \\(j\\in \\text{BRD,MLK}\\), \\(h \\in \\text{CAP,LAB}\\)).\nTo model the economy, we will assume that endowments \\(F_h\\) are in fixed quantities (so \\(F_{BRD}\\) and \\(F_{MLK}\\) are two fixed parameters.)\nWe use the following notations:\n\n\\(X_i\\) household consumption of good i\n\\(F_{h,j}\\) the h-th factor input by firm j\n\\(Z_{j}\\) output of the j-th good\n\\(p^x_{i}\\) demand price of the i-th good\n\\(p^z_{j}\\) supply price of the i-th good\n\\(p^f_{h}\\) the h-th factor price\n\nOur goal is to endogenize all these variables (there are 14 of them), by constructing a system of equations corresponding to the model we describe below.\nWe also denote by a zero exponent, the variables at the initial steady-state. For instance, \\(X_{BRD}^0\\) is the household consumption of good \\(BRD\\). We assume that the SAM corresponds to values observed at the initial steady-state.\n\n\nConsumers\nHouseholds are endowed with capital (resp labour) in quantity \\(F_{CAP}\\) resp \\(F_{LAB}\\)) that they sell to the goods producing firms at price \\(p^f_{CAP}\\) (resp \\(p^f_{LAB}\\)).\nGiven prices of goods \\(p^x_i\\), households maximize utility of consumption \\[U = \\prod_i X_i^{\\alpha_i}\\] where \\(\\alpha_i\\) is the share parameter in the utility function (with \\(\\sum_i \\alpha_i=1\\))\n\nWrite down the budget constraint of households. Show that household’s demand can be written as function of total gdp and price of goods as in \\[X_i  = \\alpha_i\\frac{ \\sum_h  p^f_h F_h}{p^x_i}\\].\nAccording to the SAM matrix, what is the value of gdp in the model? How can we calibrate the values of \\(\\alpha_i\\)?\n\n\n\\[X_i  = \\alpha_i  \\frac{\\sum_h p^f_h F_h}{p^x_i}\\] \\[\\alpha_i  = \\frac{p^x_i X^0_i}{\\sum_j p^x_j X^0_{j}}\\]\n\n\n\nProducers\nWe assume that producers of final goods, use the following production function:\n\\[Z_j   = b_j \\prod_h F_{h,j}^{\\beta_{h,j}}\\]\nwhere \\(\\beta_{h,j}\\) is the share parameter in production function (\\(\\sum_h \\beta_{h,j}=1\\)) and \\(b_j\\) is a scale parameter. Firms rent labour and capital at market price and are perfectly competitive.\n\nShow that factor demand function can be written as: \\[ F_{h,j} = \\beta_{h,j} \\frac{p^z_j Z_j }{p^f_h} \\]\nHow can we calibrate \\(\\beta_{h,j}\\) from the data? How can we recover \\(b_j\\)?\nWithout loss of generality we can assume that all prices are equal to 1 in the initial equilibrium. 2 What are then the values for endowments \\(F_h\\) consistent with the SAM? How can we calibrate the value of \\(b_j\\)?\n\n\n\n\n\n\n\nNote\n\n\n\n\\[\\beta_{h,j} = \\frac{F^0_{h,j}}{\\sum_k F^0_{k,j}}\\] \\[b_j      = \\frac{Z^0_j}{ \\prod_h  (F^0_{h,j})^{\\beta_{h,j}} }\\]\n\n\n\n\nEquilibrium\nIn equilibrium we have the following conditions:\n\ndemand prices of the i-th good must match supply price of the same good: \\[p^x_{i} = p^z_{i}\\]\ngood markets must clear: \\[ X_i   = Z_i \\]\nfactor markets must clear: \\[ \\sum_j F_{h,j} = F_h \\]\nthe price of labour is taken as the numeraire \\(p_{LAB} = 1\\) (the other prices are endogenous).\n\n\n\n\nCalibration and solution\n\nDefine a structure model, containing the parameters of the model that can be directly calibrated from the data (i.e. \\(\\alpha_i\\),\\(\\beta_{ij}\\), \\(b_j\\), \\(F_h\\) …).\n\n\n## Choose the structure your prefer\n\n\nCreate an initial set of values vars0 for the model variables to be solved for. These variables should be consistent with the Social Accounting Matrix.\n\n\n## one suggestion consists in defining a named tuple of the form:\n\nvars0 = (;\n    X=(;\n        BRD=...,\n        MLK=...\n    ),\n    F=(;\n        LAB=(;\n            BRD=...,\n            MLK=...\n        )\n    ),\n    ...\n)\n\n\nWrite a function which packs the set of variable values into a julia vector. Write the reverse function to unpack a vector into a structure of the same type as vars. Check that these two functions are consistent (i.e. pack(unpack(u)) == u).\n\n\n# input argument should be the same as vars0\nfunction pack(vars)::Vector\n\n    # [X_BRD, X_MLK, F_LAB_BRD, ...]\nend\n\n\n# output argument should be the same as vars0\nfunction unpack(u::AbstractVector)\n\n    \nend\n\nunpack (generic function with 1 method)\n\n\n\nWrite a function residuals which check whether the model equations are satisfied for a particular guess v0. The output can be a simple vector. Check the residuals for the initial guess vars0. They should be zeros is if the initial values and the model equations have been carefully chosen.\n\n\nfunction residuals(model, v)::Vector\n\n    # write down the model equations below\n    \n\nend\n\n\nAdd a new method to the function residuals, which now takes a vector as argument (using the unpack function). Compute the jacobian of that new function. Verify that the rank of this jacobian equals the number of variables to be determined. If the number of equations is greater than the number of variables to be determined, can you identify one equation that can be safely removed?.\n\n\n# Your code\n\n\nWrite a method to solve the system in all the endogenous variables when the initial guess is close to the initial equilibrium. You can use your method of choice (using your own code, using nlsolve, …). To ease convergence, you might want to take into account information about variable boundaries (all variables are positive).\n\n\n# Your code\n\n\n\nApplications\n\nA massive migration doubles the endowment of labour, which is now equal to 50. Assuming structural parameters are unchanged, Compute the new equilibrium using the same code as before. Comment on the changes in quantities and prices. Could you have predicted them?\n\n\n# Your code\n\n\nWhat happens if the labour share in both production sectors is reduced by 1%? (i.e. \\(\\beta_{LAB,j}\\) is reduced by 1%).\n\n\n# Your code"
  },
  {
    "objectID": "homework_correction.html#part-ii---endogenous-exit",
    "href": "homework_correction.html#part-ii---endogenous-exit",
    "title": "Homework",
    "section": "Part II - Endogenous Exit",
    "text": "Part II - Endogenous Exit\n\nWarmup: discretization of an AR1\nThe following code, taken from Quantecon.jl approximates an AR1 process \\[y_t = \\mu + \\rho (y_{t-1}-\\mu) + \\epsilon_t\\] (where \\(\\nu\\) is the standard deviation of normal process \\(\\epsilon\\)), using a finite markov chain with \\(N\\) different values.\nThe output is a transition matrix \\(P_ij\\) and a vector containing discretized values \\(y_1, y_2, ... y_p\\)\n\n# uncomment the following line if \"SpecialFunctions\" is not on your system\n# import Pkg; Pkg.add(\"SpecialFunctions\")\n\nusing SpecialFunctions: erfc\n\nstd_norm_cdf(x::T) where {T &lt;: Real} = 0.5 * erfc(-x/sqrt(2))\nstd_norm_cdf(x::Array{T}) where {T &lt;: Real} = 0.5 .* erfc(-x./sqrt(2))\n\nfunction tauchen(N::Integer, ρ::T1, σ::T2, μ=zero(promote_type(T1, T2)), n_std::T3=3) where {T1 &lt;: Real, T2 &lt;: Real, T3 &lt;: Real}\n    # Get discretized space\n    a_bar = n_std * sqrt(σ^2 / (1 - ρ^2))\n    y = range(-a_bar, stop=a_bar, length=N)\n    d = y[2] - y[1]\n\n    # Get transition probabilities\n    Π = zeros(promote_type(T1, T2), N, N)\n    for row = 1:N\n        # Do end points first\n        Π[row, 1] = std_norm_cdf((y[1] - ρ*y[row] + d/2) / σ)\n        Π[row, N] = 1 - std_norm_cdf((y[N] - ρ*y[row] - d/2) / σ)\n\n        # fill in the middle columns\n        for col = 2:N-1\n            Π[row, col] = (std_norm_cdf((y[col] - ρ*y[row] + d/2) / σ) -\n                           std_norm_cdf((y[col] - ρ*y[row] - d/2) / σ))\n        end\n    end\n\n    yy = y .+ μ / (1 - ρ) # center process around its mean (wbar / (1 - rho)) in new variable\n\n    (;transitions=Π, values=yy)\n\nend\n\ntauchen (generic function with 3 methods)\n\n\n\nTake \\(\\rho=0.95, \\mu=0.1, \\nu=0.1\\). Approximate the AR1 with 200 discrete states, using the tauchen function above. Check that all rows sum to 1. Compute and plot the steady-state distribution.\n\n\n# you code here\n\n\nSimulate the evolution of the above markov chain for 10000 periods. Compute the mean and standard deviation of all the simulated values. Compare these moments with the results obtained in the preceding question.\n\n\n# you code here\n\n\n\nA firm’s problem\nConsider a firm whose productivity \\(y_t\\) is exogenous and evolves according to the markov chain above.\nProfits are given by \\(\\pi(y_t) = y_t\\).\nAt the start of each period, the firm decides whether to remain in operation and receive current profit \\(\\pi_t\\) or to exit and receive scrap value \\(s&gt;0\\) for the sale of physical assets.\nTime is discounted using interest rate, that is \\(\\beta=\\frac{1}{1+r} \\in [0,1[\\).\nThe following code creates a parameterization of the firm’s problem:\n\n\"Creates an instance of the firm exit model.\"\nfunction create_exit_model(;\n    n=200, # productivity grid size\n    ρ=0.95, μ=0.1, ν=0.1, # persistence, mean and volatility\n    β=0.98, s=100.0 # discount factor and scrap value\n    )\n    mc = tauchen(n, ρ, ν, μ)\n    z_vals, Q = mc.state_values, mc.p\n    return (; n, z_vals, Q, β, s)\nend\n\ncreate_exit_model\n\n\nSolving the model consists in finding the value of the firm in each productivity state \\(y\\) as well as the optimal decision (quit or continue in each of these states).\nThe value of the firm satisfies the Bellman equation:\n\\[V(y) = \\max_{c(y)} \\pi(y) + \\beta \\begin{cases} \\mathbb{E}\\left[ V(y') | y\\right] & \\text{if c(y)=true}\\\\ s & \\text{otherwise}\\end{cases}\\]\nGiven that we have discretized the AR1 as a markov chain, one can also write it\n\\[V(y_i) = \\max_{c(y_i)} \\pi(y_i) + \\beta \\begin{cases} \\sum_j P_{ij} V(y_j)  & \\text{if } c(y_i)=\\text{true} \\\\ s & \\text{otherwise} \\end{cases}\\]\nFor this problem it is natural to encode a guess for the value function \\((V(y_i))_i\\) as a Vector{Float64} of length \\(p\\) and the vector of controls \\((c(y_i))_i\\) as a Vector{Float64} of length \\(p\\).\n\nWrite a function bellman_step(model, V) which takes in the model parameterization, as well as a guess for the next period value function (\\(V()\\) on the right hand side of the Bellman equation) and returns a new updated value (\\(V()\\) on the left).\n\n\n# you code here\n\n\nApply the bellman_step function until convergence, starting from any initial guess.3 Show that convergence speed is linear. Plot the resulting value function and optimal choices. Comment.\n\n\n# you code here"
  },
  {
    "objectID": "homework_correction.html#footnotes",
    "href": "homework_correction.html#footnotes",
    "title": "Homework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGAMS is a modeling software, routinely used for CGEs. You can checkout the code for the current model here↩︎\nthis can be done by renormalizing the unit of the quantities.↩︎\nThis is known as the value function iteration algorithm.↩︎"
  },
  {
    "objectID": "docs/tutorials/2_solow.out.html",
    "href": "docs/tutorials/2_solow.out.html",
    "title": "Convergence: Solow Model",
    "section": "",
    "text": "Pablo Winant\n\nSolow Model\nA representative agent uses capital \\(k_t\\) to produce \\(y_t\\) using the following production function:\n\\[y_t = k_t^{\\alpha}\\]\nHe chooses to consume an amount \\(c_t \\in ]0, y_t]\\) and invests what remains:\n\\[i_t = y_t - c_t\\]\nHe accumulates capital \\(k_t\\) according to:\n\\[k_{t+1} = \\left( 1-\\delta \\right) k_{t} + i_{t}\\]\nwhere \\(\\delta\\) is the depreciation rate and \\(i_t\\) is the amount invested.\nThe goal of the representative agent is to maximize:\n\\[\\sum_{t\\geq 0} \\beta^t U(c_t)\\]\nwhere \\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\) and \\(\\beta&lt;1\\) is the discount factor.\nFor now, we ignore the objective and assume that the saving rate \\(s=\\frac{c_t}{y_t}\\) is constant over time.\nCreate a NamedTuple to hold parameter values \\(\\beta=0.96\\), \\(\\delta=0.1\\), \\(\\alpha=0.3\\), \\(\\gamma=4\\).\nWrite down the formula of function \\(f\\) such that \\(k_{t+1}\\): \\(k_{t+1} = f(k_t)\\).\nDefine a function f(k::Float64, p::NamedTuple)::Float64 to represent \\(f\\) for a given calibration\nWrite a function simulate(k0::Float64, T::Int, p::NamedTuple)::Vector{Float64} to compute the simulation over T periods starting from initial capital level k0.\nMake a nice plot to illustrate the convergence. Do we get convergence from any initial level of capital?\nSuppose you were interested in using f to compute the steady-state. What would you propose to measure convergence speed? To speed-up convergence? Implement these ideas."
  },
  {
    "objectID": "tutorials/2_solow.html",
    "href": "tutorials/2_solow.html",
    "title": "Convergence: Solow Model",
    "section": "",
    "text": "Solow Model\nA representative agent uses capital \\(k_t\\) to produce \\(y_t\\) using the following production function:\n\\[y_t = k_t^{\\alpha}\\]\nHe chooses to consume an amount \\(c_t \\in ]0, y_t]\\) and invests what remains:\n\\[i_t = y_t - c_t\\]\nHe accumulates capital \\(k_t\\) according to:\n\\[k_{t+1} = \\left( 1-\\delta \\right) k_{t} + i_{t}\\]\nwhere \\(\\delta\\) is the depreciation rate and \\(i_t\\) is the amount invested.\nThe goal of the representative agent is to maximize:\n\\[\\sum_{t\\geq 0} \\beta^t U(c_t)\\]\nwhere \\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\) and \\(\\beta&lt;1\\) is the discount factor.\nFor now, we ignore the objective and assume that the saving rate \\(s=\\frac{i_t}{y_t}\\) is constant over time.\nCreate a NamedTuple to hold parameter values \\(\\beta=0.96\\), \\(\\delta=0.1\\), \\(\\alpha=0.3\\), \\(\\gamma=4\\).\n\n# how do we record the model informations\n\n\n# option 1: use structure (your own type)\nstruct ModelType\n    α::Float64\n    β\n    γ\n    δ\nend\n\n\nmodel = ModelType(0.3, 0.96, 2.0, 0.1)\n# access the fields with a dot\nmodel.α\n# problem: you cannot redefine structures without restarting the kernel\n\n0.3\n\n\n\n# use a dictionary \n\nmodel_dict = Dict(\n    :α =&gt; 0.3,\n    :β =&gt; 0.96,\n    :γ =&gt; 2.0,\n    :δ =&gt; 0.1\n    )\n\nDict{Symbol, Float64} with 4 entries:\n  :α =&gt; 0.3\n  :γ =&gt; 2.0\n  :δ =&gt; 0.1\n  :β =&gt; 0.96\n\n\n\n# a dictionary is a list of pairs\n0 =&gt; \"zero\"\n\n0 =&gt; \"zero\"\n\n\n\n# the keys to dictionaries must be immutable objects (hashable)\n# for this reason we suse symbols instead of strings\n\n\"JIijlkjljasdf  43 '4321514 5\"\n\n\"JIijlkjljasdf  43 '4321514 5\"\n\n\n\n:hkhrd  # this is a symbol (the characters are somwehat restricted.)\n\n:hkhrd\n\n\n\n# access the fields with brackets\nmodel_dict[:α]\n\n0.3\n\n\n\nmodel_dict[:α] = 0.4\n# you change values: dictionaries are mutable\n\n0.4\n\n\n\n# problems with dictionaries:\n# - a lot of model_dict[:α]\n# - not compiler frienldy\n\n\n# solution 3: use tuples\n\nmodel_tuple = (0.3, 0.96, 2.0, 0.1)\n\n(0.3, 0.96, 2.0, 0.1)\n\n\n\n# problem: very error prone because one needs to remember the positions\nβ = model_tuple[2]\n\n0.96\n\n\n\n# 🐉\n\n# solution 4: use a named tuple\n\n# convention: keyword arguments after ;\n\nmodel = (; s=0.2, α=0.3, β=0.96, γ=2.0, δ=0.1)\n\n(s = 0.2, α = 0.3, β = 0.96, γ = 2.0, δ = 0.1)\n\n\n\nmodel[1]\n\n0.2\n\n\n\nmodel.α\n\n0.3\n\n\n\n# you can unpack the content of a named tuple\na,b,c,d = model # error prone\n\n(α = 0.3, β = 0.96, γ = 2.0, δ = 0.1)\n\n\n\n# you can unpack using the names\n(;α, β, γ, δ)  = model\n\n# equivalent to \n# α = model.α\n# β = model.β\n\n(α = 0.3, β = 0.96, γ = 2.0, δ = 0.1)\n\n\nWrite down the formula of function \\(f\\) such that \\(k_{t+1}\\): \\(k_{t+1} = f(k_t)\\).\nYou get: \\[k_{t+1} = (1-\\delta) k_t + s k_t^{\\alpha}\\]\nDefine a function f(k::Float64, p::NamedTuple)::Float64 to represent \\(f\\) for a given calibration\n\n# there are three ways to define a function\n\n# 1 / with a function end block\n# and an explicit return statement\n\nfunction fun(a, b; c=1, d=2)\n    return a + b + c + d\nend\n\nfun (generic function with 1 method)\n\n\n\n# 2 / with a one-liner\n\ngun(a, b; c=1, d=2) = a + b + c + d\n\ngun (generic function with 1 method)\n\n\n\n# 3/ an anynomous function\n\n# doesn't work with keywords\nmyfun = ((a,b,c,d) -&gt;  a+b+c+d)\nmyfun(2,3,4,5)\n\n#13 (generic function with 1 method)\n\n\n\n# functions have name\n# methods implement a function for a specific type signature\n\n\nadd(a) = a+1\n\nadd (generic function with 1 method)\n\n\n\nadd(x::Int) = x+1\nadd(x::Float64) = x+1.00001\n\nadd (generic function with 3 methods)\n\n\n\nadd(232.0)\n\n233.00001\n\n\n\nmethods(add)\n\n# 3 methods for generic function add from \u001b[35mMain\u001b[39m: add(x::Float64) in Main at /home/pablo/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl:2  add(x::Int64) in Main at /home/pablo/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X52sZmlsZQ==.jl:1  add(a) in Main at /home/pablo/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X51sZmlsZQ==.jl:1 \n\n\nWrite a function simulate(k0::Float64, T::Int, p::NamedTuple)::Vector{Float64} to compute the simulation over T periods starting from initial capital level k0.\n\n# 🐉\n\nf(k0, m) = begin\n    (;α, β, γ, δ, s)  = m\n    (1-δ)*k0 + s*k0^α\nend\n\nf (generic function with 1 method)\n\n\n\nf(0.2, model)\n\n0.30340677254400195\n\n\n\n# 🐉\nfunction simulate(k0, T, p)\n\n\n    # store thre result in a non-allocated vector\n\n    # when we initialize a vector with given elements, it determines the type of the vector\n    sim = [k0]\n    for t = 1:T\n\n        k1 = f(k0, p)\n\n        # add a new element\n        # exclamation mark conventionnally means that (first) arguments are mutateted\n        push!(sim, k1)\n\n        k0 = k1\n\n    end\n\n    return sim\n    \nend\n\nsimulate (generic function with 1 method)\n\n\n\nsimulate(0.2, 100, model)\n\n101-element Vector{Float64}:\n 0.2\n 0.30340677254400195\n 0.4129080792968781\n 0.525003373019628\n 0.6373491155565568\n 0.7483344417178636\n 0.856841626057419\n 0.9620988898971137\n 1.0635841029776287\n 1.1609587708409257\n ⋮\n 2.6876186883735818\n 2.6879113388853018\n 2.6881835130861362\n 2.6884366430712516\n 2.6886720608576273\n 2.688891005366757\n 2.689094628921639\n 2.6892840032916374\n 2.6894601253165105\n\n\n\n# make the simulate function more user-friendly\n\n# 🐉\n\nsimulate(k0, model; T=100) = simulate(k0, T, model)\n\nsimulate (generic function with 2 methods)\n\n\n\nmethods(simulate)\n\n# 2 methods for generic function simulate from \u001b[35mMain\u001b[39m: simulate(k0, model; T) in Main at /home/pablo/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X65sZmlsZQ==.jl:5  simulate(k0, T, p) in Main at /home/pablo/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X13sZmlsZQ==.jl:2 \n\n\nMake a nice plot to illustrate the convergence. Do we get convergence from any initial level of capital?\n\nusing Plots\n\n\n# 🐉\n\nk0 = 0.2\nsim = simulate(k0, model);\n\n\nplot(sim; title=\"Baseline Simulation\", label=\"k0=0.2\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# plot different levels of capital\n# 🐉\n\nklevels = [0.5, 1., 1.5, 2, 2.5, 3, 3.5]\nsimulations = [simulate(k0, model) for k0 in klevels];\n\n\n# 🐉\n\n# create initially empty plot\npl = plot(; title=\"Baseline Simulation\", xlabel=\"\\$t\\$\", ylabel=\"\\$k_t\\$\")\n\nfor (k, sim) in zip(klevels, simulations)\n    plot!(pl, sim; label=\"\\$k_0=$k\\$\")\nend\n\npl\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# to add two lines, you need modify an existing plot\npl = plot(sim; title=\"Baseline Simulation\", label=\"k0=0.2\")\nplot!(pl, sim*2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# what if I want to change the parametrization? for a different saving rate\nk0 = 0.2\nsim1 = simulate(k0, merge(model, (;s=0.2)));\nsim2 = simulate(k0, merge(model, (;s=0.3)));\n\n\npl = plot(sim1)\nplot!(sim2)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you were interested in using f to compute the steady-state. What would you propose to measure convergence speed? To speed-up convergence? Implement these ideas.",
    "crumbs": [
      "Tutorials",
      "Convergence: Solow Model"
    ]
  },
  {
    "objectID": "tutorials/6_automatic_differentiation.html",
    "href": "tutorials/6_automatic_differentiation.html",
    "title": "Simulation and Automatic Differentiation",
    "section": "",
    "text": "Choose a 2x2 matrix \\(P\\) (with spectral radius &lt;1) and a 2x2 matrix Q.\nConsider the VAR1 process \\(x_t = P x_{t-1} + Q \\epsilon_t\\) where \\(\\epsilon_t= (\\eta_{1,t}, \\eta_{2,t})\\) with \\(\\eta_1\\sim\\mathcal{N}(0,1)\\) and \\(\\eta_1\\sim\\mathcal{N}(0,1)\\)\nCompute impulse response functions.\nSimulate the process for \\(T\\) periods.\nSimulate the process \\(N=1000\\) times for \\(T=1000\\) periods. How would you store the results?\nBonus: how can you make the calculation faster?\nMake density plots to illustrate the ergodic property of the process\nCompute the asymptotic variance of the process. Compare with the theoretical one."
  },
  {
    "objectID": "tutorials/6_automatic_differentiation.html#learning-the-consumption-rule",
    "href": "tutorials/6_automatic_differentiation.html#learning-the-consumption-rule",
    "title": "Simulation and Automatic Differentiation",
    "section": "Learning the Consumption Rule",
    "text": "Learning the Consumption Rule\nThis exercise is inspired from Individual learning about consumption by Todd Allen and Chris Carroll link and from Deep Learning for Solving Economic models by Maliar, Maliar and Winant link\nWe consider the following consumption saving problem. An agent receives random income \\(y_t = \\exp(\\epsilon_t)\\) where \\(\\epsilon_t\\sim \\mathcal{N}(\\sigma)\\) (\\(\\sigma\\) is the standard deviation.)\nConsumer starts the period with available income \\(w_t\\). The law of motion for available income is:\n\\[w_t = \\exp(\\epsilon_t) + (w_{t-1}-c_{t-1}) r\\]\nwhere consumption \\(c_t \\in ]0,w_t]\\) is chosen in each period in order to maximize:\n\\[E_t \\sum_{t=0}^T \\beta^t U(c_t)\\]\ngiven initial available income \\(w_0\\).\nIn the questions below, we will use the following calibration:\n\n\\(\\beta = 0.9\\)\n\\(\\sigma = 0.1\\)\n\\(T=100\\)\n\\(U(x) = \\frac{x^{1-\\gamma}}{1-\\gamma}\\) with \\(\\gamma=2\\)\n\\(w_0 = 1.1\\) (alternatively, consider values 0.5 and 1)\n\nThe theoretical solution to this problem is a concave function \\(\\varphi\\) such that \\(\\varphi(x)\\in ]0,x]\\) and \\(\\forall t,  c_t=\\varphi(w_t)\\). Qualitatively, agents accumulate savings, up to a certain point (a buffer stock), beyond which wealth is not increasing any more (in expectation).\nCarroll and Allen have noticed that the true solution can be approximated very well by a simple rule:\n\\(\\psi(x) = \\min(x, \\theta_0 + \\theta_1 (x - \\theta_0) )\\)\nThe main question they ask in the aforementioned paper is whether it is realistic that agents would learn good values of \\(\\theta_0\\) and \\(\\theta_1\\) by observing past experiences.\nWe would like to examine this result by checking convergence of speed of stochastic gradient algorithm.\n\nLifetime reward\nDefine a NamedTuple to hold the parameter values\nDefine simple rule fonction consumption(w::Number, θ_0::Number, θ_1::Number, p::NamedTuple) which compute consumption using a simple rule. What is the meaning of \\(\\theta_0\\) and \\(\\theta_1\\)? Make a plot in the space \\(w,c\\), including consumption rule and the line where \\(w_{t+1} = w_t\\).\n(remark for later: Number type is compatible with ForwardDiff.jl 😉)\nWrite a function lifetime_reward(w_0::Number, θ_0::Number, θ_1::Number, p::NamedTuple) which computes one realization of \\(\\sum \\beta^t U(c_t)\\) for initial wealth w_0 and simple rule θ_0, θ_1. Mathematically, we denote it by \\(\\xi(\\omega; \\theta_0, \\theta_1)\\), where \\(\\omega\\) represents the succession of random income draws.\nWrite a function expected_lifetime_reward(w_0::Number, θ_0::Number, θ_1::Number,  p::NamedTuple; N=1000) which computes expected lifetime reward using N Monte-Carlo draws. Mathematically, we write it \\(\\Xi^{N}(\\theta_0, \\theta_1) =\\frac{1}{N} \\sum_1^N {\\xi(\\omega_N; \\theta_0, \\theta_1)}\\). Check empirically that standard deviation of these draws decrease proportionally to \\(\\frac{1}{\\sqrt{N}}\\) .\n__Using a high enough number for N, compute optimal values for \\(\\theta_0\\) and \\(\\theta_1\\). What is the matching value for the objective function converted into an equivalent stream of determinstic consumption ? That is if V is the approximated value computed above, what is \\(\\bar{c}\\in \\R\\) such that $ V= _{t=0}^T ^t U({c})$ ?__\nUsing a high enough number for N, make contour plots of lifetime rewards as a function of θ_0 and θ_1. Ideally, represent lines with \\(1\\%\\) consumption loss, \\(5\\%\\) and \\(10\\%\\) deterministic consumption loss w.r.t. to maximum.\n\n\nLearning to save\nWe now focus on the number of steps it takes to optimize \\(\\theta_0\\), \\(\\theta_1\\).\nImplement a function ∇(θ::Vector; N=1000)::Vector which computes the gradient of the objective w.r.t. θ==[θ_0,θ_1]. (You need to use automatic differentiation, otherwise you might get incorrect results).\nImplement a gradient descent algorithm to maximize \\(\\Xi^N(\\theta_0, \\theta_1)\\) using learning rate \\(\\lambda \\in ]0,1]\\). Stop after a predefined number of iterations. Compare convergence speed for different values of \\(\\lambda\\) and plot them on the \\(\\theta_0, \\theta_1\\) plan. How many steps does it take to enter the 1% error zone? The 5% and the 10% error zone?\nEven for big N, the evaluated value of ∇ are stochastic, and always slightly inaccurate. In average, they are non-biased and the algorithm converges in expectation (it fluctuates around the maximum). This is called the stochastic gradient method.\nWhat are the values of \\(N\\) and \\(\\lambda\\) which minimize the number of iterations before reaching the target zones (at 1%, 2%, etc…)? How many simulations periods does it correspond to? Would you say it is realistic that consumers learn from their own experience?"
  },
  {
    "objectID": "tutorials/1_1_Julia_Basics.html",
    "href": "tutorials/1_1_Julia_Basics.html",
    "title": "Julia Basics",
    "section": "",
    "text": "developped at MIT on top of opensource technologies\n\nlinux / git / llvm\n\nsyntax inspired by Matlab but:\n\nmore consistent\nlots of features from high level languages\n\neverything is JIT-compiled\n\ninterpreted vs compiled treadeoff\n-&gt; very fast\nmost of the base library is written in Julia\n\nopensource/free + vibrant community\n\nSome useful links from QuantEcon:\n\nJulia cheatsheet\nJulia-Matlab comparison\nJulia essentials\nVectors, arrays and matrices\n\nExcellent resources at: julialang - checkout JuliaAcademy, it’s free",
    "crumbs": [
      "Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/1_1_Julia_Basics.html#additional-exercises",
    "href": "tutorials/1_1_Julia_Basics.html#additional-exercises",
    "title": "Julia Basics",
    "section": "Additional Exercises",
    "text": "Additional Exercises\nTaken from QuantEcon’s Julia Essentials and Vectors, Arrays, and Matrices lectures.\n\nConsider the polynomial \\[p(x) = \\sum_{i=0}^n a_0 x^0\\] Using enumerate, write a function p such that p(x, coeff) computes the value of the polynomial with coefficients coeff evaluated at x.\nWrite a function solve_discrete_lyapunov that solves the discrete Lyapunov equation \\[S = ASA' + \\Sigma \\Sigma'\\] using the iterative procedure \\[S_0 = \\Sigma \\Sigma'\\] \\[S_{t+1} = A S_t A' + \\Sigma \\Sigma'\\] taking in as arguments the \\(n \\times n\\) matrix \\(A\\), the \\(n \\times k\\) matrix \\(\\Sigma\\), and a number of iterations.",
    "crumbs": [
      "Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/2_solow_correction.html",
    "href": "tutorials/2_solow_correction.html",
    "title": "Convergence: Solow Model",
    "section": "",
    "text": "Solow Model\nA representative agent uses capital \\(k_t\\) to produce \\(y_t\\) using the following production function:\n\\[y_t = k_t^{\\alpha}\\]\nHe chooses to consume an amount \\(c_t \\in ]0, y_t]\\) and invests what remains:\n\\[i_t = y_t - c_t\\]\nHe accumulates capital \\(k_t\\) according to:\n\\[k_{t+1} = \\left( 1-\\delta \\right) k_{t} + i_{t}\\]\nwhere \\(\\delta\\) is the depreciation rate and \\(i_t\\) is the amount invested.\nThe goal of the representative agent is to maximize:\n\\[\\sum_{t\\geq 0} \\beta^t U(c_t)\\]\nwhere \\(U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}\\) and \\(\\beta&lt;1\\) is the discount factor.\nFor now, we ignore the objective and assume that the saving rate \\(s=\\frac{c_t}{y_t}\\) is constant over time.\nCreate a NamedTuple to hold parameter values \\(\\beta=0.96\\), \\(\\delta=0.1\\), \\(\\alpha=0.3\\), \\(\\gamma=4\\).\n\np = (;β=0.96, δ=0.1, α=0.3, γ=4.0)\n\n(β = 0.96, δ = 0.1, α = 0.3, γ = 4.0)\n\n\nWrite down the formula of function \\(f\\) such that \\(k_{t+1}\\): \\(k_{t+1} = f(k_t)\\).\n\\[k_{t+1}= (1-\\delta) k_t + (1-s) k_t^{\\alpha}\\]\nDefine a function f(k::Float64, p::NamedTuple)::Float64 to represent \\(f\\) for a given calibration\n\n# function f(k::Float64, p::NamedTuple)\n\n# we added a keyword argument to specify the saving rate\nfunction f(k, p; s=0.2)\n\n    (;α, δ) = p # keyword unpacking syntax\n\n    val = k*(1-δ) + s*k^α\n\n    return val \nend\n\nf (generic function with 2 methods)\n\n\n\nf(0.1, p)\n\n0.1902374467254545\n\n\nWrite a function simulate(k0::Float64, T::Int, p::NamedTuple)::Vector{Float64} to compute the simulation over T periods starting from initial capital level k0.\n\nfunction simulate(k0, T, p; s=0.5)\n\n    sim = [k0]\n\n    for i ∈ 1:T    # same as for i in ... or for i=...\n        # in Julia, intervals contain the lower and upper bound\n        k1 = f(k0, p; s=s)\n\n        # add new value to simulation vector\n        push!(sim, k1)\n\n        k0 = k1\n    end\n\n    return sim\nend\n\nsimulate (generic function with 1 method)\n\n\n\nsim = simulate(0.5, 100, p;)\n\n101-element Vector{Float64}:\n 0.2\n 0.30340677254400195\n 0.4129080792968781\n 0.525003373019628\n 0.6373491155565568\n 0.7483344417178636\n 0.856841626057419\n 0.9620988898971137\n 1.0635841029776287\n 1.1609587708409257\n ⋮\n 2.6876186883735818\n 2.6879113388853018\n 2.6881835130861362\n 2.6884366430712516\n 2.6886720608576273\n 2.688891005366757\n 2.689094628921639\n 2.6892840032916374\n 2.6894601253165105\n\n\n\n# sometimes, we want to avoid using too much memory\n# in that case we try to do as many inplace operations as possible\n\nfunction simulate_preallocate(k, T, p; s=0.2)\n    v = zeros(T) # allocates memore\n    v[1] = k\n    for t = 1:(T-1)\n        k0 = v[t]\n        v[t+1] = f(k0, p; s=s)\n    end\n    return v\nend\n\nsimulate_preallocate (generic function with 1 method)\n\n\n\n@time simulate(0.2, 1000000, p);\n@time simulate_preallocate(0.2, 1000000, p);\n\n  0.067737 seconds (14 allocations: 9.781 MiB)\n  0.055613 seconds (2 allocations: 7.629 MiB)\n\n\nMake a nice plot to illustrate the convergence. Do we get convergence from any initial level of capital?\n\npl= plot(simulate(0.5, 100, p;); label=\"baseline\", title=\"Convergence of Solow Model\")\nplot!(pl, simulate(6.0, 100, p;); label=\"high initial capital\")\nplot!(pl, simulate(0.5, 100, p;s=0.2); label=\"lower saving rate\")\n\nSuppose you were interested in using f to compute the steady-state. What would you propose to measure convergence speed? To speed-up convergence? Implement these ideas.\n\nfunction steady_state(p; s=0.2)\n    sim = simulate(0.1, 1000, p)\n    return sim[end]\nend\n\nsteady_state (generic function with 1 method)\n\n\n\nsteady_state(p)\n\n2.691800385264708"
  },
  {
    "objectID": "tutorials/homework.html",
    "href": "tutorials/homework.html",
    "title": "Homework",
    "section": "",
    "text": "Name:\nSurname:\nAfter completing the following questions, send the edited notebook to pwinant@escp.eu\nYou are allowed to use any online available resource, and to you use any julia package, but not to copy/paste any code.\nAlso, don’t forget to comment your code and take any initiative you find useful and/or interesting. Also don’t wait to be prompted to interpret your own results (or attempts).\nHave fun ! ⌣"
  },
  {
    "objectID": "tutorials/homework.html#part-i-a-simple-cge-model",
    "href": "tutorials/homework.html#part-i-a-simple-cge-model",
    "title": "Homework",
    "section": "Part I : A simple CGE model",
    "text": "Part I : A simple CGE model\nThe goal of this exercise consists in solving a simple CGE model using Julia.\nModel is taken from Handbook of Computable General Equilibrium Modeling. 1\n\nThe data and the social accounting matrix\nIn a CGE, the data about the economy’s exchanges is compiled in one single “Social Accounting Matrix”. It is essentially a matrix representation of the national accounts that can be arbitrary refined based on data availability.\nIn this course work we consider the following simplified version. All items are in million euros.\n\n\n\nSAM\nBRD\nMLK\nCAP\nLAB\nHOH\n\n\n\n\nBRD\n\n\n\n\n15\n\n\nMLK\n\n\n\n\n35\n\n\nCAP\n5\n20\n\n\n\n\n\nLAB\n10\n15\n\n\n\n\n\nHOH\n\n\n25\n25\n\n\n\n\nIn this table, there are 5 sectors:\n\nTwo Goods: BRD and MLK\nTwo Factors: CAP and LAB (capital and labour)\nOne Agent: HOH (Households)\n\nEach of the rows feature a decomposition of the spending associated with each column.\nFor instance, the value of the production in the BRD-producing sector (column BRD) is decomposed between the payment to capital (5) and the payment to labor (10). The full capital income goes to households (25). Households spend their income between BRD (15) and MLK (35.)\n\nCreate a DataFrame representing the SAM matrix.\n\n\nusing DataFrames\n\nSAM = ...\n\n\n\nNotations\nTo shorten notations while preserving readibility we associate conventional indices to each sector: \\(i \\in \\text{BRD,MLK}\\), \\(j\\in \\text{BRD,MLK}\\), \\(h \\in \\text{CAP,LAB}\\)).\nTo model the economy, we will assume that endowments \\(F_h\\) are in fixed quantities (so \\(F_{BRD}\\) and \\(F_{MLK}\\) are two fixed parameters.)\nWe use the following notations:\n\n\\(X_i\\) household consumption of good i\n\\(F_{h,j}\\) the h-th factor input by firm j\n\\(Z_{j}\\) output of the j-th good\n\\(p^x_{i}\\) demand price of the i-th good\n\\(p^z_{j}\\) supply price of the i-th good\n\\(p^f_{h}\\) the h-th factor price\n\nOur goal is to endogenize all these variables (there are 14 of them), by constructing a system of equations corresponding to the model we describe below.\nWe also denote by a zero exponent, the variables at the initial steady-state. For instance, \\(X_{BRD}^0\\) is the household consumption of good \\(BRD\\). We assume that the SAM corresponds to values observed at the initial steady-state.\n\n\nConsumers\nHouseholds are endowed with capital (resp labour) in quantity \\(F_{CAP}\\) resp \\(F_{LAB}\\)) that they sell to the goods producing firms at price \\(p^f_{CAP}\\) (resp \\(p^f_{LAB}\\)).\nGiven prices of goods \\(p^x_i\\), households maximize utility of consumption \\[U = \\prod_i X_i^{\\alpha_i}\\] where \\(\\alpha_i\\) is the share parameter in the utility function (with \\(\\sum_i \\alpha_i=1\\))\n\nWrite down the budget constraint of households. Show that household’s demand can be written as function of total gdp and price of goods as in \\[X_i  = \\alpha_i\\frac{ \\sum_h  p^f_h F_h}{p^x_i}\\].\nAccording to the SAM matrix, what is the value of gdp in the model? How can we calibrate the values of \\(\\alpha_i\\)?\n\n\n\nProducers\nWe assume that producers of final goods, use the following production function:\n\\[Z_j   = b_j \\prod_h F_{h,j}^{\\beta_{h,j}}\\]\nwhere \\(\\beta_{h,j}\\) is the share parameter in production function (\\(\\sum_h \\beta_{h,j}=1\\)) and \\(b_j\\) is a scale parameter. Firms rent labour and capital at market price and are perfectly competitive.\n\nShow that factor demand function can be written as: \\[ F_{h,j} = \\beta_{h,j} \\frac{p^z_j Z_j }{p^f_h} \\]\nHow can we calibrate \\(\\beta_{h,j}\\) from the data? How can we recover \\(b_j\\)?\nWithout loss of generality we can assume that all prices are equal to 1 in the initial equilibrium. 2 What are then the values for endowments \\(F_h\\) consistent with the SAM? How can we calibrate the value of \\(b_j\\)?\n\n\n\nEquilibrium\nIn equilibrium we have the following conditions:\n\ndemand prices of the i-th good must match supply price of the same good: \\[p^x_{i} = p^z_{i}\\]\ngood markets must clear: \\[ X_i   = Z_i \\]\nfactor markets must clear: \\[ \\sum_j F_{h,j} = F_h \\]\nthe price of labour is taken as the numeraire \\(p_{LAB} = 1\\) (the other prices are endogenous).\n\n\n\n\nCalibration and solution\n\nDefine a structure model, containing the parameters of the model that can be directly calibrated from the data (i.e. \\(\\alpha_i\\),\\(\\beta_{ij}\\), \\(b_j\\), \\(F_h\\) …).\n\n\n## Choose the structure your prefer\n\n\nCreate an initial set of values vars0 for the model variables to be solved for. These variables should be consistent with the Social Accounting Matrix.\n\n\n## one suggestion consists in defining a named tuple of the form:\n\nvars0 = (;\n    X=(;\n        BRD=...,\n        MLK=...\n    ),\n    F=(;\n        LAB=(;\n            BRD=...,\n            MLK=...\n        )\n    ),\n    ...\n)\n\n\nWrite a function which packs the set of variable values into a julia vector. Write the reverse function to unpack a vector into a structure of the same type as vars. Check that these two functions are consistent (i.e. pack(unpack(u)) == u).\n\n\n# input argument should be the same as vars0\nfunction pack(vars)::Vector\n\n    # [X_BRD, X_MLK, F_LAB_BRD, ...]\nend\n\n\n# output argument should be the same as vars0\nfunction unpack(u::AbstractVector)\n\n    \nend\n\nunpack (generic function with 1 method)\n\n\n\nWrite a function residuals which check whether the model equations are satisfied for a particular guess v0. The output can be a simple vector. Check the residuals for the initial guess vars0. They should be zeros is if the initial values and the model equations have been carefully chosen.\n\n\nfunction residuals(model, v)::Vector\n\n    # write down the model equations below\n    \n\nend\n\n\nAdd a new method to the function residuals, which now takes a vector as argument (using the unpack function). Compute the jacobian of that new function. Verify that the rank of this jacobian equals the number of variables to be determined. If the number of equations is greater than the number of variables to be determined, can you identify one equation that can be safely removed?.\n\n\n# Your code\n\n\nWrite a method to solve the system in all the endogenous variables when the initial guess is close to the initial equilibrium. You can use your method of choice (using your own code, using nlsolve, …). To ease convergence, you might want to take into account information about variable boundaries (all variables are positive).\n\n\n# Your code\n\n\n\nApplications\n\nA massive migration doubles the endowment of labour, which is now equal to 50. Assuming structural parameters are unchanged, Compute the new equilibrium using the same code as before. Comment on the changes in quantities and prices. Could you have predicted them?\n\n\n# Your code\n\n\nWhat happens if the labour share in both production sectors is reduced by 1%? (i.e. \\(\\beta_{LAB,j}\\) is reduced by 1%).\n\n\n# Your code"
  },
  {
    "objectID": "tutorials/homework.html#part-ii---endogenous-exit",
    "href": "tutorials/homework.html#part-ii---endogenous-exit",
    "title": "Homework",
    "section": "Part II - Endogenous Exit",
    "text": "Part II - Endogenous Exit\n\nWarmup: discretization of an AR1\n{The following code, taken from Quantecon.jl approximates an AR1 process \\[y_t = \\mu + \\rho (y_{t-1}-\\mu) + \\epsilon_t\\] (where \\(\\nu\\) is the standard deviation of normal process \\(\\epsilon\\)), using a finite markov chain with \\(N\\) different values.\nThe output is a transition matrix \\(P_{ij}\\) and a vector containing discretized values \\(y_1, y_2, ... y_p\\)\n\n# uncomment the following line if \"SpecialFunctions\" is not on your system\n# import Pkg; Pkg.add(\"SpecialFunctions\")\n\nusing SpecialFunctions: erfc\n\nstd_norm_cdf(x::T) where {T &lt;: Real} = 0.5 * erfc(-x/sqrt(2))\nstd_norm_cdf(x::Array{T}) where {T &lt;: Real} = 0.5 .* erfc(-x./sqrt(2))\n\nfunction tauchen(N::Integer, ρ::T1, σ::T2, μ=zero(promote_type(T1, T2)), n_std::T3=3) where {T1 &lt;: Real, T2 &lt;: Real, T3 &lt;: Real}\n    # Get discretized space\n    a_bar = n_std * sqrt(σ^2 / (1 - ρ^2))\n    y = range(-a_bar, stop=a_bar, length=N)\n    d = y[2] - y[1]\n\n    # Get transition probabilities\n    Π = zeros(promote_type(T1, T2), N, N)\n    for row = 1:N\n        # Do end points first\n        Π[row, 1] = std_norm_cdf((y[1] - ρ*y[row] + d/2) / σ)\n        Π[row, N] = 1 - std_norm_cdf((y[N] - ρ*y[row] - d/2) / σ)\n\n        # fill in the middle columns\n        for col = 2:N-1\n            Π[row, col] = (std_norm_cdf((y[col] - ρ*y[row] + d/2) / σ) -\n                           std_norm_cdf((y[col] - ρ*y[row] - d/2) / σ))\n        end\n    end\n\n    yy = y .+ μ / (1 - ρ) # center process around its mean (wbar / (1 - rho)) in new variable\n\n    (;transitions=Π, values=yy)\n\nend\n\ntauchen (generic function with 3 methods)\n\n\n\nTake \\(\\rho=0.95, \\mu=0.1, \\nu=0.1\\). Approximate the AR1 with 200 discrete states, using the tauchen function above. Check that all rows sum to 1. Compute and plot the steady-state distribution.\n\n\n# you code here\n\n\nSimulate the evolution of the above markov chain for 10000 periods. Compute the mean and standard deviation of all the simulated values. Compare these moments with the results obtained in the preceding question.\n\n\n# you code here\n\n\n\nA firm’s problem\nConsider a firm whose productivity \\(y_t\\) is exogenous and evolves according to the markov chain above.\nProfits are given by \\(\\pi(y_t) = y_t\\).\nAt the start of each period, the firm decides whether to remain in operation and receive current profit \\(\\pi_t\\) or to exit and receive scrap value \\(s&gt;0\\) for the sale of physical assets.\nTime is discounted using interest rate, that is \\(\\beta=\\frac{1}{1+r} \\in [0,1[\\).\nThe following code creates a parameterization of the firm’s problem:\n\n\"Creates an instance of the firm exit model.\"\nfunction create_exit_model(;\n    n=200, # productivity grid size\n    ρ=0.95, μ=0.1, ν=0.1, # persistence, mean and volatility\n    β=0.98, s=100.0 # discount factor and scrap value\n    )\n    mc = tauchen(n, ρ, ν, μ)\n    z_vals, Q = mc.state_values, mc.p\n    return (; n, z_vals, Q, β, s)\nend\n\ncreate_exit_model\n\n\nSolving the model consists in finding the value of the firm in each productivity state \\(y\\) as well as the optimal decision (quit or continue in each of these states).\nThe value of the firm satisfies the Bellman equation:\n\\[V(y) = \\max_{c(y)} \\pi(y) + \\beta \\begin{cases} \\mathbb{E}\\left[ V(y') | y\\right] & \\text{if c(y)=true}\\\\ s & \\text{otherwise}\\end{cases}\\]\nGiven that we have discretized the AR1 as a markov chain, one can also write it\n\\[V(y_i) = \\max_{c(y_i)} \\pi(y_i) + \\beta \\begin{cases} \\sum_j P_{ij} V(y_j)  & \\text{if } c(y_i)=\\text{true} \\\\ s & \\text{otherwise} \\end{cases}\\]\nFor this problem it is natural to encode a guess for the value function \\((V(y_i))_i\\) as a Vector{Float64} of length \\(p\\) and the vector of controls \\((c(y_i))_i\\) as a Vector{Float64} of length \\(p\\).\n\nWrite a function bellman_step(model, V) which takes in the model parameterization, as well as a guess for the next period value function (\\(V()\\) on the right hand side of the Bellman equation) and returns a new updated value (\\(V()\\) on the left).\n\n\n# you code here\n\n\nApply the bellman_step function until convergence, starting from any initial guess.3 Show that convergence speed is linear. Plot the resulting value function and optimal choices. Comment.\n\n\n# you code here"
  },
  {
    "objectID": "tutorials/homework.html#footnotes",
    "href": "tutorials/homework.html#footnotes",
    "title": "Homework",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can checkout the GAMS code for the model here. This can be useful to check that you get all equations right. GAMS is a modeling software, routinely used for CGEs.↩︎\nthis can be done by renormalizing the unit of the quantities.↩︎\nThis is known as the value function iteration algorithm.↩︎"
  },
  {
    "objectID": "tutorials/3_Optimization.html",
    "href": "tutorials/3_Optimization.html",
    "title": "Optimization",
    "section": "",
    "text": "In this tutorial you will learn to code and use common optimization algorithms for static models.",
    "crumbs": [
      "Tutorials",
      "Optimization"
    ]
  },
  {
    "objectID": "tutorials/3_Optimization.html#profit-optimization-by-a-monopolist",
    "href": "tutorials/3_Optimization.html#profit-optimization-by-a-monopolist",
    "title": "Optimization",
    "section": "Profit optimization by a monopolist",
    "text": "Profit optimization by a monopolist\nA monopolist produces quantity \\(q\\) of goods X at price \\(p\\). Its cost function is \\(c(q) = 0.5 + q (1-qe^{-q})\\)\nThe consumer’s demand for price \\(p\\) is \\(x(p)=2 e^{-0.5 p}\\) (constant elasticity of demand to price).\nWrite down the profit function of the monopolist and find the optimal production (if any). Don’t use any library except for plotting.\n\nusing Plots\n\n\nx(p) = 2exp(-0.5p)\nc(q) = 0.5+q*(1-q*exp(-q))\nxi(p) = -2log(p/2)\npi(q) = -2log(q/2)\nπ_q(q) = xi(q)*q - c(q)\nπ_p(p) = p*x(p) - c(x(p))\n\nπ_p (generic function with 1 method)\n\n\n\n# plot w.r.t. q\n\nqvec = range(0.001, 5; length=100)\n\n# π_q_vec = [π_q(q) for q in qvec]\n\n# vectorized call of π_q: ( . means vectorized or element-by-element  )\nπ_q_vec = π_q.(qvec);\n\n# plot the result\nplot(qvec, π_q_vec; xlabel=\"quantity\", ylabel=\"profit\")\n\nUndefVarError: UndefVarError: `plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nUndefVarError: `plot` not defined in `Main`\n\nSuggestion: check for spelling errors or missing imports.\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ ~/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:11\n\n\n\n# plot w.r.t. p\n\npvec = range(0.001, 5; length=100)\n\nπ_p_vec = π_p.(pvec);\n\n# plot the result\nplot(pvec, π_p_vec; ylabel=\"\\$\\\\pi(p)\\$\", xlabel=\"\\$p\\$\")\n\nUndefVarError: UndefVarError: `plot` not defined in `Main`\nSuggestion: check for spelling errors or missing imports.\nUndefVarError: `plot` not defined in `Main`\n\nSuggestion: check for spelling errors or missing imports.\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ ~/Teaching/polytechnique/eco309/tutorials/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W5sZmlsZQ==.jl:8\n\n\n\n# this function repeats twice the call (x(p))\nπ_p(p) = p*x(p) - c(x(p))\n\n# let's create an intermediary variable\n# \n# option1: create a function block\n\n# option2: create a block\nπ_p(p) = begin\n    xx = x(p)\n    p*xx - c(xx)\nend\n\nπ_p (generic function with 1 method)\n\n\n\n# let' s implement newton raphson\n\nWe will perform the optimization with respect o the quantities But first we need to compute the hessian of the profit function\n\npi(q) = -2log(q/2)\np_1(q) = -2/q\np_2(q) = 2/q^2\n\nc(q) = 0.5 + q*(1-q*exp(-q))\n\n\nc_1(q) = 1-2*q*exp(-q) + q^2*exp(-q)\nc_2(q) = -exp(-q)*(q^2-4q+2)\n\nπ_q(q) = pi(q)*q - c(q)\nπ_1(q) = p_1(q)*q + pi(q) - c_1(q)\nπ_2(q) = p_2(q)*q + 2p_1(q) - c_2(q)\n\nπ_2 (generic function with 1 method)\n\n\n\nf(x) = 1-x^2-x^3\nf_1(x) = -2x-3x^2\nf_2(x) = -2-6x\n\nf_2 (generic function with 1 method)\n\n\n\n\"\"\"\nτ_ϵ: criterium for optimality\nτ_η: criterium for progress\n\"\"\"\nfunction newton_raphson(f,f_p,f_pp,x0; T=10,τ_ϵ=1e-10,τ_η=1e-10, verbose=true)\n\n    for t=1:T\n\n        f1 = f_p(x0)\n        f2 = f_pp(x0)\n        δ = -f1/f2 # newton step\n        x1 = x0 + δ # new guess\n\n        # with both criteria\n        ϵ = abs(f1)\n        if ϵ&lt;τ_ϵ\n            return x1 # problem solved\n        end\n        η = abs(δ)\n        if η&lt;τ_η\n            error(\"No progress, problem not solved.\")\n        end\n\n        # ternary operator :    a ? b : c     -&gt; if a do b otherwise do c\n        verbose ? println(\"t=$t ; ϵ=$ϵ ;  η=$η\") : nothing\n\n        # # with only η assuming the algorithm converges\n        # η = abs(δ)\n        # if η&lt;τ_η\n        #     return x1 # xolution found\n        # end\n\n\n        x0 = x1\n    end\n\n    error(\"No convergence\")\n    \nend\n\nnewton_raphson\n\n\n\nnewton_raphson(f,f_1, f_2, 0.1)\n\nt=1 ; ϵ=0.23 ;  η=0.08846153846153847\nt=2 ; ϵ=0.023476331360946748 ;  η=0.01134543894766943\nt=3 ; ϵ=0.0003861569547458862 ;  η=0.00019296673655466368\nt=4 ; ϵ=1.1170848424967084e-7 ;  η=5.585423276574868e-8\n\n\n3.2343297114061484e-29\n\n\n\n# let's try it on the profit function\n\nqmax = newton_raphson(π_q, π_1, π_2, 1.0; T=50, τ_η=1e-12)\n\nt=1 ; ϵ=1.245826197708667 ;  η=0.5261358226465827\nt=2 ; ϵ=0.33021007594907004 ;  η=0.082229160985156\nt=3 ; ϵ=0.02037843952968632 ;  η=0.005743880464955386\nt=4 ; ϵ=7.798058772645611e-5 ;  η=2.214853592309787e-5\nt=5 ; ϵ=1.1422293111351678e-9 ;  η=3.2443264163392274e-10\n\n\n0.5618593676638843\n\n\n\nπ_max = π_q(qmax)\n\n# plot w.r.t. q\n\nqvec = range(0.001, 5; length=100)\n\n# π_q_vec = [π_q(q) for q in qvec]\n\n# vectorized call of π_q: ( . means vectorized or element-by-element  )\nπ_q_vec = π_q.(qvec);\n\n# plot the result\npl = plot(qvec, π_q_vec; xlabel=\"quantity\", ylabel=\"profit\")\nscatter!(pl, [qmax], [π_max])\npl\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nm0 = eps(Float64)\ne0 = sqrt(m0)\n\n1.4901161193847656e-8\n\n\n\nπ_1(0.1) , (π_q(0.1+e0) - π_q(0.1))/e0\n\n(4.792091008999541, 3.1633835211396217)\n\n\n\n1 + m0/2 == 1\n\ntrue\n\n\n\nm0/2 + (1 + -1)\n\n1.1102230246251565e-16",
    "crumbs": [
      "Tutorials",
      "Optimization"
    ]
  },
  {
    "objectID": "tutorials/3_Optimization.html#constrained-optimization",
    "href": "tutorials/3_Optimization.html#constrained-optimization",
    "title": "Optimization",
    "section": "Constrained optimization",
    "text": "Constrained optimization\nConsider the function \\(f(x,y) = 1-(x-0.5)^2 -(y-0.3)^2\\).\nUse Optim.jl to maximize \\(f\\) without constraint. Check you understand diagnostic information returned by the optimizer.\n\nusing Optim\n\n\nf(x,y) = 1 - (x-0.5)^2 - (y-0.3)^2\n\nf (generic function with 1 method)\n\n\n\nf(1,2)\n\n-2.1399999999999997\n\n\n\n# define another function that takes a vector as input\nf(x::Vector) = f( x[1], x[2] )\n\nf (generic function with 2 methods)\n\n\n\nf( [0.1, 0.2 ])\n\n0.83\n\n\n\nx0 = [0.4, 0.7]\n\n2-element Vector{Float64}:\n 0.4\n 0.7\n\n\n\nOptim.optimize # fully qualified name\n\noptimize    # was imported by using Optim\n\noptimize (generic function with 67 methods)\n\n\n\nresult = optimize( u -&gt; -f(u) , x0)\nresult\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     -1.000000e+00\n\n * Found with\n    Algorithm:     Nelder-Mead\n\n * Convergence measures\n    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    26\n    f(x) calls:    55\n\n\n\nOptim.minimizer(result)\n\n2-element Vector{Float64}:\n 0.49998195944788515\n 0.2999575964340524\n\n\n\nresult = optimize( u -&gt; -f(u) , x0, LBFGS(); autodiff = :forward)\nresult\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     -1.000000e+00\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 4.00e-01 ≰ 0.0e+00\n    |x - x'|/|x'|          = 8.00e-01 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.70e-01 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 1.70e-01 ≰ 0.0e+00\n    |g(x)|                 = 0.00e+00 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    1\n    f(x) calls:    3\n    ∇f(x) calls:   3\n\n\n\nOptim.minimizer(result)\n\n2-element Vector{Float64}:\n 0.5\n 0.3\n\n\nNow, consider the constraint \\(x&lt;0.3\\) and maximize \\(f\\) under this new constraint.\n\nx1 = [0.2, 0.2] # choose a valid initial guess\nresult = optimize( u -&gt; -f(u) , [-Inf, -Inf], [0.3, Inf], x1 )\nresult\n\n * Status: success\n\n * Candidate solution\n    Final objective value:     -9.600000e-01\n\n * Found with\n    Algorithm:     Fminbox with L-BFGS\n\n * Convergence measures\n    |x - x'|               = 2.00e-07 ≰ 0.0e+00\n    |x - x'|/|x'|          = 6.66e-07 ≰ 0.0e+00\n    |f(x) - f(x')|         = 7.81e-08 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 8.14e-08 ≰ 0.0e+00\n    |g(x)|                 = 2.00e-10 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    3\n    f(x) calls:    60\n    ∇f(x) calls:   60\n\n\n\nOptim.minimizer(result)\n\n2-element Vector{Float64}:\n 0.2999999998\n 0.3000000000007093\n\n\nReformulate the problem as a root finding problem with lagrangians. Write the complementarity conditions.\nSolve using NLSolve.jl",
    "crumbs": [
      "Tutorials",
      "Optimization"
    ]
  },
  {
    "objectID": "tutorials/3_Optimization.html#consumption-optimization",
    "href": "tutorials/3_Optimization.html#consumption-optimization",
    "title": "Optimization",
    "section": "Consumption optimization",
    "text": "Consumption optimization\nA consumer has preferences \\(U(c_1, c_2)\\) over two consumption goods \\(c_1\\) and \\(c_2\\).\nGiven a budget \\(I\\), consumer wants to maximize utility subject to the budget constraint \\(p_1 c_1 + p_2 c_2 \\leq I\\).\nWe choose a Stone-Geary specification where\n\\(U(c_1, c_2)=\\beta_1 \\log(c_1-\\gamma_1) + \\beta_2 \\log(c_2-\\gamma_2)\\)\nWrite the Karush-Kuhn-Tucker necessary conditions for the problem.\nVerify the KKT conditions are sufficient for optimality.\nDerive analytically the demand functions, and the shadow price.\nInterpret this problem as a complementarity problem and solve it using NLSolve.\nProduce some nice graphs with isoutility curves, the budget constraint and the optimal choice.",
    "crumbs": [
      "Tutorials",
      "Optimization"
    ]
  },
  {
    "objectID": "tutorials/7_discrete_dynamic_programming.html",
    "href": "tutorials/7_discrete_dynamic_programming.html",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "A worker’s employment dynamics is described by the stochastic matrix\n\\[P = \\begin{bmatrix}\n1-\\alpha & \\alpha \\\\\n\\beta & 1-\\beta\n\\end{bmatrix}\\]\nwith \\(\\alpha\\in(0,1)\\) and \\(\\beta\\in (0,1)\\). First line corresponds to employment, second line to unemployment.\nWhich is the stationary equilibrium? (choose any value for \\(\\alpha\\) and \\(\\beta\\))\n\nα = 0.3\nβ = 0.5\nγ = 0.2\nP = [\n    (1-α) α/2 α/2;\n    β/2  (1-β) β/2;\n    γ/2 γ/2 (1-γ);\n]\n\n3×3 Matrix{Float64}:\n 0.7   0.15  0.15\n 0.25  0.5   0.25\n 0.1   0.1   0.8\n\n\n\nμ0 = [1.0, 1.0, 1.0]/3\nμ0' * (P^10)\n\n1×3 adjoint(::Vector{Float64}) with eltype Float64:\n 0.322581  0.193548  0.483871\n\n\n\nfunction solve_steady_state(P; T=100)\n    n = size(P,1)\n    μ0 = (ones(n)/n)'\n    for t in 1:T\n        μ1 = μ0*P\n        η = maximum(abs, μ1 - μ0)\n        if η&lt;1e-10\n            return μ1'\n        end\n        μ0 = μ1\n    end\n    error(\"No convergence\")\nend\n\nsolve_steady_state (generic function with 1 method)\n\n\n\nsolve_steady_state(P)\n\n3-element Vector{Float64}:\n 0.3225806452587981\n 0.19354838711776282\n 0.48387096762343945\n\n\n\n# using linear algrebra\n\nusing LinearAlgebra: I\n# I is the identity operator\n\nM = P' - I\n\n# modify last line of M\n\nM[end,:] .= 1.0\nM\n\n# define right hand side\nr = zeros(size(M,1))\n\nr[end] = 1.0\n\nM \\ r\n\n3-element Vector{Float64}:\n 0.32258064516129037\n 0.19354838709677416\n 0.48387096774193555\n\n\n\nM = P' - I\n\n# modify last line of M\n\nM1 = [\n    M ;  # concatenate along first dimension\n    ones(size(M,1))'  # ' to turn the vector a 1x3 matrix\n]\nM1\n\n# # define right hand side\nr = [zeros(size(M,1)) ; 1]\n\nM1 \\ r\n\n3-element Vector{Float64}:\n 0.32258064516129054\n 0.19354838709677397\n 0.4838709677419355\n\n\nIn the long run, what will the the fraction \\(p\\) of time spent unemployed? (Denote by \\(X_m\\) the fraction of dates were one is unemployed)\nIllustrate this convergence by generating a simulated series of length 10000 starting at \\(X_0=1\\). Plot \\(X_m-p\\) against \\(m\\). (Take \\(\\alpha=\\beta=0.1\\)).",
    "crumbs": [
      "Tutorials",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "tutorials/7_discrete_dynamic_programming.html#markov-chains",
    "href": "tutorials/7_discrete_dynamic_programming.html#markov-chains",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "A worker’s employment dynamics is described by the stochastic matrix\n\\[P = \\begin{bmatrix}\n1-\\alpha & \\alpha \\\\\n\\beta & 1-\\beta\n\\end{bmatrix}\\]\nwith \\(\\alpha\\in(0,1)\\) and \\(\\beta\\in (0,1)\\). First line corresponds to employment, second line to unemployment.\nWhich is the stationary equilibrium? (choose any value for \\(\\alpha\\) and \\(\\beta\\))\n\nα = 0.3\nβ = 0.5\nγ = 0.2\nP = [\n    (1-α) α/2 α/2;\n    β/2  (1-β) β/2;\n    γ/2 γ/2 (1-γ);\n]\n\n3×3 Matrix{Float64}:\n 0.7   0.15  0.15\n 0.25  0.5   0.25\n 0.1   0.1   0.8\n\n\n\nμ0 = [1.0, 1.0, 1.0]/3\nμ0' * (P^10)\n\n1×3 adjoint(::Vector{Float64}) with eltype Float64:\n 0.322581  0.193548  0.483871\n\n\n\nfunction solve_steady_state(P; T=100)\n    n = size(P,1)\n    μ0 = (ones(n)/n)'\n    for t in 1:T\n        μ1 = μ0*P\n        η = maximum(abs, μ1 - μ0)\n        if η&lt;1e-10\n            return μ1'\n        end\n        μ0 = μ1\n    end\n    error(\"No convergence\")\nend\n\nsolve_steady_state (generic function with 1 method)\n\n\n\nsolve_steady_state(P)\n\n3-element Vector{Float64}:\n 0.3225806452587981\n 0.19354838711776282\n 0.48387096762343945\n\n\n\n# using linear algrebra\n\nusing LinearAlgebra: I\n# I is the identity operator\n\nM = P' - I\n\n# modify last line of M\n\nM[end,:] .= 1.0\nM\n\n# define right hand side\nr = zeros(size(M,1))\n\nr[end] = 1.0\n\nM \\ r\n\n3-element Vector{Float64}:\n 0.32258064516129037\n 0.19354838709677416\n 0.48387096774193555\n\n\n\nM = P' - I\n\n# modify last line of M\n\nM1 = [\n    M ;  # concatenate along first dimension\n    ones(size(M,1))'  # ' to turn the vector a 1x3 matrix\n]\nM1\n\n# # define right hand side\nr = [zeros(size(M,1)) ; 1]\n\nM1 \\ r\n\n3-element Vector{Float64}:\n 0.32258064516129054\n 0.19354838709677397\n 0.4838709677419355\n\n\nIn the long run, what will the the fraction \\(p\\) of time spent unemployed? (Denote by \\(X_m\\) the fraction of dates were one is unemployed)\nIllustrate this convergence by generating a simulated series of length 10000 starting at \\(X_0=1\\). Plot \\(X_m-p\\) against \\(m\\). (Take \\(\\alpha=\\beta=0.1\\)).",
    "crumbs": [
      "Tutorials",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "tutorials/7_discrete_dynamic_programming.html#job-search-model",
    "href": "tutorials/7_discrete_dynamic_programming.html#job-search-model",
    "title": "Discrete Dynamic Programming",
    "section": "Job-Search Model",
    "text": "Job-Search Model\nWe want to solve the following model, adapted from McCall.\n\nWhen unemployed in date, a job-seeker\n\nconsumes unemployment benefit \\(c_t = \\underline{c}\\)\nreceives in every date \\(t\\) a job offer \\(w_t\\)\n\n\\(w_t\\) is i.i.d.,\ntakes values \\(w_1, w_2, w_3\\) with probabilities \\(p_1, p_2, p_3\\)\n\nif job-seeker accepts, becomes employed at rate \\(w_t\\) in the next period\nelse he stays unemployed\n\nWhen employed at rate \\(w\\)\n\nworker consumes salary \\(c_t = w\\)\nwith small probability \\(\\lambda&gt;0\\) looses his job:\n\nstarts next period unemployed\n\notherwise stays employed at same rate\n\nObjective: \\(\\max E_0 \\left\\{ \\sum \\beta^t \\log(c_t) \\right\\}\\)\n\nWhat are the states, the controls, the reward of this problem ? Write down the Bellman equation.\nDefine a parameter structure for the model.\nDefine a function value_update(V_U::Vector{Float64}, V_E::Vector{Float64}, x::Vector{Bool}, p::Parameters)::Tuple{Vector, Vector}, which takes in value functions tomorrow and a policy vector and return updated values for today.\nDefine a function policy_eval(x::Vector{Bool}, p::Parameter)::Tuple{Vector, Vector} which takes in a policy vector and returns the value(s) of following this policies forever. You can add relevant arguments to the function.\nDefine a function bellman_step(V_E::Vector, V_U::Vector, p::Parameters)::Tuple{Vector, Vector, Vector} which returns updated values, together with improved policy rules.\nImplement Value Function\nImplement Policy Iteration and compare rates of convergence.\nDiscuss the Effects of the Parameters",
    "crumbs": [
      "Tutorials",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "docs/pushups/1_epidemiology_correction.out.html",
    "href": "docs/pushups/1_epidemiology_correction.out.html",
    "title": "Epidemiology Models",
    "section": "",
    "text": "H.W. Heathcote: epidemiologic models are deterministic models for infectious diseases which are spread by direct person-to-person contact in a population.\nThis kind of models has been used since by a few decades by economist, for matters that have nothing to do with health.\n\ndiffusion of information, or opinions on social medias\nasset prices and fads\nexpectation formation for macroeconomic outcomes (ex: The Epidemiology of Macroeconomic Expectations by Chris Carroll)\n\nEpidemiologic models have two features that depart from standard economic models:\n\nAgent’s behaviour does not take the full system into account, an is irrational in the sense that it isn’t forward looking. Instead, an agent is given a behavioural heuristic.\nThe transitions of the whole system can be determined without solving for complex interactions first.\n\nUnderstanding why these two assumptions are very costly for economists will keep us busy during a big part of the course. Here we just consider two simple models as small programming exercises.\n\nSimple SIR model\nThere is a continuum of agents of mass \\(1\\). Each agent can be either “Susceptible” (S), “Infected” (I) or “Recovered” (R). In each period, one agent meets another agent drawn randomly. During a meeting Susceptible agents who meet an infected agent, will always catch the disease (or the fad) but are not contagious. Infected agents, have a probability \\(\\pi\\) of being recovered. Nothing happens to Recovered agents who meet other people. No distinction is made between recovering as a healthy or a dead person.\nWe’re interested in the evolution in the number infected persons, both the speed of infection and the total amount of infected people in every period.\nWrite down the transition equations for \\(n_I\\), the number of infected people, for \\(n_R\\) the number of recovered people and \\(n_S\\) the number of susceptible people.\nHere are the transitions :\n\n\\(n_{S,t} = n_{S,t-1} (1-n_{I,t-1})\\)\n\\(n_{I,t} = n_{S,t-1} n_{I,t} + n_{I,t-1} (1-\\pi)\\)\n\\(n_{R,t} = n_{I,t-1} \\pi + n_{R,t-1}\\)\n\nCompute the transition function f for the vector state \\(s_t\\) returning \\(s_{t+1}\\).\n\n\"Compute new state from old state `s`\"\nfunction f(s::Vector{Float64}; π=0.1)::Vector{Float64}\n    \n    # comments\n    n_S, n_I, n_R = s\n    # equivalent to \n    # n_S = s[1]\n    # n_I = s[2]\n    # n_R = s[3]\n\n    N_S = n_S*(1-n_I)\n    N_I = n_S*n_I + n_I*(1-π)\n    N_R = n_I*π + n_R\n\n    return [N_S, N_I, N_R]\n\nend\n\nf\n\n\n\nv = f([0.2, 0.5, 0.3])\n\n3-element Vector{Float64}:\n 0.1\n 0.55\n 0.35\n\n\n\n# check that vector v sums to 1\nv[1] + v[2] + v[3] == 1.0\n\ntrue\n\n\n\nsum(v) == 1.0\n\ntrue\n\n\n\n# two kinds of optional arguments:\n\n# named arguments separated by semi-colon\n# like python\nfunction fun(a,b ; pi=3.14, beta=0.96)\n    nothing\nend\n\n# optional argument\nfunction gun(a,b,pi=3)\nend\n\n# equivalent to\nfunction gun(a,b)\nend\nfunction gun(a,b,pi)(1-n_I)\nend\n\n\n### MESSAGE\n\n# use semi-colon for named arguments\n\nCompute the transitions over \\(T\\) periods. Plot the result using Plots.jl. (bonus: check against closed form solution)\n\nfunction simulate(s0; π=0.1, T=20)\n\n    simul = [s0]\n\n    for i ∈ 1:T  # \\in you can also use `in` or `=`\n        s = simul[end]\n        s1 = f(s; π=π)\n        push!(simul, s1)\n    end\n\n    return simul\n\nend\n\nsimulate (generic function with 1 method)\n\n\n\nusing Plots\n\n\ns0 = [0.9, 0.1, 0.0]\nsim = simulate(s0)\n\n21-element Vector{Vector{Float64}}:\n [0.9, 0.1, 0.0]\n [0.81, 0.18000000000000002, 0.010000000000000002]\n [0.6642, 0.3078000000000001, 0.028000000000000004]\n [0.45975923999999996, 0.48146076000000015, 0.05878000000000001]\n [0.23840320689257752, 0.6546707171074226, 0.10692607600000004]\n [0.08232760847550455, 0.7452792438137533, 0.17239314771074232]\n [0.02097055068588577, 0.7321083772219967, 0.24692107209211767]\n [0.005617834853790308, 0.6742502553318925, 0.32013190981431733]\n [0.0018300082692097876, 0.6106130563832839, 0.38755693534750657]\n [0.0007125813267409158, 0.5506691776874244, 0.44861824098583497]\n ⋮\n [8.931225220166544e-5, 0.40197066326707453, 0.5979400244807241]\n [5.341134694628574e-5, 0.3618094978456225, 0.6381370908074316]\n [3.408661432839177e-5, 0.32564787279367813, 0.6743180405919939]\n [2.298638088161248e-5, 0.29309418574775714, 0.7068828278713617]\n [1.624920629382846e-5, 0.2637915043475692, 0.7361922464461375]\n [1.196280372112546e-5, 0.23741664031538498, 0.7625713968808944]\n [9.122635052903468e-6, 0.2136778164525147, 0.7863130609124329]\n [7.173330314505884e-6, 0.19231198411200165, 0.8076808425576844]\n [5.793812929032488e-6, 0.17308216521818695, 0.8269120409688845]\n\n\n\n[e[1] for e in sim]; # vector of susceptible people\n\n\npl = plot([e[1] for e in sim]; title=\"Evolution of Disease\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl, [e[2] for e in sim], label=\"\\$n_{I,t}\\$\")\nplot!(pl, [e[3] for e in sim], label=\"\\$n_{R,t}\\$\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns0 = [0.99, 0.01, 0.0]\nsim1 = simulate(s0;)\npl = plot([e[1] for e in sim1]; title=\"Evolution of Disease\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl, [e[2] for e in sim1], label=\"\\$n_{I,t}\\$\")\nplot!(pl, [e[3] for e in sim1], label=\"\\$n_{R,t}\\$\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now assume a Susceptible person who meets an infected one has a probability \\(\\mu\\) of catching the disease. Update the transition function \\(f\\) and write a function simulate of \\(\\mu\\), \\(\\pi\\) which returns the simulation. Compare different values of the parameters. How would you interpret “social distancing”? How would you choose parameters \\(\\Pi\\) and \\(\\mu\\)\n\n\"Compute new state from old state `s`\"\nfunction f(s::Vector{Float64}; π=0.1, μ=1.0)::Vector{Float64}\n    \n    # comments\n    n_S, n_I, n_R = s\n    # equivalent to \n    # n_S = s[1]\n    # n_I = s[2]\n    # n_R = s[3]\n\n    N_S = n_S - n_S*n_I*μ\n    N_I = n_S*n_I*μ + n_I*(1-π)\n    N_R = n_I*π + n_R\n\n    return [N_S, N_I, N_R]\n\nend\n\nf\n\n\n\nfunction simulate(s0; π=0.1, T=20, μ=1.0)\n\n    simul = [s0]\n\n    for i ∈ 1:T  # \\in you can also use `in` or `=`\n        s = simul[end]\n        s1 = f(s; π=π, μ=μ)\n        push!(simul, s1)\n    end\n\n    return simul\n\nend\n\nsimulate (generic function with 1 method)\n\n\n\ns0 = [0.99, 0.01, 0.0]\nsim0 = simulate(s0;)\npl0 = plot([e[1] for e in sim0]; title=\"Evolution with \\$\\\\mu=1.0\\$\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl0, [e[2] for e in sim0], label=\"\\$n_{I,t}\\$\")\nplot!(pl0, [e[3] for e in sim0], label=\"\\$n_{R,t}\\$\")\n\ns0 = [0.99, 0.01, 0.0]\nsim1 = simulate(s0; μ=0.5)\npl1 = plot([e[1] for e in sim1]; title=\"Evolution  \\$\\\\mu=0.5\\$\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl1, [e[2] for e in sim1], label=\"\\$n_{I,t}\\$\")\nplot!(pl1, [e[3] for e in sim1], label=\"\\$n_{R,t}\\$\")\n\nplot(pl0, pl1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Spatial SIR model\nWe now consider another version of the model where agents evolve in the space \\(\\mathcal{S}=[0,1]\\times[0,1]\\). There are \\(N\\in\\mathbb{N}\\) agent. At any date, each agent \\(n \\in [0,1]\\) is located at \\((x_n,y_n)\\in \\mathcal{S}\\).\nEach agent moves follows a random walk bounded by \\(\\mathcal{S}\\): \\[x_t = \\min(\\max( x_{t-1} + \\epsilon_t, 0), 1)\\] \\[y_t = \\min(\\max( y_{t-1} + \\eta_t, 0), 1)\\] where \\(\\epsilon_t\\) and \\(\\eta_t\\) are both normally distributed with standard deviation \\(\\sigma\\).\nAt any date, the individual state of an agent is \\(s_t=(x_t, y_t, h_t)\\) where \\(h_t\\) is either “S”, “I” or “R”. \\(v_t\\) denotes the states of all agents (for instance \\(v_t=(s_{n,t})_n\\). The health status of each agent is updated in the following way:\n\nAgents \\(R\\) stay \\(R\\).\nAgents \\(I\\) have probability \\(\\pi\\) to become \\(R\\). They stay \\(I\\) otherwise.\nAn agent of type \\(S\\) in position \\((x,y)\\) has a probability \\(prob(x,y,S)\\) to be infected that is \\(\\mu\\) if there is another infected agent within a radius \\(r&gt;0\\).\n\nDefine a type Agent, which holds the type of an agent. The state of the whole system will be held in a Vector[Agent] type.\nWrite a function spatial_transition(S::Vector{Agent})::Vector{Agent} to compute the transition of the positions. Write another function random_guess(T=100) which simulates for \\(T\\) periods in order to find a good initial guess.\nWrite a function show_population to plot all agents with different colors for different health status.\nWrite a function evolve(S::Vector[Agent])::Vector[Agent] which takes the population in a given state and returns the same population with updated health status.\nWrite a function simulate(S0::Vector[Agent], k=1) to simulate the economy starting from an initially random position with k infected individuals. The returned object should be of type Vector[Vector[Agent]].\nCompute statistics along a simulated path for \\(n_I\\), \\(n_S\\), \\(n_R\\). Plot and compare with the basic SIR model\n\n\nAdditional questions\nHave fun by trying to answer one of these questions:\n\nchange probability of infection so that it depends on the number of infected people in the vincinity (with some suitable functional formulation for)\ncompute an animation of the transition\ncompute an interactive visualisation (with Interact.jl if available)"
  },
  {
    "objectID": "pushups/2_Optimization_pushups.html",
    "href": "pushups/2_Optimization_pushups.html",
    "title": "Optimization Pushups",
    "section": "",
    "text": "The spirit of this simple tutorial consists in learning how to write simple solution algorithms. For each algorithm, test that it works, using simple test functions whose solution is known.\nWrite a function fixed_point(f::Function, x0::Float64) which computes the fixed point of f starting from initial point x0.\nWrite a function bisection(f::Function, a::Float64, b::Float64) which computes the zero of function f within (a,b) using a bisection method.\nWrite a function golden(f::Function, a::Float64, b::Float64) which computes a zero of function f within (a,b) using a golden ratio method.\nWrite a function zero_newton(f::Function, x0::Float64) which computes the zero of function f starting from initial point x0.\nAdd an option zero_newton(f::Function, x0::Float64, backtracking=true) which computes the zero of function f starting from initial point x0 using backtracking in each iteration.\nWrite a function min_gd(f::Function, x0::Float64) which computes the minimum of function f using gradient descent. Assume f returns a scalar and a gradient.\nWrite a function min_nr(f::Function, x0::Float64) which computes the minimum of function f using Newton-Raphson method. Assume f returns a scalar, a gradient, and a hessian.\nWrite a method zero_newton(f::Function, x0::Vector{Float64}) which computes the zero of a vector valued function f starting from initial point x0.\nAdd an method zero_newton(f::Function, x0::Vector{Float64}, backtracking=true) which computes the zero of function f starting from initial point x0 using backtracking in each iteration.\nAdd a method zero_newton(f::Function, x0::Vector{Float64}, backtracking=true, lb=Vector{Float64}) which computes the zero of function f starting from initial point x0 taking complementarity constraint into account x&gt;=lb using the Fischer-Burmeister method.",
    "crumbs": [
      "Pushups",
      "Optimization Pushups"
    ]
  },
  {
    "objectID": "pushups/1_epidemiology_correction.html",
    "href": "pushups/1_epidemiology_correction.html",
    "title": "Epidemiology Models",
    "section": "",
    "text": "H.W. Heathcote: epidemiologic models are deterministic models for infectious diseases which are spread by direct person-to-person contact in a population.\nThis kind of models has been used since by a few decades by economist, for matters that have nothing to do with health.\n\ndiffusion of information, or opinions on social medias\nasset prices and fads\nexpectation formation for macroeconomic outcomes (ex: The Epidemiology of Macroeconomic Expectations by Chris Carroll)\n\nEpidemiologic models have two features that depart from standard economic models:\n\nAgent’s behaviour does not take the full system into account, an is irrational in the sense that it isn’t forward looking. Instead, an agent is given a behavioural heuristic.\nThe transitions of the whole system can be determined without solving for complex interactions first.\n\nUnderstanding why these two assumptions are very costly for economists will keep us busy during a big part of the course. Here we just consider two simple models as small programming exercises.\n\nSimple SIR model\nThere is a continuum of agents of mass \\(1\\). Each agent can be either “Susceptible” (S), “Infected” (I) or “Recovered” (R). In each period, one agent meets another agent drawn randomly. During a meeting Susceptible agents who meet an infected agent, will always catch the disease (or the fad) but are not contagious. Infected agents, have a probability \\(\\pi\\) of being recovered. Nothing happens to Recovered agents who meet other people. No distinction is made between recovering as a healthy or a dead person.\nWe’re interested in the evolution in the number infected persons, both the speed of infection and the total amount of infected people in every period.\nWrite down the transition equations for \\(n_I\\), the number of infected people, for \\(n_R\\) the number of recovered people and \\(n_S\\) the number of susceptible people.\nHere are the transitions :\n\n\\(n_{S,t} = n_{S,t-1} (1-n_{I,t-1})\\)\n\\(n_{I,t} = n_{S,t-1} n_{I,t} + n_{I,t-1} (1-\\pi)\\)\n\\(n_{R,t} = n_{I,t-1} \\pi + n_{R,t-1}\\)\n\nCompute the transition function f for the vector state \\(s_t\\) returning \\(s_{t+1}\\).\n\n\"Compute new state from old state `s`\"\nfunction f(s::Vector{Float64}; π=0.1)::Vector{Float64}\n    \n    # comments\n    n_S, n_I, n_R = s\n    # equivalent to \n    # n_S = s[1]\n    # n_I = s[2]\n    # n_R = s[3]\n\n    N_S = n_S*(1-n_I)\n    N_I = n_S*n_I + n_I*(1-π)\n    N_R = n_I*π + n_R\n\n    return [N_S, N_I, N_R]\n\nend\n\nf\n\n\n\nv = f([0.2, 0.5, 0.3])\n\n3-element Vector{Float64}:\n 0.1\n 0.55\n 0.35\n\n\n\n# check that vector v sums to 1\nv[1] + v[2] + v[3] == 1.0\n\ntrue\n\n\n\nsum(v) == 1.0\n\ntrue\n\n\n\n# two kinds of optional arguments:\n\n# named arguments separated by semi-colon\n# like python\nfunction fun(a,b ; pi=3.14, beta=0.96)\n    nothing\nend\n\n# optional argument\nfunction gun(a,b,pi=3)\nend\n\n# equivalent to\nfunction gun(a,b)\nend\nfunction gun(a,b,pi)(1-n_I)\nend\n\n\n### MESSAGE\n\n# use semi-colon for named arguments\n\nErrorException: syntax: \"(1 - n_I)\" is not a valid function argument name around /home/pablo/Teaching/polytechnique/eco309_past/pushups/1_epidemiology.ipynb:16\nsyntax: \"(1 - n_I)\" is not a valid function argument name around /home/pablo/Teaching/polytechnique/eco309_past/pushups/1_epidemiology.ipynb:16\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ ~/Teaching/polytechnique/eco309_past/pushups/1_epidemiology.ipynb:16\n\n\nCompute the transitions over \\(T\\) periods. Plot the result using Plots.jl. (bonus: check against closed form solution)\n\nfunction simulate(s0; π=0.1, T=20)\n\n    simul = [s0]\n\n    for i ∈ 1:T  # \\in you can also use `in` or `=`\n        s = simul[end]\n        s1 = f(s; π=π)\n        push!(simul, s1)\n    end\n\n    return simul\n\nend\n\nsimulate (generic function with 1 method)\n\n\n\nusing Plots\n\n\ns0 = [0.9, 0.1, 0.0]\nsim = simulate(s0)\n\n21-element Vector{Vector{Float64}}:\n [0.9, 0.1, 0.0]\n [0.81, 0.18000000000000002, 0.010000000000000002]\n [0.6642, 0.3078000000000001, 0.028000000000000004]\n [0.45975923999999996, 0.48146076000000015, 0.05878000000000001]\n [0.23840320689257752, 0.6546707171074226, 0.10692607600000004]\n [0.08232760847550455, 0.7452792438137533, 0.17239314771074232]\n [0.02097055068588577, 0.7321083772219967, 0.24692107209211767]\n [0.005617834853790308, 0.6742502553318925, 0.32013190981431733]\n [0.0018300082692097876, 0.6106130563832839, 0.38755693534750657]\n [0.0007125813267409158, 0.5506691776874244, 0.44861824098583497]\n ⋮\n [8.931225220166544e-5, 0.40197066326707453, 0.5979400244807241]\n [5.341134694628574e-5, 0.3618094978456225, 0.6381370908074316]\n [3.408661432839177e-5, 0.32564787279367813, 0.6743180405919939]\n [2.298638088161248e-5, 0.29309418574775714, 0.7068828278713617]\n [1.624920629382846e-5, 0.2637915043475692, 0.7361922464461375]\n [1.196280372112546e-5, 0.23741664031538498, 0.7625713968808944]\n [9.122635052903468e-6, 0.2136778164525147, 0.7863130609124329]\n [7.173330314505884e-6, 0.19231198411200165, 0.8076808425576844]\n [5.793812929032488e-6, 0.17308216521818695, 0.8269120409688845]\n\n\n\n[e[1] for e in sim]; # vector of susceptible people\n\n\npl = plot([e[1] for e in sim]; title=\"Evolution of Disease\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl, [e[2] for e in sim], label=\"\\$n_{I,t}\\$\")\nplot!(pl, [e[3] for e in sim], label=\"\\$n_{R,t}\\$\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns0 = [0.99, 0.01, 0.0]\nsim1 = simulate(s0;)\npl = plot([e[1] for e in sim1]; title=\"Evolution of Disease\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl, [e[2] for e in sim1], label=\"\\$n_{I,t}\\$\")\nplot!(pl, [e[3] for e in sim1], label=\"\\$n_{R,t}\\$\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe now assume a Susceptible person who meets an infected one has a probability \\(\\mu\\) of catching the disease. Update the transition function \\(f\\) and write a function simulate of \\(\\mu\\), \\(\\pi\\) which returns the simulation. Compare different values of the parameters. How would you interpret “social distancing”? How would you choose parameters \\(\\Pi\\) and \\(\\mu\\)\n\n\"Compute new state from old state `s`\"\nfunction f(s::Vector{Float64}; π=0.1, μ=1.0)::Vector{Float64}\n    \n    # comments\n    n_S, n_I, n_R = s\n    # equivalent to \n    # n_S = s[1]\n    # n_I = s[2]\n    # n_R = s[3]\n\n    N_S = n_S - n_S*n_I*μ\n    N_I = n_S*n_I*μ + n_I*(1-π)\n    N_R = n_I*π + n_R\n\n    return [N_S, N_I, N_R]\n\nend\n\nf\n\n\n\nfunction simulate(s0; π=0.1, T=20, μ=1.0)\n\n    simul = [s0]\n\n    for i ∈ 1:T  # \\in you can also use `in` or `=`\n        s = simul[end]\n        s1 = f(s; π=π, μ=μ)\n        push!(simul, s1)\n    end\n\n    return simul\n\nend\n\nsimulate (generic function with 1 method)\n\n\n\ns0 = [0.99, 0.01, 0.0]\nsim0 = simulate(s0;)\npl0 = plot([e[1] for e in sim0]; title=\"Evolution with \\$\\\\mu=1.0\\$\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl0, [e[2] for e in sim0], label=\"\\$n_{I,t}\\$\")\nplot!(pl0, [e[3] for e in sim0], label=\"\\$n_{R,t}\\$\")\n\ns0 = [0.99, 0.01, 0.0]\nsim1 = simulate(s0; μ=0.5)\npl1 = plot([e[1] for e in sim1]; title=\"Evolution  \\$\\\\mu=0.5\\$\", xlabel=\"\\$t\\$\", label=\"\\$n_{S,t}\\$\")\n# \"\\$t\\$\" is needed because in \"$t$\" the dollar sign would be interpreted as interpolation command\nplot!(pl1, [e[2] for e in sim1], label=\"\\$n_{I,t}\\$\")\nplot!(pl1, [e[3] for e in sim1], label=\"\\$n_{R,t}\\$\")\n\nplot(pl0, pl1)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Spatial SIR model\nWe now consider another version of the model where agents evolve in the space \\(\\mathcal{S}=[0,1]\\times[0,1]\\). There are \\(N\\in\\mathbb{N}\\) agent. At any date, each agent \\(n \\in [0,1]\\) is located at \\((x_n,y_n)\\in \\mathcal{S}\\).\nEach agent moves follows a random walk bounded by \\(\\mathcal{S}\\): \\[x_t = \\min(\\max( x_{t-1} + \\epsilon_t, 0), 1)\\] \\[y_t = \\min(\\max( y_{t-1} + \\eta_t, 0), 1)\\] where \\(\\epsilon_t\\) and \\(\\eta_t\\) are both normally distributed with standard deviation \\(\\sigma\\).\nAt any date, the individual state of an agent is \\(s_t=(x_t, y_t, h_t)\\) where \\(h_t\\) is either “S”, “I” or “R”. \\(v_t\\) denotes the states of all agents (for instance \\(v_t=(s_{n,t})_n\\). The health status of each agent is updated in the following way:\n\nAgents \\(R\\) stay \\(R\\).\nAgents \\(I\\) have probability \\(\\pi\\) to become \\(R\\). They stay \\(I\\) otherwise.\nAn agent of type \\(S\\) in position \\((x,y)\\) has a probability \\(prob(x,y,S)\\) to be infected that is \\(\\mu\\) if there is another infected agent within a radius \\(r&gt;0\\).\n\nDefine a type Agent, which holds the type of an agent. The state of the whole system will be held in a Vector[Agent] type.\nWrite a function spatial_transition(S::Vector{Agent})::Vector{Agent} to compute the transition of the positions. Write another function random_guess(T=100) which simulates for \\(T\\) periods in order to find a good initial guess.\nWrite a function show_population to plot all agents with different colors for different health status.\nWrite a function evolve(S::Vector[Agent])::Vector[Agent] which takes the population in a given state and returns the same population with updated health status.\nWrite a function simulate(S0::Vector[Agent], k=1) to simulate the economy starting from an initially random position with k infected individuals. The returned object should be of type Vector[Vector[Agent]].\nCompute statistics along a simulated path for \\(n_I\\), \\(n_S\\), \\(n_R\\). Plot and compare with the basic SIR model\n\n\nAdditional questions\nHave fun by trying to answer one of these questions:\n\nchange probability of infection so that it depends on the number of infected people in the vincinity (with some suitable functional formulation for)\ncompute an animation of the transition\ncompute an interactive visualisation (with Interact.jl if available)"
  },
  {
    "objectID": "coursework/coursework_1.html",
    "href": "coursework/coursework_1.html",
    "title": "Coursework 2025 - Deep Learning",
    "section": "",
    "text": "This exercise is inspired from Individual learning about consumption by Todd Allen and Chris Carroll link and from Deep Learning for Solving Economic models by Maliar, Maliar and Winant link\nWe consider the following consumption saving problem. An agent receives random income \\(y_t = \\exp(\\epsilon_t)\\) where \\(\\epsilon_t\\sim \\mathcal{N}(\\sigma)\\) (\\(\\sigma\\) is the standard deviation.)\nConsumer starts the period with available income \\(w_t\\). The law of motion for available income is:\n\\[w_t = \\exp(\\epsilon_t) + (w_{t-1}-c_{t-1}) r\\]\nwhere consumption \\(c_t \\in ]0,w_t]\\) is chosen in each period in order to maximize:\n\\[E_t \\sum_{t=0}^T \\beta^t U(c_t)\\]\ngiven initial available income \\(w_0\\).\nIn the questions below, we will use the following calibration:\n\n\\(\\beta = 0.9\\)\n\\(\\sigma = 0.1\\)\n\\(T=100\\)\n\\(U(x) = \\frac{x^{1-\\gamma}}{1-\\gamma}\\) with \\(\\gamma=2\\)\n\\(w_0 = 1.1\\) (alternatively, consider values 0.5 and 1)\n\nThe theoretical solution to this problem is a concave function \\(\\varphi\\) such that \\(\\varphi(x)\\in ]0,x]\\) and \\(\\forall t,  c_t=\\varphi(w_t)\\). Qualitatively, agents accumulate savings, up to a certain point (a buffer stock), beyond which wealth is not increasing any more (in expectation).\nCarroll and Allen have noticed that the true solution can be approximated very well by a simple rule:\n\\(\\psi(x) = \\min(x, \\theta_0 + \\theta_1 (x - \\theta_0) )\\)\nThe main question they ask in the aforementioned paper is whether it is realistic that agents would learn good values of \\(\\theta_0\\) and \\(\\theta_1\\) by observing past experiences.\nWe would like to examine this result by checking convergence of speed of stochastic gradient algorithm.\n\n\nDefine a NamedTuple to hold the parameter values\nDefine simple rule fonction consumption(w::Number, θ_0::Number, θ_1::Number, p::NamedTuple) which compute consumption using a simple rule. What is the meaning of \\(\\theta_0\\) and \\(\\theta_1\\)? Make a plot in the space \\(w,c\\), including consumption rule and the line where \\(w_{t+1} = w_t\\).\n(remark for later: Number type is compatible with ForwardDiff.jl 😉)\n# your code...\nWrite a function lifetime_reward(w_0::Number, θ_0::Number, θ_1::Number, p::NamedTuple) which computes one realization of \\(\\sum \\beta^t U(c_t)\\) for initial wealth w_0 and simple rule θ_0, θ_1. Mathematically, we denote it by \\(\\xi(\\omega; \\theta_0, \\theta_1)\\), where \\(\\omega\\) represents the succession of random income draws.\n# your code...\nWrite a function expected_lifetime_reward(w_0::Number, θ_0::Number, θ_1::Number,  p::NamedTuple; N=1000) which computes expected lifetime reward using N Monte-Carlo draws. Mathematically, we write it \\(\\Xi^{N}(\\theta_0, \\theta_1) =\\frac{1}{N} \\sum_1^N {\\xi(\\omega_N; \\theta_0, \\theta_1)}\\). Check empirically that standard deviation of these draws decrease proportionally to \\(\\frac{1}{\\sqrt{N}}\\) .\n# your code...\n__Using a high enough number for N, compute optimal values for \\(\\theta_0\\) and \\(\\theta_1\\). What is the matching value for the objective function converted into an equivalent stream of deterministic consumption ? That is if V is the approximated value computed above, what is \\(\\bar{c}\\in \\R\\) such that $ V= _{t=0}^T ^t U({c})$ ?__\n# your code...\nUsing a high enough number for N, make contour plots of lifetime rewards as a function of θ_0 and θ_1. Ideally, represent lines with \\(1\\%\\) consumption loss, \\(5\\%\\) and \\(10\\%\\) deterministic consumption loss w.r.t. to maximum.\n# your code...\n\n\n\nWe now focus on the number of steps it takes to optimize \\(\\theta_0\\), \\(\\theta_1\\).\nImplement a function ∇(θ::Vector; N=1000)::Vector which computes the gradient of the objective w.r.t. θ==[θ_0,θ_1]. (You need to use automatic differentiation, otherwise you might get incorrect results).\n# your code...\nImplement a gradient descent algorithm to maximize \\(\\Xi^N(\\theta_0, \\theta_1)\\) using learning rate \\(\\lambda \\in ]0,1]\\). Stop after a predefined number of iterations. Compare convergence speed for different values of \\(\\lambda\\) and plot them on the \\(\\theta_0, \\theta_1\\) plan. How many steps does it take to enter the 1% error zone? The 5% and the 10% error zone?\n# your code...\nEven for big N, the evaluated value of ∇ are stochastic, and always slightly inaccurate. In average, they are non-biased and the algorithm converges in expectation (it fluctuates around the maximum). This is called the stochastic gradient method.\n# your code...\nWhat are the values of \\(N\\) and \\(\\lambda\\) which minimize the number of iterations before reaching the target zones (at 1%, 2%, etc…)? How many simulations periods does it correspond to? Would you say it is realistic that consumers learn from their own experience?\n# your code...",
    "crumbs": [
      "Coursework",
      "Coursework 2025 - Deep Learning"
    ]
  },
  {
    "objectID": "coursework/coursework_1.html#learning-the-consumption-rule",
    "href": "coursework/coursework_1.html#learning-the-consumption-rule",
    "title": "Coursework 2025 - Deep Learning",
    "section": "",
    "text": "This exercise is inspired from Individual learning about consumption by Todd Allen and Chris Carroll link and from Deep Learning for Solving Economic models by Maliar, Maliar and Winant link\nWe consider the following consumption saving problem. An agent receives random income \\(y_t = \\exp(\\epsilon_t)\\) where \\(\\epsilon_t\\sim \\mathcal{N}(\\sigma)\\) (\\(\\sigma\\) is the standard deviation.)\nConsumer starts the period with available income \\(w_t\\). The law of motion for available income is:\n\\[w_t = \\exp(\\epsilon_t) + (w_{t-1}-c_{t-1}) r\\]\nwhere consumption \\(c_t \\in ]0,w_t]\\) is chosen in each period in order to maximize:\n\\[E_t \\sum_{t=0}^T \\beta^t U(c_t)\\]\ngiven initial available income \\(w_0\\).\nIn the questions below, we will use the following calibration:\n\n\\(\\beta = 0.9\\)\n\\(\\sigma = 0.1\\)\n\\(T=100\\)\n\\(U(x) = \\frac{x^{1-\\gamma}}{1-\\gamma}\\) with \\(\\gamma=2\\)\n\\(w_0 = 1.1\\) (alternatively, consider values 0.5 and 1)\n\nThe theoretical solution to this problem is a concave function \\(\\varphi\\) such that \\(\\varphi(x)\\in ]0,x]\\) and \\(\\forall t,  c_t=\\varphi(w_t)\\). Qualitatively, agents accumulate savings, up to a certain point (a buffer stock), beyond which wealth is not increasing any more (in expectation).\nCarroll and Allen have noticed that the true solution can be approximated very well by a simple rule:\n\\(\\psi(x) = \\min(x, \\theta_0 + \\theta_1 (x - \\theta_0) )\\)\nThe main question they ask in the aforementioned paper is whether it is realistic that agents would learn good values of \\(\\theta_0\\) and \\(\\theta_1\\) by observing past experiences.\nWe would like to examine this result by checking convergence of speed of stochastic gradient algorithm.\n\n\nDefine a NamedTuple to hold the parameter values\nDefine simple rule fonction consumption(w::Number, θ_0::Number, θ_1::Number, p::NamedTuple) which compute consumption using a simple rule. What is the meaning of \\(\\theta_0\\) and \\(\\theta_1\\)? Make a plot in the space \\(w,c\\), including consumption rule and the line where \\(w_{t+1} = w_t\\).\n(remark for later: Number type is compatible with ForwardDiff.jl 😉)\n# your code...\nWrite a function lifetime_reward(w_0::Number, θ_0::Number, θ_1::Number, p::NamedTuple) which computes one realization of \\(\\sum \\beta^t U(c_t)\\) for initial wealth w_0 and simple rule θ_0, θ_1. Mathematically, we denote it by \\(\\xi(\\omega; \\theta_0, \\theta_1)\\), where \\(\\omega\\) represents the succession of random income draws.\n# your code...\nWrite a function expected_lifetime_reward(w_0::Number, θ_0::Number, θ_1::Number,  p::NamedTuple; N=1000) which computes expected lifetime reward using N Monte-Carlo draws. Mathematically, we write it \\(\\Xi^{N}(\\theta_0, \\theta_1) =\\frac{1}{N} \\sum_1^N {\\xi(\\omega_N; \\theta_0, \\theta_1)}\\). Check empirically that standard deviation of these draws decrease proportionally to \\(\\frac{1}{\\sqrt{N}}\\) .\n# your code...\n__Using a high enough number for N, compute optimal values for \\(\\theta_0\\) and \\(\\theta_1\\). What is the matching value for the objective function converted into an equivalent stream of deterministic consumption ? That is if V is the approximated value computed above, what is \\(\\bar{c}\\in \\R\\) such that $ V= _{t=0}^T ^t U({c})$ ?__\n# your code...\nUsing a high enough number for N, make contour plots of lifetime rewards as a function of θ_0 and θ_1. Ideally, represent lines with \\(1\\%\\) consumption loss, \\(5\\%\\) and \\(10\\%\\) deterministic consumption loss w.r.t. to maximum.\n# your code...\n\n\n\nWe now focus on the number of steps it takes to optimize \\(\\theta_0\\), \\(\\theta_1\\).\nImplement a function ∇(θ::Vector; N=1000)::Vector which computes the gradient of the objective w.r.t. θ==[θ_0,θ_1]. (You need to use automatic differentiation, otherwise you might get incorrect results).\n# your code...\nImplement a gradient descent algorithm to maximize \\(\\Xi^N(\\theta_0, \\theta_1)\\) using learning rate \\(\\lambda \\in ]0,1]\\). Stop after a predefined number of iterations. Compare convergence speed for different values of \\(\\lambda\\) and plot them on the \\(\\theta_0, \\theta_1\\) plan. How many steps does it take to enter the 1% error zone? The 5% and the 10% error zone?\n# your code...\nEven for big N, the evaluated value of ∇ are stochastic, and always slightly inaccurate. In average, they are non-biased and the algorithm converges in expectation (it fluctuates around the maximum). This is called the stochastic gradient method.\n# your code...\nWhat are the values of \\(N\\) and \\(\\lambda\\) which minimize the number of iterations before reaching the target zones (at 1%, 2%, etc…)? How many simulations periods does it correspond to? Would you say it is realistic that consumers learn from their own experience?\n# your code...",
    "crumbs": [
      "Coursework",
      "Coursework 2025 - Deep Learning"
    ]
  },
  {
    "objectID": "coursework/coursework_1.html#bonus",
    "href": "coursework/coursework_1.html#bonus",
    "title": "Coursework 2025 - Deep Learning",
    "section": "(Bonus)",
    "text": "(Bonus)\nIf you want to go further, you can try to answer one of the following two questions.\nUse a deeplearning library (for instance Lux.jl) to find the optimal simple rule\nImplement that All-in-one expectation operator to learn the optimal rule (check the notebook on quantecon)",
    "crumbs": [
      "Coursework",
      "Coursework 2025 - Deep Learning"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#main-approaches",
    "href": "slides/session_perturbation/index.html#main-approaches",
    "title": "Perturbation",
    "section": "Main approaches",
    "text": "Main approaches\n\nManual\nFinite Differences\nSymbolic Differentiation\nAutomatic Differentiation",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#manual-differentiation",
    "href": "slides/session_perturbation/index.html#manual-differentiation",
    "title": "Perturbation",
    "section": "Manual Differentiation",
    "text": "Manual Differentiation\n\nTrick:\n\nnever use \\(\\frac{d}{dx} \\frac{u(x)}{v(x)} = \\frac{u'(x)v(x)-u(x)v'(x)}{v(x)^2}\\)\nuse instead \\[\\frac{d}{dx} {u(x)v(x)} = {u'(x)v(x)+u(x)v'(x)}\\] and \\[\\frac{d}{dx} u(x) = -\\frac{u^{\\prime}}{u(x)^2}\\]\n\nJust kidding…",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#finite-differences",
    "href": "slides/session_perturbation/index.html#finite-differences",
    "title": "Perturbation",
    "section": "Finite Differences",
    "text": "Finite Differences\n\nChoose small \\(\\epsilon&gt;0\\), typically \\(\\sqrt{ \\textit{machine eps}}\\)\nForward Difference scheme:\n\n\\(f'(x) \\approx \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}\\)\nprecision: \\(o(\\epsilon)\\)\nbonus: if \\(f(x+\\epsilon)\\) can compute \\(f(x)-f(x-\\epsilon)\\) instead (Backward)\n\nCentral Difference scheme:\n\n\\(f'(x) \\approx \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2\\epsilon}\\)\naverage of forward and backward\nprecision: \\(o(\\epsilon^2)\\)",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#finite-differences-higher-order",
    "href": "slides/session_perturbation/index.html#finite-differences-higher-order",
    "title": "Perturbation",
    "section": "Finite Differences: Higher order",
    "text": "Finite Differences: Higher order\n\nCentral formula: \\[\\begin{aligned}\nf''(x) & \\approx & \\frac{f'(x)-f'(x-\\epsilon)}{\\epsilon} \\approx \\frac{(f(x+\\epsilon))-f(x))-(f(x)-f(x-\\epsilon))}{\\epsilon^2}  \\\\ & = & \\frac{f(x+\\epsilon)-2f(x)+f(x-\\epsilon)}{\\epsilon^2}\n\\end{aligned}\\]\n\nprecision: \\(o(\\epsilon)\\)\n\nGeneralizes to higher order but becomes more and more innacurate",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#symbolic-differentiation",
    "href": "slides/session_perturbation/index.html#symbolic-differentiation",
    "title": "Perturbation",
    "section": "Symbolic Differentiation",
    "text": "Symbolic Differentiation\n\nmanipulate the tree of algebraic expressions\n\nimplements various simplification rules\n\nrequires mathematical expression\ncan produce mathematical insights\nsometimes inaccurate:\n\ncf: \\(\\left(\\frac{1+u(x)}{1+v(x)}\\right)^{100}\\)",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#julia-packages",
    "href": "slides/session_perturbation/index.html#julia-packages",
    "title": "Perturbation",
    "section": "Julia Packages:",
    "text": "Julia Packages:\n\nLots of packages\nFiniteDiff.jl, FiniteDifferences.jl, SparseDiffTools.jl\n\ncareful implementation of finite diff\n\nSymEngine.jl\n\nfast symbolic calculation\n\nSymbolics.jl\n\nfast, pure Julia\nless complete than SymEngine",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#automatic-differentiation",
    "href": "slides/session_perturbation/index.html#automatic-differentiation",
    "title": "Perturbation",
    "section": "Automatic Differentiation",
    "text": "Automatic Differentiation\n\ndoes not provide mathematical insights but solves the other problems\ncan differentiate any piece of code\ntwo flavours\n\nforward accumulation\nreverse accumulation",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#automatic-rewrite-source-code-transform",
    "href": "slides/session_perturbation/index.html#automatic-rewrite-source-code-transform",
    "title": "Perturbation",
    "section": "Automatic rewrite: source code transform",
    "text": "Automatic rewrite: source code transform\nfunction f(x::Float64)\n    a = x + 1\n    b = x^2\n    c = sin(a) + a + b\nend",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#automatic-rewrite-source-code-transform-1",
    "href": "slides/session_perturbation/index.html#automatic-rewrite-source-code-transform-1",
    "title": "Perturbation",
    "section": "Automatic rewrite: source code transform",
    "text": "Automatic rewrite: source code transform\nfunction f(x::Float64)\n\n    # x is an argument\n    x_dx = 1.0\n\n    a = x + 1\n    a_dx = x_dx\n\n    b = x^2\n    b_dx = 2*x*x_dx\n\n    t = sin(a)\n    t_x = cos(a)*a_dx\n\n    c = t + b\n    c_x = t_dx + b_dx\n\n    return (c, c_x)\nend",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#dual-numbers-operator-overloading",
    "href": "slides/session_perturbation/index.html#dual-numbers-operator-overloading",
    "title": "Perturbation",
    "section": "Dual numbers: operator overloading",
    "text": "Dual numbers: operator overloading\nstruct DN\n    x::Float64\n    dx::Float64\nend\n\n+(a::DN,b::DN) = DN(a.x+b.x, a.dx+b.dx)\n-(a::DN,b::DN) = DN(a.x-b.x, a.dx-b.dx)\n*(a::DN,b::DN) = DN(a.x*b.x, a.x*b.dx+a.dx*b.x)\n/(a::DN,b::DN) = DN(a.x/b.x, (a.dx*b.x-a.x*b.dx)/b.dx^2)\n\n...\n\nCan you compute f(x) using dual numbers?\n\n\n(it might require some more definitions)",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#compatible-with-control-flow",
    "href": "slides/session_perturbation/index.html#compatible-with-control-flow",
    "title": "Perturbation",
    "section": "Compatible with control flow",
    "text": "Compatible with control flow\nimport ForwardDiff: Dual\n\nx = Dual(1.0, 1.0)\na = 0.5*x\nb = sum([(x)^i/i*(-1)^(i+1) for i=1:5000])\n# compare with log(1+x)\n\ngeneralizes nicely to gradient computations\n\nx = Dual(1.0, 1.0, 0.0)\ny = Dual(1.0, 0.0, 1.0)\nexp(x) + log(y)",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#technical-remark",
    "href": "slides/session_perturbation/index.html#technical-remark",
    "title": "Perturbation",
    "section": "Technical remark",
    "text": "Technical remark\n\nautodiff libraries, use special types and operator overloading to perform operations (like Dual numbers)\nthis relies on Julia duck-typing ability\n\nso don’t specify type arguments for functions you want to autodiff\n\nThis works:\n\nusing ForwardDiff\nf(x) = [x[1] + x[2], x[1]*x[2]]\nForwardDiff.jacobian(f, [0.4, 0.1])\n\nThis doesn’t:\n\nusing ForwardDiff\ng(x::Vector{Float64}) = [x[1] + x[2], x[1]*x[2]]\nForwardDiff.jacobian(g, [0.4, 0.1])",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#forward-accumulation-mode",
    "href": "slides/session_perturbation/index.html#forward-accumulation-mode",
    "title": "Perturbation",
    "section": "Forward Accumulation Mode",
    "text": "Forward Accumulation Mode\n\nForward Accumulation mode: isomorphic to dual number calculation\n\ncompute tree with values and derivatives at the same time\nefficient for \\(f: R^n\\rightarrow R^m\\), with \\(n&lt;&lt;m\\)\n\n(keeps lots of empty gradients when \\(n&gt;&gt;m\\))",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#reverse-accumulation-mode",
    "href": "slides/session_perturbation/index.html#reverse-accumulation-mode",
    "title": "Perturbation",
    "section": "Reverse Accumulation Mode",
    "text": "Reverse Accumulation Mode\n\nReverse Accumulation / Back Propagation\n\nefficient for \\(f: R^n\\rightarrow R^m\\), with \\(m&lt;&lt;n\\)\nrequires data storage (to keep intermediate values)\ngraph / example\n\nVery good for machine learning:\n\n\\(\\nabla_{\\theta} F(x;\\theta)\\) where \\(F\\) can be an objective",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#libraries-for-autodiff",
    "href": "slides/session_perturbation/index.html#libraries-for-autodiff",
    "title": "Perturbation",
    "section": "Libraries for AutoDiff",
    "text": "Libraries for AutoDiff\n\nSee JuliaDiff: http://www.juliadiff.org/\n\nForwardDiff.jl\nReverseDiff.jl\n\nNew approaches:\n\nZygote.jl: source to source\nEnzyme.jl: differentiates llvm code\n\nOther libraries like NLsolve or Optim.jl rely on on the former libraries to perform automatic differentiation automatically.\n\nusing NLSolve\nfunction fun!(F, x)\n    F[1] = (x[1]+3)*(x[2]^3-7)+18\n    F[2] = sin(x[2]*exp(x[1])-1)\nend\nnlsolve(fun!, [0.1, 0.2], autodiff = :forward)",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_perturbation/index.html#the-future-of-ai-deep-learning",
    "href": "slides/session_perturbation/index.html#the-future-of-ai-deep-learning",
    "title": "Perturbation",
    "section": "The future of AI / Deep Learning",
    "text": "The future of AI / Deep Learning\nWhat are the main options for deep learning?\n\ntensorflow, pytorch, jax python interfaces to manipulate tensors\n\ngets compiled to fast code\n\nmojo: right now for inference\n\nWhat features are needed?\n\ncompilation\nvectorization (processor SIMD, GPU)\nautomatic differentiation for training\n\nJulia should be perfecty positioned\n\nBut deep learning frameworks might not have reached critical mass. You can still try:\n\nFlux.jl\nLux.jl",
    "crumbs": [
      "Slides",
      "Perturbation"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#life-of-a-computational-economist",
    "href": "slides/session_convergence/index.html#life-of-a-computational-economist",
    "title": "Convergence of Sequences",
    "section": "Life of a computational economist",
    "text": "Life of a computational economist",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#life-of-a-computational-economist-1",
    "href": "slides/session_convergence/index.html#life-of-a-computational-economist-1",
    "title": "Convergence of Sequences",
    "section": "Life of a computational economist",
    "text": "Life of a computational economist\nVideo\n\n\nWe spend a lot of time waiting for algorithms to converge!\n\n\nsolution 1: program better\n\nsolution 2: better algorithms\n\neven better: understand convergence properties (information about the model)",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#recursive-sequence",
    "href": "slides/session_convergence/index.html#recursive-sequence",
    "title": "Convergence of Sequences",
    "section": "Recursive sequence",
    "text": "Recursive sequence\nConsider a function \\(f: R^n\\rightarrow R^n\\) and a recursive sequence \\((x_n)\\) defined by \\(x_0\\in R^n\\) and \\(x_n = f(x_{n-1})\\).\nWe want to compute a fixed point of \\(f\\) and study its properties.\n  \n\nToday: Some methods for the case \\(n=1\\).\n  \n\n\nAnother day:\n\nthe matrix case: \\(x_n = A^n x_0\\) whre \\(A\\in R^n \\times R^n\\)\nthe finite nonlinear case: \\(x_n = f(x_{n-1})\\)",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#example-growth-model",
    "href": "slides/session_convergence/index.html#example-growth-model",
    "title": "Convergence of Sequences",
    "section": "Example: growth model",
    "text": "Example: growth model\n\n\n\nSolow growth model:\n\ncapital accumulation: \\[k_t = (1-\\delta)k_{t-1} + i_{t-1}\\]\nproduction: \\[y_t = k_t^\\alpha\\]\nconsumption: \\[c_t = (1-{\\color{red}s})y_t\\] \\[i_t = s y_t\\]\n\n\n\n\n\n\n\nFor a given value of \\({\\color{red} s}\\in\\mathbb{R}^{+}\\) ( \\({\\color{red} s}\\) is a decision rule) \\[k_{t+1} = f(k_t, {\\color{red} s})\\]\n\nbackward-looking iterations\nSolow hypothesis: saving rate is invariant\n\n\n\nQuestions:\n\nWhat is the steady-state?\nCan we characterize the transition back the steady-state?\nCharacterize the dynamics close to the steady-state?\nwhat is the optimal \\(s\\) ?",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#another-example-linear-new-keynesian-model",
    "href": "slides/session_convergence/index.html#another-example-linear-new-keynesian-model",
    "title": "Convergence of Sequences",
    "section": "Another example: linear new keynesian model",
    "text": "Another example: linear new keynesian model\n\n\n\nBasic New Keynesian model (full derivation if curious )\n\nnew philips curve (PC):\\[\\pi_t = \\beta \\mathbb{E}_t \\pi_{t+1} + \\kappa y_t\\]\ndynamic investment-saving equation (IS):\\[y_t = \\beta \\mathbb{E}_t y_{t+1} - \\frac{1}{\\sigma}(i_t - \\mathbb{E}_t(\\pi_{t+1}) ) - {\\color{green} z_t}\\]\ninterest rate setting (taylor rule): \\[i_t = \\alpha_{\\pi} \\pi_t + \\alpha_{y} y_t\\]\n\n\nSolving the system:\n\nsolution: \\(\\begin{bmatrix}\\pi_t \\\\\\\\ y_t \\end{bmatrix} = {\\color{red} c} z_t\\)\n\n\n\n\n\nforward looking:\n\ntake \\(\\begin{bmatrix}\\pi_{t+1} \\\\\\\\ y_{t+1} \\end{bmatrix} = {\\color{red} {c_n}} z_{t+1}\\)\ndeduce \\(\\begin{bmatrix}\\pi_{t} \\\\\\\\ y_{t} \\end{bmatrix} = {\\color{red} {c_{n+1}}} z_{t}\\)\n\\(\\mathcal{T}: \\underbrace{c_{n}}_{t+1: \\; \\text{tomorrow}} \\rightarrow \\underbrace{c_{n+1}}_{t: \\text{today}}\\) is the time-iteration operator (a.k.a. Coleman operator)\n\n\n\n\n\n\nQuestions:\n\nWhat is the limit to \\(c_{t+1} = \\mathcal{T} c_n\\) ?\nUnder wich conditions (on \\(\\alpha_{\\pi}, \\alpha_y\\)) is it convergent ?\n\ndeterminacy conditions\ninterpretation: does the central bank manage to control inflation expectations?",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#recursive-sequence-2",
    "href": "slides/session_convergence/index.html#recursive-sequence-2",
    "title": "Convergence of Sequences",
    "section": "Recursive sequence (2)",
    "text": "Recursive sequence (2)\n\n\nWait: does a fixed point exist?\n\nwe’re not very concerned by the existence problem here\nwe’ll be happy with local conditions (existence, uniqueness) around a solution\n\nIn theoretical work, there are many fixed-point theorems to choose from.\nFor instance, we can assume there is an interval such that \\(f([a,b])\\subset[a,b]\\). Then we know there exists \\(x\\) in \\([a,b]\\) such that \\(f(x)=x\\). But there can be many such points.",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#example-growth-model-with-multiple-fixed-points",
    "href": "slides/session_convergence/index.html#example-growth-model-with-multiple-fixed-points",
    "title": "Convergence of Sequences",
    "section": "Example: growth model with multiple fixed points",
    "text": "Example: growth model with multiple fixed points\n\nMultiple equilibria in the growth modelIn the growth model, if we change the production function: \\(y=k^{\\alpha}\\) for a nonconvex/nonmonotonic one, we can get multiple fixed points.",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#convergence",
    "href": "slides/session_convergence/index.html#convergence",
    "title": "Convergence of Sequences",
    "section": "Convergence",
    "text": "Convergence\n\nGiven \\(f: R \\rightarrow R\\)\nHow do we characterize behaviour around \\(x\\) such that \\(f(x)=x\\)?\n\n\n\nStability criterium:\n\nif \\(|f^{\\prime}(x)|&gt;1\\): sequence is unstable and will not converge to \\(x\\) except by chance\nif \\(|f^{\\prime}(x)|&lt;1\\): \\(x\\) is a stable fixed point\nif \\(|f^{\\prime}(x)|=1\\): ??? (look at higher order terms, details ↓)",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#section",
    "href": "slides/session_convergence/index.html#section",
    "title": "Convergence of Sequences",
    "section": "",
    "text": "To get the intution about local convergence assume, you have an initial point \\(x_n\\) close to the steady state and consider the following expresion:\n\\(x_{n+1} - x = f(x_n) - f(x) = f^{\\prime}(x) (x_n-x) + o( (x_n-x) )\\)\nIf one sets aside the error term (which one can do with full mathematical rigour), the dynamics for very small perturbations are given by:\n\\(|x_{n+1} - x| = |f^{\\prime}(x)| |x_n-x|\\)\nWhen \\(|f^{\\prime}(x)|&lt;1\\), the distance to the target decreases at each iteration and we have convergence. When \\(|f^{\\prime}(x)|&gt;1\\) there is local divergence.",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#section-1",
    "href": "slides/session_convergence/index.html#section-1",
    "title": "Convergence of Sequences",
    "section": "",
    "text": "What about the case \\(|f^{\\prime}(x)=1|\\)? Many cases are possible. To distinguish between them, one needs to inspect higher order derivatives.\n\nwhen \\(|f^{\\prime}(x)=1|\\), \\(|f^{\\prime\\prime}(x)|\\neq 0\\) the series will convergence, only if \\((x_0-x)f^{\\prime\\prime}(x)&lt;0\\), i.e. starting from one side of the fixed point. The steady-state is not stable.\nWhen \\(|f^{\\prime}(x)=1|\\), \\(|f^{\\prime\\prime}(x)| = 0\\), \\(|f^{\\prime \\prime\\prime}(x)|\\neq 0\\) the series will converge, only if \\(f^{\\prime}(x)(f^{\\prime\\prime\\prime}(x))&lt;1\\)\n\nIn general, there is stability only if the function \\(f\\) is crossing the 45 degrees line (when \\(f^ {\\prime}(x)=1)\\), or the -45 degrees line (when \\(f^ {\\prime}(x)=1\\))\nMathematically, this involves, that:\n\nthe first non-zero coefficient \\(f^{k}(x)\\) with \\(k&gt;1\\) has odd order (\\(k\\) odd)\nit has the right sign",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#change-the-problem",
    "href": "slides/session_convergence/index.html#change-the-problem",
    "title": "Convergence of Sequences",
    "section": "Change the problem",
    "text": "Change the problem\n\nSometimes, we are interested in tweaking the convergence speed: \\[x_{n+1} = (1-\\lambda) x_n + \\lambda f(x_n)\\]\n\n\\(\\lambda\\) is the learning rate:\n\n\\(\\lambda&gt;1\\): acceleration\n\\(\\lambda&lt;1\\): dampening\n\n\n\n\n\n\nWe can also replace the function by another one \\(g\\) such that \\(g(x)=x\\iff f(x)=x\\), for instance: \\[g(x)=x-\\frac{f(x)-x}{f^{\\prime}(x)-1}\\]\n\nthis is the Newton iteration",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#dynamics-around-a-stable-point",
    "href": "slides/session_convergence/index.html#dynamics-around-a-stable-point",
    "title": "Convergence of Sequences",
    "section": "Dynamics around a stable point",
    "text": "Dynamics around a stable point\n\n\nWe can write successive approximation errors: \\[|x_t - x_{t-1}| =  | f(x_{t-1}) - f(x_{t-2})| \\] \\[|x_t - x_{t-1}| \\sim |f^{\\prime}(x_{t-1})| |x_{t-1} - x_{t-2}| \\]\nRatio of successive approximation errors \\[\\lambda_t =  \\frac{ |x_{t} - x_{t-1}| } { |x_{t-1} - x_{t-2}|}\\]\nWhen the sequence converges: \\[\\lambda_t \\rightarrow | f^{\\prime}(\\overline{x}) |\\]",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#dynamics-around-a-stable-point-2",
    "href": "slides/session_convergence/index.html#dynamics-around-a-stable-point-2",
    "title": "Convergence of Sequences",
    "section": "Dynamics around a stable point (2)",
    "text": "Dynamics around a stable point (2)\nHow do we derive an error bound? Suppose that we have \\(\\overline{\\lambda}&gt;|f^{\\prime}(x_k)|\\) for all \\(k\\geq k_0\\):\n\\[|x_t - x| \\leq |x_t - x_{t+1}| + |x_{t+1} - x_{t+2}| + |x_{t+2} - x_{t+3}| + ... \\]\n\\[|x_t - x| \\leq |x_t - x_{t+1}| + |f(x_{t}) - f(x_{t+1})| + |f(x_{t+1}) - f(x_{t+2})| + ... \\]\n\\[|x_t - x| \\leq |x_t - x_{t+1}| + \\overline{\\lambda} |x_t - x_{t+1}| + \\overline{\\lambda}^2 |x_t - x_{t+1}| + ... \\]\n\\[|x_t - x| \\leq \\frac{1} {1-\\overline{\\lambda}} | x_t - x_{t+1} |\\]",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#how-do-we-improve-convergence",
    "href": "slides/session_convergence/index.html#how-do-we-improve-convergence",
    "title": "Convergence of Sequences",
    "section": "How do we improve convergence ?",
    "text": "How do we improve convergence ?\n\\[\\frac{|x_{t-1} - x_{t-2}|} {|x_t - x_{t-1}|} \\sim |f^{\\prime}(x_{t-1})|  \\]\ncorresponds to the case of linear convergence (kind of slow).",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#aitkens-extrapolation",
    "href": "slides/session_convergence/index.html#aitkens-extrapolation",
    "title": "Convergence of Sequences",
    "section": "Aitken’s extrapolation",
    "text": "Aitken’s extrapolation\nWhen convergence is geometric, we have: \\[ \\lim_{x\\rightarrow \\infty}\\frac{ x_{t+1}-x}{x_t-x} = \\lambda \\in \\mathbb{R}^{\\star}\\]\nWhich implies:\n\\[\\frac{ x_{t+1}-x}{x_t-x} \\sim \\frac{ x_{t}-x}{x_{t-1}-x}\\]",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#aitkens-extrapolation-2",
    "href": "slides/session_convergence/index.html#aitkens-extrapolation-2",
    "title": "Convergence of Sequences",
    "section": "Aitken’s extrapolation (2)",
    "text": "Aitken’s extrapolation (2)\nTake \\(x_{t-1}, x_t\\) and \\(x_{t+1}\\) as given and solve for \\(x\\):\n\\[x = \\frac{x_{t+1}x_{t-1} - x_{t}^2}{x_{t+1}-2x_{t} + x_{t-1}}\\]\nor after some reordering\n\\[x = x_{t-1} - \\frac{(x_t-x_{t-1})^2}{x_{t+1}-2 x_t + x_{t-1}}\\]",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#steffensens-method",
    "href": "slides/session_convergence/index.html#steffensens-method",
    "title": "Convergence of Sequences",
    "section": "Steffensen’s Method:",
    "text": "Steffensen’s Method:\n\nstart with a guess \\(x_0\\), compute \\(x_1=f(x_0)\\) and \\(x_2=f(x_1)\\)\nuse Aitken’s guess for \\(x^{\\star}\\). If required tolerance is met, stop.\notherwise, set \\(x_0 = x^{\\star}\\) and go back to step 1.\n\nIt can be shown that the sequence generated from Steffensen’s method converges quadratically, that is\n\\(\\lim_{t\\rightarrow\\infty} \\frac{x_{t+1}-x_t}{(x_t-x_{t-1})^2} \\leq M  \\in \\mathbb{R}^{\\star}\\)",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#convergence-speed",
    "href": "slides/session_convergence/index.html#convergence-speed",
    "title": "Convergence of Sequences",
    "section": "Convergence speed",
    "text": "Convergence speed\nRate of convergence of series \\(x_t\\) towards \\(x^{\\star}\\) is:\n\nlinear: \\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|} = \\mu \\in R^+\\]\nsuperlinear: \\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|} = 0\\]\nquadratic: \\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|^{\\color{red}2}} = \\mu \\in R^+\\]",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#convergence-speed-1",
    "href": "slides/session_convergence/index.html#convergence-speed-1",
    "title": "Convergence of Sequences",
    "section": "Convergence speed",
    "text": "Convergence speed\nRemark: in the case of linear convergence:\n\\[{\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x_t|}{|x_{t}-x_{t-1}|} = \\mu \\in R^+ \\iff {\\lim}_{t\\rightarrow\\infty} \\frac{|x_{t+1}-x^{\\star}|}{|x_{t}-x^{\\star}|}=\\frac{1}{1-\\mu}\\]",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_convergence/index.html#in-practice",
    "href": "slides/session_convergence/index.html#in-practice",
    "title": "Convergence of Sequences",
    "section": "In practice",
    "text": "In practice\n\n\nProblem: Suppose one is trying to find \\(x\\) solving the model \\(G(x)=0\\)\n\nAn iterative algorithm provides a function \\(f\\) defining a recursive series \\(x_{t+1}\\).\n\nThe best practice consists in monitoring at the same time:\n\nthe success criterion: \\[\\epsilon_n = |G(x_n)|\\]\n\n\nhave you found the solution?\n\nthe successive approximation errors \\[\\eta_n = |x_{n+1} - x_n|\\]\n\n\nare you making progress?\n\nthe ratio of successive approximation errors \\[\\lambda_n = \\frac{\\eta_n}{\\eta_{n-1}}\\]\n\n\nwhat kind of convergence? (if \\(|\\lambda_n|&lt;1\\): OK, otherwise: ❓)",
    "crumbs": [
      "Slides",
      "Convergence of Sequences"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#introduction",
    "href": "slides/session_ddp/index.html#introduction",
    "title": "Discrete Dynamic Programming",
    "section": "Introduction",
    "text": "Introduction",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#section",
    "href": "slides/session_ddp/index.html#section",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "The imperialism of Dynamic Programming\n— Recursive Macroeconomic Theory (Ljunqvist & Sargent)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#section-1",
    "href": "slides/session_ddp/index.html#section-1",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, “Where did the name, dynamic programming, come from?” The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Let’s take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it’s impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.\n\n— Richard Bellman, Eye of the Hurricane: An Autobiography (1984, page 159)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#markov-chain-and-markov-process-1",
    "href": "slides/session_ddp/index.html#markov-chain-and-markov-process-1",
    "title": "Discrete Dynamic Programming",
    "section": "Markov chain and Markov process",
    "text": "Markov chain and Markov process\n\nStochastic process: family of random variables indexed by time\nA stochastic process has the Markov property if its future evolution depends only on its current state.\nSpecial cases:\n\n\n\n\n\n\n\n\n\n\nDiscrete States\nContinuous States\n\n\n\n\nDiscrete Time\nDiscrete Markov Chain\nContinuous Markov Chain\n\n\nContinuous Time\nMarkov Jump Process\nMarkov Process",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#stochastic-matrices",
    "href": "slides/session_ddp/index.html#stochastic-matrices",
    "title": "Discrete Dynamic Programming",
    "section": "Stochastic matrices",
    "text": "Stochastic matrices\n\na matrix \\(M \\in R^n\\times R^n\\) matrix is said to be stochastic if\n\nall coefficents are non-negative\nall the lines lines sum to 1 (\\(\\forall i, \\sum_j M_{ij} = 1\\))\n\na probability density is a vector \\(\\mu \\in R^n\\) such that :\n\nall components are non-negative\nall coefficients sum to 1 (\\(\\sum_{i=1}^n \\mu_{i} = 1\\))\n\na distribution is a vector with such that:\n\nall components are non-negative",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#simulation",
    "href": "slides/session_ddp/index.html#simulation",
    "title": "Discrete Dynamic Programming",
    "section": "Simulation",
    "text": "Simulation\n\nConsider: \\(\\mu_{i,t+1}' =\\mu_t' P\\)\nWe have \\(\\mu_{i,t+1} = \\sum_{k=1}^n  \\mu_{k,t}  P_{k, i}\\)\nAnd: \\(\\sum_i\\mu_{i,t+1} = \\sum_i \\mu_{i,t}\\)\nPostmultiplication by a stochastic matrix preserves the mass.\nInterpretation: \\(P_{ij}\\) is the fraction of the mass initially in state \\(i\\) which ends up in \\(j\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#example",
    "href": "slides/session_ddp/index.html#example",
    "title": "Discrete Dynamic Programming",
    "section": "Example",
    "text": "Example\n\\[\\underbrace{\n\\begin{pmatrix}\n? & ? & ?\n\\end{pmatrix}\n}_{\\mu_{t+1}'} = \\underbrace{\n\\begin{pmatrix}\n0.5 & 0.3 & 0.2\n\\end{pmatrix}\n}_{\\mu_t'} \\begin{pmatrix}\n0.4 & 0.6 & 0.0 \\\\\\\\\n0.2 & 0.5 & 0.3 \\\\\\\\\n0 & 0 & 1.0\n\\end{pmatrix}\\]\n\nGraphical Representation:",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#probabilistic-interpretation",
    "href": "slides/session_ddp/index.html#probabilistic-interpretation",
    "title": "Discrete Dynamic Programming",
    "section": "Probabilistic interpretation",
    "text": "Probabilistic interpretation\n\nDenote by \\(S=(s_1,...s_n)\\) a finite set with \\(n\\) elements (\\(|S|=n\\)).\nA Markov Chain with values in \\(S\\) and with transitions given by a stochastic matrix \\(P\\in R^n\\times R^n\\) identfies a stochastic process \\((X_t)_{t\\geq 0}\\) such that \\[P_{ij} = Prob(X_{t+1}=s_j|X_t=s_i)\\]\nIn words, line \\(i\\) describes the conditional distribution of \\(X_{t+1}\\) conditional on \\(X_t=s_i\\).",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#what-about-longer-horizons",
    "href": "slides/session_ddp/index.html#what-about-longer-horizons",
    "title": "Discrete Dynamic Programming",
    "section": "What about longer horizons?",
    "text": "What about longer horizons?\n\nIt is easy to show that for any \\(k\\), \\(P^k\\) is a stochastic matrix.\n\\(P^k_{ij}\\) denotes the probability of ending in \\(j\\), after \\(k\\) periods, starting from \\(i\\)\nGiven an initial distribution \\(\\mu_0\\in R^{+ n}\\)\n\nWhich states will visited with positive probability between t=0 and t=k?\nWhat happens in the very long run?\n\nWe need to study a little bit the properties of Markov Chains",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#connectivity",
    "href": "slides/session_ddp/index.html#connectivity",
    "title": "Discrete Dynamic Programming",
    "section": "Connectivity",
    "text": "Connectivity\n\nTwo states \\(s_i\\) and \\(s_j\\) are connected if \\(P_{ij}&gt;0\\)\nWe call incidence matrix: \\(\\mathcal{I}(P)=(\\delta_{P_{ij}&gt;0})_{ij}\\)\nTwo states \\(i\\) and \\(j\\) communicate with each other if there are \\(k\\) and \\(l\\) such that: \\((P^k)_ {i,j}&gt;0\\) and \\((P^l)_ {j,i}&gt;0\\)\n\nit is an equivalence relation\nwe can define equivalence classes\n\nA stochastic matrix \\(P\\) is irreducible if all states communicate\n\nthere is a unique communication class",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#connectivity-and-irreducibility",
    "href": "slides/session_ddp/index.html#connectivity-and-irreducibility",
    "title": "Discrete Dynamic Programming",
    "section": "Connectivity and irreducibility",
    "text": "Connectivity and irreducibility\n\n\nIrreducible\n\n\n\n\n\n\n\n\n\n\n\nIrreducible: all states can be reached with positive probability from any initial state.\n\nNot irreducible\n\n\n\n\n\n\n\n\n\n\n\n\nHere there is a subset of states (poor), which absorbs all the mass coming in.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#aperiodicity",
    "href": "slides/session_ddp/index.html#aperiodicity",
    "title": "Discrete Dynamic Programming",
    "section": "Aperiodicity",
    "text": "Aperiodicity\n\nAre there cycles? Starting from a state \\(i\\), how long does it take to return to \\(i\\)?\nThe period of a state is defined as \\[gcd( {k\\geq 1 | (P^k)_{i,i}&gt;0} )\\]\nIf a state has a period d&gt;1 the chain returns to the state only at dates multiple of d.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#aperiodicity-1",
    "href": "slides/session_ddp/index.html#aperiodicity-1",
    "title": "Discrete Dynamic Programming",
    "section": "Aperiodicity",
    "text": "Aperiodicity\n\n\nPeriodic\n\n\n\n\n\n\n\n\n\n\n\n\nIf you start from some states, you return to it, but not before two periods.\n\n\nAperiodic\n\n\n\n\n\n\n\n\n\n\n\n\nIf some mass leaves a state, some of it returns to the state in the next period.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#stationary-distribution",
    "href": "slides/session_ddp/index.html#stationary-distribution",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution",
    "text": "Stationary distribution\n\n\\(\\mu\\) is a stationary distribution if \\(\\mu' = \\mu' P\\)\nTheorem: there always exists such a distribution\n\nproof: Brouwer theorem (fixed-point result for compact-convex set)\n\\(f: \\mu\\rightarrow (\\mu'P)'\\)\n\nTheorem:\n\nif P is irreducible the fixed point \\(\\mu^{\\star}\\) is unique\nif P is irreducible and aperiodic \\(|\\mu_0' P^k - \\mu^{\\star}| \\underset{k\\to+\\infty}{\\longrightarrow}0\\) for any initial distribution \\(\\mu_0\\)\n\nWe then say the Markov chain is ergodic\n\\(\\mu^{\\star}\\) is the ergodic distribution\n\nit is the best guess, one can do for the state of the chain in the very far future",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#stationary-distribution-proof",
    "href": "slides/session_ddp/index.html#stationary-distribution-proof",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution (proof)",
    "text": "Stationary distribution (proof)\n\nBrouwer’s theorem: Let \\(\\mathcal{C}\\) be a compact convex subset of \\(R^n\\) and \\(f\\) a continuous mapping \\(\\mathcal{C}\\rightarrow \\mathcal{C}\\). Then there exists a fixed point \\(x_0\\in \\mathcal{C}\\) such that \\(f(x_0)=x_0\\)\nResult hinges on:\n\ncontinuity of \\(f: \\mu \\mapsto \\mu P\\)\nconvexity of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\) (easy to check)\ncompactness of \\(\\\\{x \\in R^n | |x|=1 \\\\}\\)\n\nit is bounded\nand closed (the inverse image of 1 for \\(u\\mapsto |u|\\) which is continuous)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#stationary-distribution-1",
    "href": "slides/session_ddp/index.html#stationary-distribution-1",
    "title": "Discrete Dynamic Programming",
    "section": "Stationary distribution?",
    "text": "Stationary distribution?\nHow do we compute the stationary distribution?\n\nSimulation\nLinear algebra\nDecomposition",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#simulating-a-markov-chain",
    "href": "slides/session_ddp/index.html#simulating-a-markov-chain",
    "title": "Discrete Dynamic Programming",
    "section": "Simulating a Markov Chain",
    "text": "Simulating a Markov Chain\n\nVery simple idea:\n\nstart with any \\(\\mu_0\\) and compute the iterates recursively\n\\(\\mu_{n+1}' = \\mu_n' P\\)\nconvergence is linear:\n\n\\(|\\mu_{n+1} - \\mu_n| \\leq |P| |\\mu_n - \\mu_{n-1}|\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#using-linear-algebra",
    "href": "slides/session_ddp/index.html#using-linear-algebra",
    "title": "Discrete Dynamic Programming",
    "section": "Using Linear Algebra",
    "text": "Using Linear Algebra\n\nFind the solution of \\(\\mu'(P-I) = 0\\) ?\n\nnot well defined, 0 is a solution\nwe need to incorporate the constraint \\(\\sum_i(\\mu_i)=1\\)\n\nMethod:\n\nDefine \\(M_{ij} =  \\begin{cases} 1  &\\text{if} & j =0 \\\\\\\\ (P-I)_{ij}  & \\text{if} & j&gt; 1  \\end{cases}\\)\nDefine \\(D_i = \\begin{cases} 1 & \\text{if} & j = 0 \\\\\\\\0 & \\text{if} & j&gt;0 \\end{cases}\\)\nWith a linear algebra solver\n\nlook for a solution \\(\\mu\\) of \\(\\mu' M = D\\)\nor \\(M^{\\prime} \\mu = D\\prime\\)\nif you find a solution, it is unique (theorem)\n\n\nAlternative:\n\nminimize residual squares of overidentified system",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#code-example",
    "href": "slides/session_ddp/index.html#code-example",
    "title": "Discrete Dynamic Programming",
    "section": "Code example",
    "text": "Code example\n# we use the identity matrix and the \\ operator\nusing LinearAlgebra: I, \\\n# define a stochastic matrix (lines sum to 1)\nP = [  0.9  0.1 0.0  ;\n       0.05 0.9 0.05 ;\n       0.0  0.9 0.1  ]\n# define an auxiliary matrix\nM = P' - I\nM[end,:] .= 1.0\n# define rhs\nR = zeros(3)\nR[end] = 1\n# solve the system\nμ = M\\R\n# check that you have a solution:\n@assert sum(μ) == 1\n@assert all(abs.(μ'P - μ').&lt;1e-10)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#further-comments",
    "href": "slides/session_ddp/index.html#further-comments",
    "title": "Discrete Dynamic Programming",
    "section": "Further comments",
    "text": "Further comments\n\nKnowledge about the structure of the Markov Chain can help speedup the calculations\nThere are methods for potentially very-large linear system\n\nNewton-Krylov based methods, GMRES\n\nBasic algorithms are easy to implement by hand\nQuantEcon toolbox has very good methods to study markov chains",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#section-2",
    "href": "slides/session_ddp/index.html#section-2",
    "title": "Discrete Dynamic Programming",
    "section": "",
    "text": "Consider the following problems:\n  \n\n\n\nMonopoly pricing:\n\\[\\max_{q} \\pi(q) - c(q)\\]\n\nShopping problem\n\\[\\max_{\\substack{c_1, c_2 \\\\ p_1 c_1 + p_2 c_2 \\leq B}} U(c_1,c_2)\\]\n\nConsumption Savings\n\\[\\max_{\\substack{c() \\\\ w_{t+1}=(w_t-c(w_t))(1+r)) + y_{t+1}}} E_0 \\sum_t \\beta^t U(c(w_t))\\]\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nobjective\naction\nstate\ntransition\ntype\n\n\n\n\nmonopoly pricing\nprofit\nchoose quantity to produce\n\n\noptimization\n\n\nshopping problem\nutility\nchoose consumption composition\nbudget \\(B\\)\n\ncomparative statics\n\n\nconsumption/savings\nexpected welfare\nsave or consume\navailable income\nevolution of wealth\ndynamic optimization",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#general-formulation",
    "href": "slides/session_ddp/index.html#general-formulation",
    "title": "Discrete Dynamic Programming",
    "section": "General Formulation",
    "text": "General Formulation\nMarkov Decision Problem\n\n\n\n\nEnvironment\n\nstates: \\(s \\in S\\)\nactions: \\(x \\in X(s)\\)\ntransitions: \\(\\pi(s'| s, x) \\in S\\)\n\n\\(probability\\) of going to \\(s'\\) in state \\(s\\)…\n… given action \\(x\\)\n\n\n\n\n\n\nReward: \\(r(s,x) \\in R\\)\n\naka felicity, intratemporal utility\n\n\nPolicy: \\(x(): s \\rightarrow x\\in X(s)\\)\n\na.k.a. decision rule\nwe consider deterministic policy\ngiven \\(x()\\), the evolution of \\(s\\) is a Markov process\n\n\\(\\pi(. |s, x())\\) is a distribution for \\(s'\\) over \\(S\\)\nit depends only on \\(s\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#objective",
    "href": "slides/session_ddp/index.html#objective",
    "title": "Discrete Dynamic Programming",
    "section": "Objective",
    "text": "Objective\n\nexpected lifetime reward:\n\nvalue of following policy \\(x()\\) starting from \\(s\\): \\[R(s; x()) =  E_0 \\sum_t^T \\delta^t \\left[ r_t\\right]\\]\n\\(\\delta \\in [0,1[\\): discount factor\nhorizon: \\(T \\in \\\\{N, \\infty\\\\}\\)\n\nvalue of a state \\(s\\)\n\nvalue of following the optimal policy starting from \\(s\\) \\[V(s) = \\max_{ x()} R(s, x())\\]\n\\(V()\\) is the value function (t.b.d.)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#classes-of-dynamic-optimization",
    "href": "slides/session_ddp/index.html#classes-of-dynamic-optimization",
    "title": "Discrete Dynamic Programming",
    "section": "Classes of Dynamic Optimization",
    "text": "Classes of Dynamic Optimization\n\nThe formulation so far is very general. It encompasses several variants of the problem:\n\nfinite horizon vs infinite horizon\ndiscrete-space problem vs continuous-state space problem\nsome learning problems (reinforcement learning…)\n\nThere are also variants not included:\n\nnon time-separable problems\nnon time-homogenous problems\nsome learning problems (bayesian updating, …)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#finite-horizon-vs-infinite-horizon",
    "href": "slides/session_ddp/index.html#finite-horizon-vs-infinite-horizon",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon vs infinite horizon",
    "text": "Finite horizon vs infinite horizon\n\nRecall objective: \\(V(s; x()) =  \\max E_0\\sum_{t=0}^T \\delta^t \\left[ r(s_t, x_t) \\right]\\)\nIf \\(T&lt;\\infty\\), the decision in the last periods, will be different from the periods before\n\none must find a decision rule \\(\\pi_t()\\) per period\nor, equivalently, add \\(t\\) to the state space: \\(\\tilde{S}=S\\times[0,T]\\)\n\nIf \\(T=\\infty\\), the continuation value of being in state \\(s_t\\) is independent from \\(t\\)\n\n\\[V(s; x()) = E_0 \\max \\sum_ {t=0}^{T_0} \\delta^t \\left[ r(s_t, x_t) \\right] + \\delta^{T_0} E_0  \\sum_ {t=T_0}^{\\infty} \\delta^t \\left[ r(s_t, x_t) \\right]\\]\n\\[ = E_0 \\left[ \\max \\sum_ {t=0}^{T_0} \\delta^t \\left[ r(s_t, x_t) \\right] +  \\delta^{T_0} V(s_ {T_0}; x()) \\right]\\]",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#continuous-vs-discrete",
    "href": "slides/session_ddp/index.html#continuous-vs-discrete",
    "title": "Discrete Dynamic Programming",
    "section": "Continuous vs discrete",
    "text": "Continuous vs discrete\n\nDiscrete Dynamic Programming (today)\n\ndiscrete states: \\(s \\in {s_1, \\cdots, s_N}\\)\ndiscrete controls: \\(|X(s)|&lt;\\infty\\)\nthere is a finite number of policies, the can be represented exactly\nunless \\(|S|\\) is very large (cf go game)\n\nContinuous problem:\n\n\\(x(s)\\), \\(V(s; \\pi)\\) require an infinite number of coefficients\nsame general approach but different implementation\ntwo main variants:\n\ndiscretize the initial problem: back to DDP\nuse approximation techniques (i.e. interpolation)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#non-time-separable-example",
    "href": "slides/session_ddp/index.html#non-time-separable-example",
    "title": "Discrete Dynamic Programming",
    "section": "Non time separable example",
    "text": "Non time separable example\n\nFor instance Epstein-Zin preferences: \\[\\max V(;c())\\] where \\[V_t = (1-\\delta) \\frac{c_t^{1-\\sigma}}{1-\\sigma} + \\delta \\left[ E_t V_{t+1}^{\\alpha} \\right]^{\\frac{1}{\\alpha}}\\]\nWhy would you do that?\n\nto disentangle risk aversion and elasticity of intertemporal substitution\nrobust control\n\nYou can still use ideas from Dynamic Programming.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#non-homogenous-preference",
    "href": "slides/session_ddp/index.html#non-homogenous-preference",
    "title": "Discrete Dynamic Programming",
    "section": "Non homogenous preference",
    "text": "Non homogenous preference\n\nLook at the \\(\\alpha-\\beta\\) model. \\[V_t = \\max \\sum_t^{\\infty} \\beta_t U(c_t)\\] where \\(\\delta_0 = 1\\), \\(\\delta_1=\\alpha\\), \\(\\delta_k=\\alpha\\beta^{k-1}\\)\nMakes the problem time-inconsistent:\n\nthe optimal policy you would choose for the continuation value after \\(T\\) is not the same if you maximize it in expectation from \\(0\\) or at \\(T\\).",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#learning-problems",
    "href": "slides/session_ddp/index.html#learning-problems",
    "title": "Discrete Dynamic Programming",
    "section": "Learning problems",
    "text": "Learning problems\n\nBayesian learning: Uncertainty about some model parameters\n\nex: variance and return of a stock market\nagent models this uncertainty as a distribution\nagent updates his priors after observing the result of his actions\nactions are taken optimally taken into account the revelation power of some actions\n\nIs it good?\n\nclean: the rational thing to do with uncertainty\nsuper hard: the state-space should contain all possible priors\nmathematical cleanness comes with many assumptions\n\nUsed to estimate rather big (mostly linear) models",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#learning-problems-2",
    "href": "slides/session_ddp/index.html#learning-problems-2",
    "title": "Discrete Dynamic Programming",
    "section": "Learning problems (2)",
    "text": "Learning problems (2)\n\nReinforcement learning\n\nmodel can be partially or totally unknown\ndecision rule is updated by observing the reward from actions\n\nno priors\n\nsolution does not derive directly from model\n\ncan be used to solve dynamic programming problems\n\n\nGood solutions maximize a criterion similar to lifetime reward but are usually not optimal:\n\nusually evaluated by replaying the game many times\ntradeoff exploration / exploitations",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#finite-horizon-dmdp-1",
    "href": "slides/session_ddp/index.html#finite-horizon-dmdp-1",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\nWhen \\(T&lt;\\infty\\). With discrete action the problem can be represented by a tree.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#finite-horizon-dmdp-2",
    "href": "slides/session_ddp/index.html#finite-horizon-dmdp-2",
    "title": "Discrete Dynamic Programming",
    "section": "Finite horizon DMDP",
    "text": "Finite horizon DMDP\n\nIntuition: backward induction.\n\nFind optimal policy \\(x_T(s_T)\\) in all terminal states \\(s_T\\). Set \\(V_T(s_T)\\) equal to \\(r(s_T, \\pi_T)\\)\nFor each state \\(s_{k-1}\\in S\\) find \\(x_{k-1}\\in X(s_{k-1})\\) which maximizes \\[V_{k-1}(s_{k-1}) = \\max_{x_{k-1}(s_{k-1})\\in X(s_{k-1})}r(s_{k-1},x_{k-1}) + \\delta \\underbrace{ \\sum_{s_k\\in S} \\pi(s_k | s_{k-1}, x_{k-1} ) V_k(s_k)} _{ \\textit{expected continuation value} }\\]\n\nPolicies \\(x_0(), ... x_T()\\) are Markov-perfect:\n\nthey maximize utility on all subsets of the “game”\nalso from t=0",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#remarks",
    "href": "slides/session_ddp/index.html#remarks",
    "title": "Discrete Dynamic Programming",
    "section": "Remarks",
    "text": "Remarks\n\nCan we do better than this naive algorithm?\n\nnot really\nbut we can try to limit \\(S\\) to make the maximization step faster\nexclude a priori some branches in the tree using knowledge of the problem",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#infinite-horizon-dmdp-1",
    "href": "slides/session_ddp/index.html#infinite-horizon-dmdp-1",
    "title": "Discrete Dynamic Programming",
    "section": "Infinite horizon DMDP",
    "text": "Infinite horizon DMDP\n\nHorizon is infinite: \\[V(s) =  \\max E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) \\]\nIntuition:\n\nlet’s consider the finite horizon version \\(T&lt;\\infty\\) and \\(T &gt;&gt; 1\\)\ncompute the solution, increase \\(T\\) until the solution doesn’t change\nin practice: take an initial guess for \\(V_{T}\\) then compute optimal \\(V_{T-1}\\), \\(V_{T_2}\\) and so on, until convergence of the \\(V\\)s",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#infinite-horizon-dmdp-2",
    "href": "slides/session_ddp/index.html#infinite-horizon-dmdp-2",
    "title": "Discrete Dynamic Programming",
    "section": "Infinite horizon DMDP (2)",
    "text": "Infinite horizon DMDP (2)\n\nThis is possible, it’s called Successive Approximation or Value Function Iteration\n\nhow fast does it converge? linearly\ncan we do better? yes, quadratically\n\nwith howard improvement steps",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#successive-approximation",
    "href": "slides/session_ddp/index.html#successive-approximation",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation",
    "text": "Successive Approximation\n\nConsider the decomposition: \\[V(s; x()) = E_0 \\sum_{t=0}^{\\infty} \\delta^t r(s_t, x_t) = E_0 \\left[ r(s, x(s)) + \\sum_{t=1}^{\\infty} \\delta^t r(s_t, x_t) \\right]\\]\n\nor\n\\[V(s; x()) =  r(s, x(s)) + \\delta \\sum_{s'} p(s'|s,x(s)) V(s'; x()) \\]",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#successive-approximation-2",
    "href": "slides/session_ddp/index.html#successive-approximation-2",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation (2)",
    "text": "Successive Approximation (2)\n\nTaking continuation value as given we can certainly improve the value in every state \\(\\tilde{V}\\) by choosing \\(\\tilde{x}()\\) so as to maximize \\[\\tilde{V}(s; \\tilde{x}(), x()) =  r(s, \\tilde{x}(s)) + \\delta \\sum_{s'} \\pi(s'|s,\\tilde{x}(s) )V(s'; x()) \\]\nBy construction: \\(\\forall s, \\tilde{V}(s, \\tilde{x}(), x()) &gt; {V}(s, x())\\)\n\nit is an improvement step\n\nCan \\({V}(s, \\tilde{x}())\\) be worse for some states than \\({V}(s, x())\\) ?\n\nactually no",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#bellman-equation",
    "href": "slides/session_ddp/index.html#bellman-equation",
    "title": "Discrete Dynamic Programming",
    "section": "Bellman equation",
    "text": "Bellman equation\n\nIdea:\n\nit should not be possible to improve upon the optimal solution.\nHence the optimal value \\(V\\) and policy \\(x^{\\star}\\) should satisfy: \\[\\forall s\\in S, V(s) = \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\] with the maximum attained at \\(x(s)\\).\n\nThis is referred to as the Bellman equation.\nConversely, it is possible to show that a solution to the Bellman equation is also an optimal solution to the initial problem.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#bellman-operator",
    "href": "slides/session_ddp/index.html#bellman-operator",
    "title": "Discrete Dynamic Programming",
    "section": "Bellman operator",
    "text": "Bellman operator\n\nThe function \\(G\\) is known as the Bellman operator: \\[G: V \\rightarrow \\max_{y(s)} r(s, y(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, y(s)) V(s^{\\prime})\\]\nDefine sequence \\(V_n = G(V_{n-1})\\)\n\nit goes back in time\nbut is not the time-iteration operator\n\nOptimal value is a fixed point of G\nDoes \\(G\\) converges to it ? Yes, if \\(G\\) is a contraction mapping.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#blackwells-theorem",
    "href": "slides/session_ddp/index.html#blackwells-theorem",
    "title": "Discrete Dynamic Programming",
    "section": "Blackwell’s theorem",
    "text": "Blackwell’s theorem\n\nLet \\(X\\subset R^n\\) and let \\(\\mathcal{C}(X)\\) be a space of bounded functions \\(f: X\\rightarrow  R\\), with the sup-metric. \\(B: \\mathcal{C}(X)\\rightarrow \\mathcal{C}(X)\\) be an operator satisfying two conditions:\n\n(monotonicity) if \\(f,g \\in \\mathcal{C}(X)\\) and \\(\\forall x\\in X, f(x)\\leq g(x)\\) then\n\n\\(\\forall x \\in X (Bf)(x)\\leq(Bg)(x)\\)\n\n(discounting) there exists some \\(\\delta\\in]0,1[\\) such that: \\(B.(f+a)(x)\\leq (B.f)(x) + \\delta a, \\forall f \\in \\mathcal{C}(X), a\\geq 0, x\\in X\\)\n\nThen \\(B\\) is a contraction mapping with modulus \\(\\delta\\).",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#successive-approximation-1",
    "href": "slides/session_ddp/index.html#successive-approximation-1",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation",
    "text": "Successive Approximation\n\nUsing the Blackwell’s theorem, we can prove the Bellman operator is a contraction mapping.\nThis justifies the Value Function Iteration algorithm:\n\nchoose an initial \\(V_0\\)\ngiven \\(V_n\\) compute \\(V_{n+1} = G(V_n)\\)\niterate until \\(|V_{n+1}- V_n|\\leq \\eta\\)\n\nPolicy rule is deduced from \\(V\\) as the maximand in the Bellman step",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#successive-approximation-2-1",
    "href": "slides/session_ddp/index.html#successive-approximation-2-1",
    "title": "Discrete Dynamic Programming",
    "section": "Successive Approximation (2)",
    "text": "Successive Approximation (2)\nAssume that \\(X\\) is finite.\n\nNote that convergence of \\(V_n\\) is geometric\nBut \\(x_n\\) converges after a finite number of iteration.\n\nsurely the latest iterations are suboptimal\nthey serve only to evaluate the value of \\(x\\)\n\nIn fact:\n\n\\(V_n\\) is never the value of \\(x_n()\\)\nshould we try to keep both in sync?",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#policy-iteration-for-dmdp",
    "href": "slides/session_ddp/index.html#policy-iteration-for-dmdp",
    "title": "Discrete Dynamic Programming",
    "section": "Policy iteration for DMDP",
    "text": "Policy iteration for DMDP\n\nChoose initial policy \\(x_0()\\)\nGiven initial guess \\(x_n()\\)\n\ncompute the value function \\(V_n=V( ;x_n)\\) which satisfies\n\\(\\forall s,  V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\)\nimprove policy by maximizing in \\(x_n()\\) \\[\\max_{x_n()} r(s, x_n(s)) + \\delta \\sum_{s^{\\prime}\\in S} \\pi(s^{\\prime}| s, x_n(s)) V_{n-1}(s^{\\prime})\\]\n\nRepeat until convergence, i.e. \\(x_n=x_{n+1}\\)\nOne can show the speed of convergence (for \\(V_n\\)) is quadratic\n\nit corresponds the Newton-Raphson steps applied to \\(V\\rightarrow G(V)-V\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#how-do-we-compute-the-value-of-a-policy",
    "href": "slides/session_ddp/index.html#how-do-we-compute-the-value-of-a-policy",
    "title": "Discrete Dynamic Programming",
    "section": "How do we compute the value of a policy?",
    "text": "How do we compute the value of a policy?\n\nGiven \\(x_n\\), goal is to find \\(V_n(s)\\) in \\[\\forall s,  V_n(s) = r(s, x_n(s)) + \\delta \\sum_{s'} \\pi(s'| s, x_n(s)) V_n(s')\\]\nThree approaches:\n\nsimulate the policy rule and compute \\(E\\left[ \\sum_t \\delta^t r(s_t, x_t) \\right]\\) with Monte-Carlo draws\nsuccessive approximation:\n\nput \\(V_k\\) in the rhs and recompute the lhs \\(V_{k+1}\\), replace \\(V_k\\) by \\(V_{k+1}\\) and iterate until convergence\n\nsolve a linear system in \\(V_n\\)\n\nFor 2 and 3 it is useful to constuct a linear operator \\(M\\) such that \\(V_{n+1} = R_n + \\delta M_n .  V_n\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#idea",
    "href": "slides/session_ddp/index.html#idea",
    "title": "Discrete Dynamic Programming",
    "section": "Idea",
    "text": "Idea\n\nMcCall model:\n\nwhen should an unemployed person accept a job offer?\nchoice between:\n\nwait for a better offer (and receive low unemp. benefits)\naccept a suboptimal job offer\n\n\nWe present a variant of it, with a small probability of loosing a job.",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#formalization",
    "href": "slides/session_ddp/index.html#formalization",
    "title": "Discrete Dynamic Programming",
    "section": "Formalization",
    "text": "Formalization\n\nWhen unemployed in date, a job-seeker\n\nconsumes unemployment benefit \\(c_t = \\underline{c}\\)\nreceives in every date \\(t\\) a job offer \\(w_t\\)\n\n\\(w_t\\) is i.i.d.,\ntakes values \\(w_1, w_2, w_3\\) with probabilities \\(p_1, p_2, p_3\\)\n\nif job-seeker accepts, becomes employed at rate \\(w_t\\) in the next period\nelse he stays unemployed\n\nWhen employed at rate \\(w\\)\n\nworker consumes salary \\(c_t = w\\)\nwith small probability \\(\\lambda&gt;0\\) looses his job:\n\nstarts next period unemployed\n\notherwise stays employed at same rate\n\nObjective: \\(\\max E_0 \\left\\{ \\sum \\beta^t \\log(c_t) \\right\\}\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#states-reward",
    "href": "slides/session_ddp/index.html#states-reward",
    "title": "Discrete Dynamic Programming",
    "section": "States / reward",
    "text": "States / reward\n\nWhat are the states?\n\nemployement status: Unemployed / Employed\nif Unemployed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) of the salary that is currently proposed\n\nif Employed:\n\nthe level \\(w\\in {w_1, w_2, w_3}\\) at which worker was hired\n\ncurrent state, can be represented by a 2x3 index\n\nWhat are the actions?\n\nif Unemployed:\n\nreject (false) / accept (true)\n\nif Employed: None\nactions (when unemployed) are represented by a 3 elements binary vector\n\nWhat is the (intratemporal) reward?\n\nif Unemployed: \\(U(c)\\)\nif Employed at rate w: \\(U(w)\\)\nhere it doesn’t depend on the action",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#value-function",
    "href": "slides/session_ddp/index.html#value-function",
    "title": "Discrete Dynamic Programming",
    "section": "Value function",
    "text": "Value function\n\\(\\newcommand{\\E}{\\mathbb{E}}\\)\n\nWhat is the value of being in a given state?\nIf Unemployed, facing current offer \\(w\\):\n\\[V^U(w) = U(\\underline{c}) + \\max_{a} \\begin{cases} \\beta V^E(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ V^U(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\]\nIf Employed, at rate \\(w\\) \\[V^E(w) = U(w) +  (1-\\lambda) \\beta V^E(w) +  \\lambda \\beta E_{w'}\\left[ V^U(w^{\\prime}) \\right] \\]\nWe can represent value as two functions \\(V^U\\) and \\(V^E\\) of the states as\n\ntwo vectors of Floats, with three elements (recall: value-function is real valued)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#value-function-iteration",
    "href": "slides/session_ddp/index.html#value-function-iteration",
    "title": "Discrete Dynamic Programming",
    "section": "Value function iteration",
    "text": "Value function iteration\n\nTake a guess for value function \\(\\tilde{V^E}\\), \\(\\tilde{V^U}\\), tomorrow\nUse it to compute value function today: \\[V^U(w) = U(\\underline{c}) + \\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ \\tilde{V}^U(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V^E(w) = U(w) +  (1-\\lambda) \\beta \\tilde{V}^E(w) +  \\lambda \\beta E_{w'}\\left[\\tilde{V}^U(w^{\\prime}) \\right] \\]\n\\((\\tilde{V}^E, \\tilde{V}^U)\\mapsto (V^E, V^U)\\) is one value iteration step\nNote that we don’t have to keep track of policies tomorrow\n\nall information about future decisions is contained in \\(\\tilde{V}^E, \\tilde{V}^U\\)\nbut we can keep track of current policy: \\(a(w): \\arg\\max \\cdots\\)",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#value-evaluation",
    "href": "slides/session_ddp/index.html#value-evaluation",
    "title": "Discrete Dynamic Programming",
    "section": "Value evaluation",
    "text": "Value evaluation\n\nSuppose we take a policy \\(a(w)\\) as given. What is the value of following this policy forever?\nThe value function \\(V_a^E\\), \\(V_a^U\\) satisfies \\[V_a^U(w) = U(\\underline{c}) + \\begin{cases} \\beta {V}^E_a(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ {V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) = U(w) +  (1-\\lambda) \\beta {V}^E_a(w) +  \\lambda \\beta E_{w'}\\left[{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function: we don’t reoptimize",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#value-evaluation-2",
    "href": "slides/session_ddp/index.html#value-evaluation-2",
    "title": "Discrete Dynamic Programming",
    "section": "Value evaluation (2)",
    "text": "Value evaluation (2)\n\nHow do you compute value of policy \\(a(w)\\) recursively?\nIterate: \\((\\tilde{V}^E_a, \\tilde{V}^U)\\mapsto (V^E_a, V^U_a)\\) \\[V_a^U(w) \\leftarrow U(\\underline{c}) + \\begin{cases} \\beta \\tilde{V}^E_a(w) & \\text{if $a(w)$ is true} \\\\ \\beta  E_{w'}\\left[ \\tilde{V}^U_a(w^{\\prime}) \\right]  & \\text{if $a(w)$ is false} \\end{cases}\\] \\[V_a^E(w) \\leftarrow U(w) +  (1-\\lambda) \\beta \\tilde{V}^E_a(w) +  \\lambda \\beta E_{w'}\\left[\\tilde{V}^U_a(w^{\\prime}) \\right] \\]\nNote the absence of the max function:\n\nwe don’t reoptimize\nwe we keep the same policy all along",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "slides/session_ddp/index.html#policy-iteration",
    "href": "slides/session_ddp/index.html#policy-iteration",
    "title": "Discrete Dynamic Programming",
    "section": "Policy iteration",
    "text": "Policy iteration\n\nstart with policy \\(a(w)\\)\nevaluate the value of this policy \\(V^E_a, V^U_a\\)\ncompute the optimal policy \\(a(w)\\) in the Bellman iteration\n\nhere: \\(a(w) = \\arg\\max_{a(w)} \\begin{cases} \\beta \\tilde{V}^E(w)\\\\ \\beta  E_{a'}\\left[ \\tilde{V}^U(a^{\\prime}) \\right] \\end{cases}\\)\n\niterate until \\(a(w)\\) converges",
    "crumbs": [
      "Slides",
      "Discrete Dynamic Programming"
    ]
  },
  {
    "objectID": "Exam_mie37_2024.html",
    "href": "Exam_mie37_2024.html",
    "title": "MIE 37 - Final Exam 2024",
    "section": "",
    "text": "Name:\nSurname:\nAfter completing the following questions, send the edited notebook to pablo.winant@ensae.fr.\nYou are allowed to use any online available resource, even to install Julia packages, but not to copy/paste any code.\nAlso, don’t forget to comment your code and take any initiative you find relevant."
  },
  {
    "objectID": "Exam_mie37_2024.html#part-i---linear-regression-and-stochastic-gradient-descent",
    "href": "Exam_mie37_2024.html#part-i---linear-regression-and-stochastic-gradient-descent",
    "title": "MIE 37 - Final Exam 2024",
    "section": "Part I - Linear Regression and Stochastic Gradient Descent",
    "text": "Part I - Linear Regression and Stochastic Gradient Descent\nWe consider the following data generation process:\n\\[y=0.4+2.5 x + \\epsilon\\]\nwhere \\(x_i\\) is uniformly distributed between 0 and 1 and \\(\\epsilon\\) is drawn from a normal distribution with standard deviation \\(\\sigma=0.5\\).\n\nWrite a function draw(a::Number, b::Number)::Tuple{Float64, Float64} which generates one random draw for a pair \\((x,y)\\).\n\n\n# you code here\n\n\nGenerate a sample \\(d=(x_i, y_i)_{i=[1,N]}\\) of \\(N=100000\\) different observations. Justify your choice for how to represent this data.\n\n\n# you code here\n\n\nWrite the loss function \\(L(d,a,b)=\\frac{1}{N}\\sum_{i=1}^N ( a x_i + b-y_i)^2\\). Find the values of \\(a\\) and \\(b\\) minimizing this function by implementing the gradient descent algorithm (do not use any library). What is the best learning rate?\n\n\n# you code here\n\n\nWrite another function ξ(a::Number, b::Number)::Tuple{Float64, Float64, Float64} which returns a random realization of \\(( a x + b - y)^2\\) as well as its derivatives w.r.t. a and b (make sure the derivatives are computed for the same realization of \\(\\epsilon\\)). We call ξ the empirical loss.\n\n(hint: here you can either compute the derivatives by hand, or use an automatic differentiation library)\n\n# you code here\n\n5. Stochastic gradient algorithm.\nThe stochastic gradient algorithm minimizes \\[E\\xi(a,b)\\] with the following steps:\n\nstart with an initial guess \\(a_0,b_0\\)\ngiven a guess \\(a_k, b_k\\)\n\ncompute \\(\\xi, \\xi^{prime}_a, \\xi^{prime}_b\\) using the function from the last function\nmake a new guess \\((a_{k+1}, b_{k+1}) = (1-\\lambda) (a_{k}, b_{k}) - \\lambda (\\xi^{\\prime}_a, \\xi^{\\prime}_b)\\)\n\n\nImplement the SGD algorithm. How many iterations does one needs to get a good approximation of \\(a,b\\)? What value of $ $ works better? Compare with question 3.\n\n# you code here\n\n6. (bonus) Illustrate the convergence by plotting the empirical loss for each step \\(k\\) in the above alogorithm, as well as the validation loss in the same step. (Given \\(a,b\\), the validation loss is the empirical mean of \\(\\xi(a,b)\\) computed using \\(N=1000\\) observations.)\n\n# you code here"
  },
  {
    "objectID": "Exam_mie37_2024.html#part-ii---endogenous-exit",
    "href": "Exam_mie37_2024.html#part-ii---endogenous-exit",
    "title": "MIE 37 - Final Exam 2024",
    "section": "Part II - Endogenous Exit",
    "text": "Part II - Endogenous Exit\n\nDiscretization of an AR1\nThe following code, taken from Quantecon.jl approximates an AR1 process \\[y_t = \\mu + \\rho (y_{t-1}-\\mu) + \\epsilon_t\\] (where \\(\\nu\\) is the standard deviation of normal process \\(\\epsilon\\)), using a finite markov chain with \\(N\\) different values.\nThe output is a transition matrix and a vector containing discretized values \\(y_1, y_2, ... y_p\\)\n\n# uncomment the following line if \"SpecialFunctions\" is not on your system\n# import Pkg; Pkg.add(\"SpecialFunctions\")\n\n\nusing SpecialFunctions: erfc\n\nstd_norm_cdf(x::T) where {T &lt;: Real} = 0.5 * erfc(-x/sqrt(2))\nstd_norm_cdf(x::Array{T}) where {T &lt;: Real} = 0.5 .* erfc(-x./sqrt(2))\n\nfunction tauchen(N::Integer, ρ::T1, σ::T2, μ=zero(promote_type(T1, T2)), n_std::T3=3) where {T1 &lt;: Real, T2 &lt;: Real, T3 &lt;: Real}\n    # Get discretized space\n    a_bar = n_std * sqrt(σ^2 / (1 - ρ^2))\n    y = range(-a_bar, stop=a_bar, length=N)\n    d = y[2] - y[1]\n\n    # Get transition probabilities\n    Π = zeros(promote_type(T1, T2), N, N)\n    for row = 1:N\n        # Do end points first\n        Π[row, 1] = std_norm_cdf((y[1] - ρ*y[row] + d/2) / σ)\n        Π[row, N] = 1 - std_norm_cdf((y[N] - ρ*y[row] - d/2) / σ)\n\n        # fill in the middle columns\n        for col = 2:N-1\n            Π[row, col] = (std_norm_cdf((y[col] - ρ*y[row] + d/2) / σ) -\n                           std_norm_cdf((y[col] - ρ*y[row] - d/2) / σ))\n        end\n    end\n\n    yy = y .+ μ / (1 - ρ) # center process around its mean (wbar / (1 - rho)) in new variable\n\n    (;transitions=Π, values=yy)\n\nend\n\ntauchen (generic function with 3 methods)\n\n\n1. Take \\(\\rho=0.95, \\mu=0.1, \\nu=0.1\\). Approximate the AR1 with 200 discrete states, using the tauchen function above. Check that all rows sum to 1. Compute and plot the steady-state distribution.\n\n# you code here\n\nConsider a firm whose productivity \\(y_t\\) is exogenous and evolves according to the markov chain above.\nProfits are given by \\(\\pi(y_t) = y_t\\).\nAt the start of each period, the firm decides whether to remain in operation and receive current profit \\(\\pi_t\\) or to exit and receive scrap value \\(s&gt;0\\) for the sale of physical assets.\nTime is discounted using interest rate, that is \\(\\beta=\\frac{1}{1+r} \\in [0,1[\\).\nThe following code creates a parameterization of the firm’s problem:\n\n\"Creates an instance of the firm exit model.\"\nfunction create_exit_model(;\n    n=200, # productivity grid size\n    ρ=0.95, μ=0.1, ν=0.1, # persistence, mean and volatility\n    β=0.98, s=100.0 # discount factor and scrap value\n    )\n    mc = tauchen(n, ρ, ν, μ)\n    z_vals, Q = mc.state_values, mc.p\n    return (; n, z_vals, Q, β, s)\nend\n\ncreate_exit_model\n\n\n2. What are the states of the problem? The controls? The rewards? What equation defines the value of the firm? How would you represent numerically the value function and the decision rule?\n\n# you code here\n\n3. Solve for the optimal exit decision using value function iteration. Plot the results.\n\n# you code here\n\n4. (bonus) Taking into account the specific nature of this problem, propose a more efficient algorithm.\n\n# you code here"
  }
]