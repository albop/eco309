<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.549">

  <meta name="author" content="Pablo Winant">
  <meta name="dcterms.date" content="2024-03-28">
  <title>Introduction to Computational Economics - Discrete Dynamic Programming</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Discrete Dynamic Programming</h1>
  <p class="subtitle">Computational Economics (ECO309)</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Pablo Winant 
</div>
</div>
</div>

  <p class="date">2024-03-28</p>
</section><section id="TOC">
<nav role="doc-toc"> 
<h2 id="toc-title">Table of contents</h2>
<ul>
<li><a href="#/markov-chain-and-markov-process" id="/toc-markov-chain-and-markov-process">Markov chain and Markov process</a></li>
<li><a href="#/dynamic-programing-notations" id="/toc-dynamic-programing-notations">Dynamic Programing: notations</a></li>
<li><a href="#/finite-horizon-dmdp" id="/toc-finite-horizon-dmdp">Finite horizon DMDP</a></li>
<li><a href="#/infinite-horizon-dmdp" id="/toc-infinite-horizon-dmdp">Infinite horizon DMDP</a></li>
<li><a href="#/example-the-mccall-model" id="/toc-example-the-mccall-model">Example : the McCall Model</a></li>
</ul>
</nav>
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
</section>
<section id="section" class="slide level2">
<h2></h2>
<p>The imperialism of Dynamic Programming</p>
<p>— Recursive Macroeconomic Theory (Ljunqvist &amp; Sargent)</p>
</section>
<section id="section-1" class="slide level2">
<h2></h2>
<div style="text-align:justify">
<p>I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, “Where did the name, dynamic programming, come from?” The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word “research”. I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word “programming”. I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let’s kill two birds with one stone. Let’s take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it’s impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought <mark>dynamic programming</mark> was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.</p>
</div>
<p>— Richard Bellman, Eye of the Hurricane: An Autobiography (1984, page 159)</p>
</section>
<section>
<section id="markov-chain-and-markov-process" class="title-slide slide level1 center">
<h1>Markov chain and Markov process</h1>

</section>
<section id="markov-chain-and-markov-process-1" class="slide level2">
<h2>Markov chain and Markov process</h2>
<ul>
<li>Stochastic process: family of random variables indexed by time</li>
<li>A stochastic process has the Markov property if its future evolution depends only on its current state.</li>
<li>Special cases:</li>
</ul>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 37%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Discrete States</th>
<th>Continuous States</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Discrete Time</td>
<td>Discrete Markov Chain</td>
<td>Continuous Markov Chain</td>
</tr>
<tr class="even">
<td>Continuous Time</td>
<td>Markov Jump Process</td>
<td>Markov Process</td>
</tr>
</tbody>
</table>
</section>
<section id="stochastic-matrices" class="slide level2">
<h2>Stochastic matrices</h2>
<ul>
<li>a matrix <span class="math inline">\(M \in R^n\times R^n\)</span> matrix is said to be <strong>stochastic</strong> if
<ul>
<li>all coefficents are non-negative</li>
<li>all the lines lines sum to 1 (<span class="math inline">\(\forall i, \sum_j M_{ij} = 1\)</span>)</li>
</ul></li>
<li>a <strong>probability density</strong> is a vector <span class="math inline">\(\mu \in R^n\)</span> such that :
<ul>
<li>all components are non-negative</li>
<li>all coefficients sum to 1 (<span class="math inline">\(\sum_{i=1}^n \mu_{i} = 1\)</span>)</li>
</ul></li>
<li>a <strong>distribution</strong> is a vector with such that:
<ul>
<li>all components are non-negative</li>
</ul></li>
</ul>
</section>
<section id="simulation" class="slide level2">
<h2>Simulation</h2>
<ul>
<li><p>Consider: <span class="math inline">\(\mu_{i,t+1}' =\mu_t' P\)</span></p></li>
<li><p>We have <span class="math inline">\(\mu_{i,t+1} = \sum_{k=1}^n  \mu_{k,t}  P_{k, i}\)</span></p></li>
<li><p>And: <span class="math inline">\(\sum_i\mu_{i,t+1} = \sum_i \mu_{i,t}\)</span></p></li>
<li><p>Postmultiplication by a stochastic matrix preserves the mass.</p></li>
<li><p>Interpretation: <span class="math inline">\(P_{ij}\)</span> is the fraction of the mass initially in state <span class="math inline">\(i\)</span> which ends up in <span class="math inline">\(j\)</span></p></li>
</ul>
</section>
<section id="example" class="slide level2">
<h2>Example</h2>
<p><span class="math display">\[\underbrace{
\begin{pmatrix}
? &amp; ? &amp; ?
\end{pmatrix}
}_{\mu_{t+1}'} = \underbrace{
\begin{pmatrix}
0.5 &amp; 0.3 &amp; 0.2
\end{pmatrix}
}_{\mu_t'} \begin{pmatrix}
0.4 &amp; 0.6 &amp; 0.0 \\\\
0.2 &amp; 0.5 &amp; 0.3 \\\\
0 &amp; 0 &amp; 1.0
\end{pmatrix}\]</span></p>
<div class="fragment">
<p>Graphical Representation:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="index_files/figure-revealjs/mermaid-figure-1.png" style="width:10in;height:5in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</div>
</section>
<section id="probabilistic-interpretation" class="slide level2">
<h2>Probabilistic interpretation</h2>
<ul>
<li>Denote by <span class="math inline">\(S=(s_1,...s_n)\)</span> a finite set with <span class="math inline">\(n\)</span> elements (<span class="math inline">\(|S|=n\)</span>).</li>
<li>A <strong>Markov Chain</strong> with values in <span class="math inline">\(S\)</span> and with transitions given by a stochastic matrix <span class="math inline">\(P\in R^n\times R^n\)</span> identfies a <strong>stochastic process</strong> <span class="math inline">\((X_t)_{t\geq 0}\)</span> such that <span class="math display">\[P_{ij} = Prob(X_{t+1}=s_j|X_t=s_i)\]</span></li>
<li>In words, line <span class="math inline">\(i\)</span> describes the conditional distribution of <span class="math inline">\(X_{t+1}\)</span> conditional on <span class="math inline">\(X_t=s_i\)</span>.</li>
</ul>
</section>
<section id="what-about-longer-horizons" class="slide level2">
<h2>What about longer horizons?</h2>
<ul>
<li><p>It is easy to show that for any <span class="math inline">\(k\)</span>, <span class="math inline">\(P^k\)</span> is a stochastic matrix.</p></li>
<li><p><span class="math inline">\(P^k_{ij}\)</span> denotes the probability of ending in <span class="math inline">\(j\)</span>, after <span class="math inline">\(k\)</span> periods, starting from <span class="math inline">\(i\)</span></p></li>
<li><p>Given an initial distribution <span class="math inline">\(\mu_0\in R^{+ n}\)</span></p>
<ul>
<li>Which states will visited with positive probability between t=0 and t=k?</li>
<li>What happens in the very long run?</li>
</ul></li>
<li><p>We need to study a little bit the properties of Markov Chains</p></li>
</ul>
</section>
<section id="connectivity" class="slide level2">
<h2>Connectivity</h2>
<ul>
<li><p>Two states <span class="math inline">\(s_i\)</span> and <span class="math inline">\(s_j\)</span> are connected if <span class="math inline">\(P_{ij}&gt;0\)</span></p></li>
<li><p>We call incidence matrix: <span class="math inline">\(\mathcal{I}(P)=(\delta_{P_{ij}&gt;0})_{ij}\)</span></p></li>
<li><p>Two states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> communicate with each other if there are <span class="math inline">\(k\)</span> and <span class="math inline">\(l\)</span> such that: <span class="math inline">\((P^k)_ {i,j}&gt;0\)</span> and <span class="math inline">\((P^l)_ {j,i}&gt;0\)</span></p>
<ul>
<li>it is an equivalence relation</li>
<li>we can define equivalence classes</li>
</ul></li>
<li><p>A stochastic matrix <span class="math inline">\(P\)</span> is irreducible if all states communicate</p>
<ul>
<li>there is a unique communication class</li>
</ul></li>
</ul>
</section>
<section id="connectivity-and-irreducibility" class="slide level2">
<h2>Connectivity and irreducibility</h2>
<div class="columns">
<div class="column">
<h5 id="irreducible">Irreducible</h5>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="index_files/figure-revealjs/mermaid-figure-5.png" style="width:10in;height:5in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p><strong>Irreducible</strong>: all states can be reached with positive probability from any initial state.</p>
</div><div class="column">
<h5 id="not-irreducible">Not irreducible</h5>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="index_files/figure-revealjs/mermaid-figure-4.png" style="width:10in;height:5in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>Here there is a subset of states (poor), which absorbs all the mass coming in.</li>
</ul>
</div>
</div>
</section>
<section id="aperiodicity" class="slide level2">
<h2>Aperiodicity</h2>
<ul>
<li>Are there cycles? Starting from a state <span class="math inline">\(i\)</span>, how long does it take to return to <span class="math inline">\(i\)</span>?</li>
<li>The <strong>period</strong> of a state is defined as <span class="math display">\[gcd( {k\geq 1 | (P^k)_{i,i}&gt;0} )\]</span></li>
<li>If a state has a period d&gt;1 the chain returns to the state only at dates multiple of d.</li>
</ul>
</section>
<section id="aperiodicity-1" class="slide level2">
<h2>Aperiodicity</h2>
<div class="columns">
<div class="column">
<h5 id="periodic">Periodic</h5>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="index_files/figure-revealjs/mermaid-figure-3.png" style="width:10in;height:4.82in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>If you start from some states, you return to it, but not before two periods.</li>
</ul>
</div><div class="column">
<h5 id="aperiodic">Aperiodic</h5>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<p><img data-src="index_files/figure-revealjs/mermaid-figure-2.png" style="width:10in;height:2.14in"></p>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ul>
<li>If some mass leaves a state, some of it returns to the state in the next period.</li>
</ul>
</div>
</div>
</section>
<section id="stationary-distribution" class="slide level2">
<h2>Stationary distribution</h2>
<ul>
<li><p><span class="math inline">\(\mu\)</span> is a <strong>stationary</strong> distribution if <span class="math inline">\(\mu' = \mu' P\)</span></p></li>
<li><p>Theorem: there always exists such a distribution</p>
<ul>
<li>proof: Brouwer theorem (fixed-point result for compact-convex set)</li>
<li><span class="math inline">\(f: \mu\rightarrow (\mu'P)'\)</span></li>
</ul></li>
<li><p>Theorem:</p>
<ol type="1">
<li>if P is irreducible the fixed point <span class="math inline">\(\mu^{\star}\)</span> is unique</li>
<li>if P is irreducible and aperiodic <span class="math inline">\(|\mu_0' P^k - \mu^{\star}| \underset{k\to+\infty}{\longrightarrow}0\)</span> for any initial distribution <span class="math inline">\(\mu_0\)</span></li>
</ol></li>
<li><p>We then say the Markov chain is <strong>ergodic</strong></p></li>
<li><p><span class="math inline">\(\mu^{\star}\)</span> is the <strong>ergodic distribution</strong></p>
<ul>
<li>it is the best guess, one can do for the state of the chain in the very far future</li>
</ul></li>
</ul>
</section>
<section id="stationary-distribution-proof" class="slide level2">
<h2>Stationary distribution (proof)</h2>
<ul>
<li><strong>Brouwer’s theorem</strong>: Let <span class="math inline">\(\mathcal{C}\)</span> be a compact convex subset of <span class="math inline">\(R^n\)</span> and <span class="math inline">\(f\)</span> a continuous mapping <span class="math inline">\(\mathcal{C}\rightarrow \mathcal{C}\)</span>. Then there exists a fixed point <span class="math inline">\(x_0\in \mathcal{C}\)</span> such that <span class="math inline">\(f(x_0)=x_0\)</span></li>
<li>Result hinges on:
<ul>
<li>continuity of <span class="math inline">\(f: \mu \mapsto \mu P\)</span></li>
<li>convexity of <span class="math inline">\(\\{x \in R^n | |x|=1 \\}\)</span> (easy to check)</li>
<li>compactness of <span class="math inline">\(\\{x \in R^n | |x|=1 \\}\)</span>
<ul>
<li>it is bounded</li>
<li>and closed (the inverse image of 1 for <span class="math inline">\(u\mapsto |u|\)</span> which is continuous)</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="stationary-distribution-1" class="slide level2">
<h2>Stationary distribution?</h2>
<p>How do we compute the stationary distribution?</p>
<ul>
<li>Simulation</li>
<li>Linear algebra</li>
<li>Decomposition</li>
</ul>
</section>
<section id="simulating-a-markov-chain" class="slide level2">
<h2>Simulating a Markov Chain</h2>
<ul>
<li>Very simple idea:
<ul>
<li>start with any <span class="math inline">\(\mu_0\)</span> and compute the iterates recursively</li>
<li><span class="math inline">\(\mu_{n+1}' = \mu_n' P\)</span></li>
<li>convergence is linear:
<ul>
<li><span class="math inline">\(|\mu_{n+1} - \mu_n| \leq |P| |\mu_n - \mu_{n-1}|\)</span></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="using-linear-algebra" class="slide level2">
<h2>Using Linear Algebra</h2>
<ul>
<li>Find the solution of <span class="math inline">\(\mu'(P-I) = 0\)</span> ?
<ul>
<li>not well defined, 0 is a solution</li>
<li>we need to incorporate the constraint <span class="math inline">\(\sum_i(\mu_i)=1\)</span></li>
</ul></li>
<li>Method:
<ul>
<li>Define <span class="math inline">\(M_{ij} =  \begin{cases} 1  &amp;\text{if} &amp; j =0 \\\\ (P-I)_{ij}  &amp; \text{if} &amp; j&gt; 1  \end{cases}\)</span></li>
<li>Define <span class="math inline">\(D_i = \begin{cases} 1 &amp; \text{if} &amp; j = 0 \\\\0 &amp; \text{if} &amp; j&gt;0 \end{cases}\)</span></li>
<li>With a linear algebra solver
<ul>
<li>look for a solution <span class="math inline">\(\mu\)</span> of <span class="math inline">\(\mu' M = D\)</span></li>
<li>or <span class="math inline">\(M^{\prime} \mu = D\prime\)</span></li>
<li>if you find a solution, it is unique (theorem)</li>
</ul></li>
</ul></li>
<li>Alternative:
<ul>
<li>minimize residual squares of overidentified system</li>
</ul></li>
</ul>
<!-- TODO add an example matrix -->
</section>
<section id="code-example" class="slide level2">
<h2>Code example</h2>
<div class="sourceCode" id="cb1" data-code-line-numbers="[1-2|3-6|7-9|10-12|13-14|15-17]"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a></a><span class="co"># we use the identity matrix and the \ operator</span></span>
<span id="cb1-2"><a></a><span class="im">using</span> <span class="bu">LinearAlgebra</span>: I, \</span>
<span id="cb1-3"><a></a><span class="co"># define a stochastic matrix (lines sum to 1)</span></span>
<span id="cb1-4"><a></a>P <span class="op">=</span> [  <span class="fl">0.9</span>  <span class="fl">0.1</span> <span class="fl">0.0</span>  ;</span>
<span id="cb1-5"><a></a>       <span class="fl">0.05</span> <span class="fl">0.9</span> <span class="fl">0.05</span> ;</span>
<span id="cb1-6"><a></a>       <span class="fl">0.0</span>  <span class="fl">0.9</span> <span class="fl">0.1</span>  ]</span>
<span id="cb1-7"><a></a><span class="co"># define an auxiliary matrix</span></span>
<span id="cb1-8"><a></a>M <span class="op">=</span> P<span class="op">'</span> <span class="op">-</span> I</span>
<span id="cb1-9"><a></a>M[<span class="kw">end</span>,<span class="op">:</span>] <span class="op">.=</span> <span class="fl">1.0</span></span>
<span id="cb1-10"><a></a><span class="co"># define rhs</span></span>
<span id="cb1-11"><a></a>R <span class="op">=</span> <span class="fu">zeros</span>(<span class="fl">3</span>)</span>
<span id="cb1-12"><a></a>R[<span class="kw">end</span>] <span class="op">=</span> <span class="fl">1</span></span>
<span id="cb1-13"><a></a><span class="co"># solve the system</span></span>
<span id="cb1-14"><a></a>μ <span class="op">=</span> M<span class="op">\</span>R</span>
<span id="cb1-15"><a></a><span class="co"># check that you have a solution:</span></span>
<span id="cb1-16"><a></a><span class="pp">@assert</span> <span class="fu">sum</span>(μ) <span class="op">==</span> <span class="fl">1</span></span>
<span id="cb1-17"><a></a><span class="pp">@assert</span> <span class="fu">all</span>(<span class="fu">abs</span>.(μ<span class="ch">'P - μ'</span>)<span class="op">.&lt;</span><span class="fl">1e-10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="further-comments" class="slide level2">
<h2>Further comments</h2>
<ul>
<li>Knowledge about the structure of the Markov Chain can help speedup the calculations</li>
<li>There are methods for potentially very-large linear system
<ul>
<li>Newton-Krylov based methods, GMRES</li>
</ul></li>
<li>Basic algorithms are easy to implement by hand</li>
<li>QuantEcon toolbox has very good methods to study markov chains</li>
</ul>
</section></section>
<section>
<section id="dynamic-programing-notations" class="title-slide slide level1 center">
<h1>Dynamic Programing: notations</h1>

</section>
<section id="section-2" class="slide level2">
<h2></h2>
<p>&nbsp;&nbsp;</p>
<p>Consider the following problems:</p>
<p>&nbsp;&nbsp;<br>
</p>
<div class="columns">
<div class="column" style="width:25%;">
<p>Monopoly pricing:</p>
<p><span class="math display">\[\max_{q} \pi(q) - c(q)\]</span></p>
</div><div class="column" style="width:25%;">
<p>Shopping problem</p>
<p><span class="math display">\[\max_{\substack{c_1, c_2 \\ p_1 c_1 + p_2 c_2 \leq B}} U(c_1,c_2)\]</span></p>
</div><div class="column" style="width:50%;">
<p>Consumption Savings</p>
<p><span class="math display">\[\max_{\substack{c() \\ w_{t+1}=(w_t-c(w_t))(1+r)) + y_{t+1}}} E_0 \sum_t \beta^t U(c(w_t))\]</span></p>
</div>
</div>
<p>&nbsp;&nbsp;<br>
</p>
<div class="fragment">
<table style="width:100%;">
<colgroup>
<col style="width: 15%">
<col style="width: 13%">
<col style="width: 25%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Problem</th>
<th>objective</th>
<th>action</th>
<th>state</th>
<th>transition</th>
<th>type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>monopoly pricing</td>
<td>profit</td>
<td>choose quantity to produce</td>
<td></td>
<td></td>
<td>optimization</td>
</tr>
<tr class="even">
<td>shopping problem</td>
<td>utility</td>
<td>choose consumption composition</td>
<td>budget <span class="math inline">\(B\)</span></td>
<td></td>
<td>comparative statics</td>
</tr>
<tr class="odd">
<td>consumption/savings</td>
<td>expected welfare</td>
<td>save or consume</td>
<td>available income</td>
<td>evolution of wealth</td>
<td>dynamic optimization</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="general-formulation" class="slide level2">
<h2>General Formulation</h2>
<h3 id="markov-decision-problem">Markov Decision Problem</h3>
<div class="columns">
<div class="column">
<ul>
<li><!-- .element class="fragment" -->
<b>Environment</b>
<ul>
<li>states: <span class="math inline">\(s \in S\)</span></li>
<li>actions: <span class="math inline">\(x \in X(s)\)</span></li>
<li>transitions: <span class="math inline">\(\pi(s'| s, x) \in S\)</span>
<ul>
<li><span class="math inline">\(probability\)</span> of going to <span class="math inline">\(s'\)</span> in state <span class="math inline">\(s\)</span>…</li>
<li>… given action <span class="math inline">\(x\)</span></li>
</ul></li>
</ul></li>
</ul>
</div><div class="column">
<ul>
<li><!-- .element class="fragment" -->
<b>Reward</b>: <span class="math inline">\(r(s,x) \in R\)</span>
<ul>
<li>aka felicity, intratemporal utility</li>
</ul></li>
<li><!-- .element class="fragment" -->
<b>Policy</b>: <span class="math inline">\(x(): s \rightarrow x\in X(s)\)</span>
<ul>
<li>a.k.a. decision rule</li>
<li>we consider <em>deterministic</em> policy</li>
<li>given <span class="math inline">\(x()\)</span>, the evolution of <span class="math inline">\(s\)</span> is a Markov process
<ul>
<li><span class="math inline">\(\pi(. |s, x())\)</span> is a distribution for <span class="math inline">\(s'\)</span> over <span class="math inline">\(S\)</span></li>
<li>it depends only on <span class="math inline">\(s\)</span></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="objective" class="slide level2">
<h2>Objective</h2>
<ul>
<li>expected <strong>lifetime reward</strong>:
<ul>
<li>value of following policy <span class="math inline">\(x()\)</span> starting from <span class="math inline">\(s\)</span>: <span class="math display">\[R(s; x()) =  E_0 \sum_t^T \delta^t \left[ r_t\right]\]</span></li>
<li><span class="math inline">\(\delta \in [0,1[\)</span>: discount factor</li>
<li>horizon: <span class="math inline">\(T \in \\{N, \infty\\}\)</span></li>
</ul></li>
<li>value of a state <span class="math inline">\(s\)</span>
<ul>
<li>value of following the optimal policy starting from <span class="math inline">\(s\)</span> <span class="math display">\[V(s) = \max_{ x()} R(s, x())\]</span></li>
<li><span class="math inline">\(V()\)</span> is the value function (t.b.d.)</li>
</ul></li>
</ul>
</section>
<section id="classes-of-dynamic-optimization" class="slide level2">
<h2>Classes of Dynamic Optimization</h2>
<ul>
<li>The formulation so far is very general. It encompasses several variants of the problem:
<ul>
<li>finite horizon vs infinite horizon</li>
<li>discrete-space problem vs continuous-state space problem</li>
<li>some learning problems (reinforcement learning…)</li>
</ul></li>
<li>There are also variants not included:
<ul>
<li>non time-separable problems</li>
<li>non time-homogenous problems</li>
<li>some learning problems (bayesian updating, …)</li>
</ul></li>
</ul>
</section>
<section id="finite-horizon-vs-infinite-horizon" class="slide level2">
<h2>Finite horizon vs infinite horizon</h2>
<ul>
<li>Recall objective: <span class="math inline">\(V(s; x()) =  \max E_0\sum_{t=0}^T \delta^t \left[ r(s_t, x_t) \right]\)</span></li>
<li>If <span class="math inline">\(T&lt;\infty\)</span>, the decision in the last periods, will be different from the periods before
<ul>
<li>one must find a decision rule <span class="math inline">\(\pi_t()\)</span> per period</li>
<li>or, equivalently, add <span class="math inline">\(t\)</span> to the state space: <span class="math inline">\(\tilde{S}=S\times[0,T]\)</span></li>
</ul></li>
<li>If <span class="math inline">\(T=\infty\)</span>, the continuation value of being in state <span class="math inline">\(s_t\)</span> is independent from <span class="math inline">\(t\)</span></li>
</ul>
<p><span class="math display">\[V(s; x()) = E_0 \max \sum_ {t=0}^{T_0} \delta^t \left[ r(s_t, x_t) \right] + \delta^{T_0} E_0  \sum_ {t=T_0}^{\infty} \delta^t \left[ r(s_t, x_t) \right]\]</span></p>
<p><span class="math display">\[ = E_0 \left[ \max \sum_ {t=0}^{T_0} \delta^t \left[ r(s_t, x_t) \right] +  \delta^{T_0} V(s_ {T_0}; x()) \right]\]</span></p>
</section>
<section id="continuous-vs-discrete" class="slide level2">
<h2>Continuous vs discrete</h2>
<ul>
<li><strong>Discrete Dynamic Programming</strong> (today)
<ul>
<li>discrete states: <span class="math inline">\(s \in {s_1, \cdots, s_N}\)</span></li>
<li>discrete controls: <span class="math inline">\(|X(s)|&lt;\infty\)</span></li>
<li>there is a finite number of policies, the can be represented exactly</li>
<li>unless <span class="math inline">\(|S|\)</span> is very large (cf go game)</li>
</ul></li>
<li><strong>Continuous problem</strong>:
<ul>
<li><span class="math inline">\(x(s)\)</span>, <span class="math inline">\(V(s; \pi)\)</span> require an infinite number of coefficients</li>
<li>same general approach but different implementation</li>
<li>two main variants:
<ul>
<li><strong>discretize</strong> the initial problem: back to DDP</li>
<li>use <strong>approximation</strong> techniques (i.e.&nbsp;interpolation)</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="non-time-separable-example" class="slide level2">
<h2>Non time separable example</h2>
<ul>
<li>For instance Epstein-Zin preferences: <span class="math display">\[\max V(;c())\]</span> where <span class="math display">\[V_t = (1-\delta) \frac{c_t^{1-\sigma}}{1-\sigma} + \delta \left[ E_t V_{t+1}^{\alpha} \right]^{\frac{1}{\alpha}}\]</span></li>
<li>Why would you do that?
<ul>
<li>to disentangle risk aversion and elasticity of intertemporal substitution</li>
<li>robust control</li>
</ul></li>
<li>You can still use ideas from Dynamic Programming.</li>
</ul>
</section>
<section id="non-homogenous-preference" class="slide level2">
<h2>Non homogenous preference</h2>
<ul>
<li><p>Look at the <span class="math inline">\(\alpha-\beta\)</span> model. <span class="math display">\[V_t = \max \sum_t^{\infty} \beta_t U(c_t)\]</span> where <span class="math inline">\(\delta_0 = 1\)</span>, <span class="math inline">\(\delta_1=\alpha\)</span>, <span class="math inline">\(\delta_k=\alpha\beta^{k-1}\)</span></p></li>
<li><p>Makes the problem time-inconsistent:</p>
<ul>
<li>the optimal policy you would choose for the continuation value after <span class="math inline">\(T\)</span> is not the same if you maximize it in expectation from <span class="math inline">\(0\)</span> or at <span class="math inline">\(T\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="learning-problems" class="slide level2">
<h2>Learning problems</h2>
<ul>
<li>Bayesian learning: Uncertainty about some model parameters
<ul>
<li>ex: variance and return of a stock market</li>
<li>agent models this uncertainty as a distribution</li>
<li>agent updates his priors after observing the result of his actions</li>
<li>actions are taken optimally taken into account the revelation power of some actions</li>
</ul></li>
<li>Is it good?
<ul>
<li>clean: the <em>rational</em> thing to do with uncertainty</li>
<li>super hard: the state-space should contain all possible priors</li>
<li>mathematical cleanness comes with many assumptions</li>
</ul></li>
<li>Used to estimate rather big (mostly linear) models</li>
</ul>
</section>
<section id="learning-problems-2" class="slide level2">
<h2>Learning problems (2)</h2>
<ul>
<li>Reinforcement learning
<ul>
<li>model can be partially or totally unknown</li>
<li>decision rule is updated by observing the reward from actions
<ul>
<li>no priors</li>
</ul></li>
<li>solution does not derive directly from model
<ul>
<li>can be used to solve dynamic programming problems</li>
</ul></li>
</ul></li>
<li>Good solutions maximize a criterion similar to lifetime reward but are usually not optimal:
<ul>
<li>usually evaluated by replaying the game many times</li>
<li>tradeoff exploration / exploitations</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="finite-horizon-dmdp" class="title-slide slide level1 center">
<h1>Finite horizon DMDP</h1>

</section>
<section id="finite-horizon-dmdp-1" class="slide level2">
<h2>Finite horizon DMDP</h2>
<p>When <span class="math inline">\(T&lt;\infty\)</span>. With discrete action the problem can be represented by a tree.</p>

<img data-src="graphs/ddp.svg" class="r-stretch"></section>
<section id="finite-horizon-dmdp-2" class="slide level2">
<h2>Finite horizon DMDP</h2>
<ul>
<li>Intuition: backward induction.
<ul>
<li>Find optimal policy <span class="math inline">\(x_T(s_T)\)</span> in all terminal states <span class="math inline">\(s_T\)</span>. Set <span class="math inline">\(V_T(s_T)\)</span> equal to <span class="math inline">\(r(s_T, \pi_T)\)</span></li>
<li>For each state <span class="math inline">\(s_{k-1}\in S\)</span> find <span class="math inline">\(x_{k-1}\in X(s_{k-1})\)</span> which maximizes <span class="math display">\[V_{k-1}(s_{k-1}) = \max_{x_{k-1}(s_{k-1})\in X(s_{k-1})}r(s_{k-1},x_{k-1}) + \delta \underbrace{ \sum_{s_k\in S} \pi(s_k | s_{k-1}, x_{k-1} ) V_k(s_k)} _{ \textit{expected continuation value} }\]</span></li>
</ul></li>
<li>Policies <span class="math inline">\(x_0(), ... x_T()\)</span> are <strong>Markov-perfect</strong>:
<ul>
<li>they maximize utility on all subsets of the “game”</li>
<li>also from t=0</li>
</ul></li>
</ul>
</section>
<section id="remarks" class="slide level2">
<h2>Remarks</h2>
<ul>
<li>Can we do better than this naive algorithm?
<ul>
<li>not really</li>
<li>but we can try to limit <span class="math inline">\(S\)</span> to make the maximization step faster</li>
<li>exclude a priori some branches in the tree using knowledge of the problem</li>
</ul></li>
</ul>
<!-- [TODO: chess: bongcloud attack] -->
</section></section>
<section>
<section id="infinite-horizon-dmdp" class="title-slide slide level1 center">
<h1>Infinite horizon DMDP</h1>

</section>
<section id="infinite-horizon-dmdp-1" class="slide level2">
<h2>Infinite horizon DMDP</h2>
<ul>
<li>Horizon is infinite: <span class="math display">\[V(s) =  \max E_0 \sum_{t=0}^{\infty} \delta^t r(s_t, x_t) \]</span></li>
<li>Intuition:
<ul>
<li>let’s consider the finite horizon version <span class="math inline">\(T&lt;\infty\)</span> and <span class="math inline">\(T &gt;&gt; 1\)</span></li>
<li>compute the solution, increase <span class="math inline">\(T\)</span> until the solution doesn’t change</li>
<li>in practice: take an initial guess for <span class="math inline">\(V_{T}\)</span> then compute optimal <span class="math inline">\(V_{T-1}\)</span>, <span class="math inline">\(V_{T_2}\)</span> and so on, until convergence of the <span class="math inline">\(V\)</span>s</li>
</ul></li>
</ul>
</section>
<section id="infinite-horizon-dmdp-2" class="slide level2">
<h2>Infinite horizon DMDP (2)</h2>
<ul>
<li>This is possible, it’s called <strong>Successive Approximation</strong> or <strong>Value Function Iteration</strong>
<ul>
<li>how fast does it converge? <em>linearly</em></li>
<li>can we do better? yes, <em>quadratically</em>
<ul>
<li>with <strong>howard improvement</strong> steps</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="successive-approximation" class="slide level2">
<h2>Successive Approximation</h2>
<ul>
<li>Consider the decomposition: <span class="math display">\[V(s; x()) = E_0 \sum_{t=0}^{\infty} \delta^t r(s_t, x_t) = E_0 \left[ r(s, x(s)) + \sum_{t=1}^{\infty} \delta^t r(s_t, x_t) \right]\]</span></li>
</ul>
<p>or</p>
<p><span class="math display">\[V(s; x()) =  r(s, x(s)) + \delta \sum_{s'} p(s'|s,x(s)) V(s'; x()) \]</span></p>
</section>
<section id="successive-approximation-2" class="slide level2">
<h2>Successive Approximation (2)</h2>
<ul>
<li>Taking continuation value as given we can certainly improve the value in every state <span class="math inline">\(\tilde{V}\)</span> by choosing <span class="math inline">\(\tilde{x}()\)</span> so as to maximize <span class="math display">\[\tilde{V}(s; \tilde{x}(), x()) =  r(s, \tilde{x}(s)) + \delta \sum_{s'} \pi(s'|s,\tilde{x}(s) )V(s'; x()) \]</span></li>
<li>By construction: <span class="math inline">\(\forall s, \tilde{V}(s, \tilde{x}(), x()) &gt; {V}(s, x())\)</span>
<ul>
<li>it is an <strong>improvement step</strong></li>
</ul></li>
<li>Can <span class="math inline">\({V}(s, \tilde{x}())\)</span> be worse for some states than <span class="math inline">\({V}(s, x())\)</span> ?
<ul>
<li>actually no</li>
</ul></li>
</ul>
</section>
<section id="bellman-equation" class="slide level2">
<h2>Bellman equation</h2>
<ul>
<li>Idea:
<ul>
<li>it should not be possible to improve upon the optimal solution.</li>
<li>Hence the optimal value <span class="math inline">\(V\)</span> and policy <span class="math inline">\(x^{\star}\)</span> should satisfy: <span class="math display">\[\forall s\in S, V(s) = \max_{y(s)} r(s, y(s)) + \delta \sum_{s^{\prime}\in S} \pi(s^{\prime}| s, y(s)) V(s^{\prime})\]</span> with the maximum attained at <span class="math inline">\(x(s)\)</span>.</li>
</ul></li>
<li>This is referred to as the <strong>Bellman equation</strong>.</li>
<li>Conversely, it is possible to show that a solution to the Bellman equation is also an optimal solution to the initial problem.</li>
</ul>
</section>
<section id="bellman-operator" class="slide level2">
<h2>Bellman operator</h2>
<ul>
<li>The function <span class="math inline">\(G\)</span> is known as the <strong>Bellman operator</strong>: <span class="math display">\[G: V \rightarrow \max_{y(s)} r(s, y(s)) + \delta \sum_{s^{\prime}\in S} \pi(s^{\prime}| s, y(s)) V(s^{\prime})\]</span></li>
<li>Define sequence <span class="math inline">\(V_n = G(V_{n-1})\)</span>
<ul>
<li>it goes back in time</li>
<li>but is <em>not</em> the time-iteration operator</li>
</ul></li>
<li>Optimal value is a fixed point of G</li>
<li>Does <span class="math inline">\(G\)</span> converges to it ? Yes, if <span class="math inline">\(G\)</span> is a contraction mapping.</li>
</ul>
</section>
<section id="blackwells-theorem" class="slide level2">
<h2>Blackwell’s theorem</h2>
<ul>
<li>Let <span class="math inline">\(X\subset R^n\)</span> and let <span class="math inline">\(\mathcal{C}(X)\)</span> be a space of bounded functions <span class="math inline">\(f: X\rightarrow  R\)</span>, with the sup-metric. <span class="math inline">\(B: \mathcal{C}(X)\rightarrow \mathcal{C}(X)\)</span> be an operator satisfying two conditions:
<ol type="1">
<li>(monotonicity) if <span class="math inline">\(f,g \in \mathcal{C}(X)\)</span> and <span class="math inline">\(\forall x\in X, f(x)\leq g(x)\)</span> then</li>
</ol>
<span class="math inline">\(\forall x \in X (Bf)(x)\leq(Bg)(x)\)</span>
<ol start="2" type="1">
<li>(discounting) there exists some <span class="math inline">\(\delta\in]0,1[\)</span> such that: <span class="math inline">\(B.(f+a)(x)\leq (B.f)(x) + \delta a, \forall f \in \mathcal{C}(X), a\geq 0, x\in X\)</span></li>
</ol></li>
<li>Then <span class="math inline">\(B\)</span> is a contraction mapping with modulus <span class="math inline">\(\delta\)</span>.</li>
</ul>
</section>
<section id="successive-approximation-1" class="slide level2">
<h2>Successive Approximation</h2>
<ul>
<li><p>Using the Blackwell’s theorem, we can prove the Bellman operator is a contraction mapping.</p></li>
<li><p>This justifies the Value Function Iteration algorithm:</p>
<ul>
<li>choose an initial <span class="math inline">\(V_0\)</span></li>
<li>given <span class="math inline">\(V_n\)</span> compute <span class="math inline">\(V_{n+1} = G(V_n)\)</span></li>
<li>iterate until <span class="math inline">\(|V_{n+1}- V_n|\leq \eta\)</span></li>
</ul></li>
<li><p>Policy rule is deduced from <span class="math inline">\(V\)</span> as the maximand in the Bellman step</p></li>
</ul>
</section>
<section id="successive-approximation-2-1" class="slide level2">
<h2>Successive Approximation (2)</h2>
<p>Assume that <span class="math inline">\(X\)</span> is finite.</p>
<ul>
<li>Note that convergence of <span class="math inline">\(V_n\)</span> is geometric</li>
<li>But <span class="math inline">\(x_n\)</span> converges after a finite number of iteration.
<ul>
<li>surely the latest iterations are suboptimal</li>
<li>they serve only to evaluate the value of <span class="math inline">\(x\)</span></li>
</ul></li>
<li>In fact:
<ul>
<li><span class="math inline">\(V_n\)</span> is never the value of <span class="math inline">\(x_n()\)</span></li>
<li>should we try to keep both in sync?</li>
</ul></li>
</ul>
</section>
<section id="policy-iteration-for-dmdp" class="slide level2">
<h2>Policy iteration for DMDP</h2>
<ul>
<li>Choose initial policy <span class="math inline">\(x_0()\)</span></li>
<li>Given initial guess <span class="math inline">\(x_n()\)</span>
<ul>
<li>compute the value function <span class="math inline">\(V_n=V( ;x_n)\)</span> which satisfies<br>
<span class="math inline">\(\forall s,  V_n(s) = r(s, x_n(s)) + \delta \sum_{s'} \pi(s'| s, x_n(s)) V_n(s')\)</span></li>
<li>improve policy by maximizing in <span class="math inline">\(x_n()\)</span> <span class="math display">\[\max_{x_n()} r(s, x_n(s)) + \delta \sum_{s^{\prime}\in S} \pi(s^{\prime}| s, x_n(s)) V_{n-1}(s^{\prime})\]</span></li>
</ul></li>
<li>Repeat until convergence, i.e.&nbsp;<span class="math inline">\(x_n=x_{n+1}\)</span></li>
<li>One can show the speed of convergence (for <span class="math inline">\(V_n\)</span>) is <em>quadratic</em>
<ul>
<li>it corresponds the Newton-Raphson steps applied to <span class="math inline">\(V\rightarrow G(V)-V\)</span></li>
</ul></li>
</ul>
</section>
<section id="how-do-we-compute-the-value-of-a-policy" class="slide level2">
<h2>How do we compute the value of a policy?</h2>
<ul>
<li><p>Given <span class="math inline">\(x_n\)</span>, goal is to find <span class="math inline">\(V_n(s)\)</span> in <span class="math display">\[\forall s,  V_n(s) = r(s, x_n(s)) + \delta \sum_{s'} \pi(s'| s, x_n(s)) V_n(s')\]</span></p></li>
<li><p>Three approaches:</p>
<ol type="1">
<li>simulate the policy rule and compute <span class="math inline">\(E\left[ \sum_t \delta^t r(s_t, x_t) \right]\)</span> with Monte-Carlo draws</li>
<li>successive approximation:
<ul>
<li>put <span class="math inline">\(V_k\)</span> in the rhs and recompute the lhs <span class="math inline">\(V_{k+1}\)</span>, replace <span class="math inline">\(V_k\)</span> by <span class="math inline">\(V_{k+1}\)</span> and iterate until convergence</li>
</ul></li>
<li>solve a <em>linear</em> system in <span class="math inline">\(V_n\)</span></li>
</ol></li>
<li><p>For 2 and 3 it is useful to constuct a linear operator <span class="math inline">\(M\)</span> such that <span class="math inline">\(V_{n+1} = R_n + \delta M_n .  V_n\)</span></p></li>
</ul>
<!-- Another approach consist: compute $(\sum_{t\geq 0} \delta^t M_n^t)$ -->
</section></section>
<section>
<section id="example-the-mccall-model" class="title-slide slide level1 center">
<h1>Example : the McCall Model</h1>

</section>
<section id="idea" class="slide level2">
<h2>Idea</h2>
<ul>
<li>McCall model:
<ul>
<li>when should an unemployed person accept a job offer?</li>
<li>choice between:
<ul>
<li>wait for a better offer (and receive low unemp. benefits)</li>
<li>accept a suboptimal job offer</li>
</ul></li>
</ul></li>
<li>We present a variant of it, with a small probability of loosing a job.</li>
</ul>
</section>
<section id="formalization" class="slide level2">
<h2>Formalization</h2>
<ul>
<li>When <mark>unemployed</mark> in date, a job-seeker
<ul>
<li>consumes unemployment benefit <span class="math inline">\(c_t = \underline{c}\)</span></li>
<li>receives in every date <span class="math inline">\(t\)</span> a job offer <span class="math inline">\(w_t\)</span>
<ul>
<li><span class="math inline">\(w_t\)</span> is i.i.d.,</li>
<li>takes values <span class="math inline">\(w_1, w_2, w_3\)</span> with probabilities <span class="math inline">\(p_1, p_2, p_3\)</span></li>
</ul></li>
<li>if job-seeker accepts, becomes employed at rate <span class="math inline">\(w_t\)</span> in the next period</li>
<li>else he stays unemployed</li>
</ul></li>
<li>When <mark>employed</mark> at rate <span class="math inline">\(w\)</span>
<ul>
<li>worker consumes salary <span class="math inline">\(c_t = w\)</span></li>
<li>with small probability <span class="math inline">\(\lambda&gt;0\)</span> looses his job:
<ul>
<li>starts next period unemployed</li>
</ul></li>
<li>otherwise stays employed at same rate</li>
</ul></li>
<li>Objective: <span class="math inline">\(\max E_0 \left\{ \sum \beta^t \log(w_t) \right\}\)</span></li>
</ul>
</section>
<section id="states-reward" class="slide level2">
<h2>States / reward</h2>
<ul>
<li>What are the states?
<ul>
<li>employement status: Unemployed / Employed</li>
<li>if Unemployed:
<ul>
<li>the level <span class="math inline">\(w\in {w_1, w_2, w_3}\)</span> of the salary that is currently proposed</li>
</ul></li>
<li>if Employed:
<ul>
<li>the level <span class="math inline">\(w\in {w_1, w_2, w_3}\)</span> at which worker was hired</li>
</ul></li>
<li>current state, can be represented by a 2x3 index</li>
</ul></li>
<li>What are the actions?
<ul>
<li>if Unemployed:
<ul>
<li>reject (false) / accept (true)</li>
</ul></li>
<li>if Employed: None</li>
<li>actions (when unemployed) are represented by a 3 elements binary vector</li>
</ul></li>
<li>What is the (intratemporal) reward?
<ul>
<li>if Unemployed: <span class="math inline">\(U(c)\)</span></li>
<li>if Employed at rate w: <span class="math inline">\(U(w)\)</span></li>
<li>here it doesn’t depend on the action</li>
</ul></li>
</ul>
</section>
<section id="value-function" class="slide level2">
<h2>Value function</h2>
<p><span class="math inline">\(\newcommand{\E}{\mathbb{E}}\)</span></p>
<ul>
<li><p>What is the value of being in a given state?</p></li>
<li><p>If Unemployed, facing current offer <span class="math inline">\(w\)</span>:<br>
<span class="math display">\[V^U(w) = U(\underline{c}) + \max_{a} \begin{cases} \beta V^E(w) &amp; \text{if $a(w)$ is true} \\ \beta  E_{w'}\left[ V^U(w^{\prime}) \right]  &amp; \text{if $a(w)$ is false} \end{cases}\]</span></p></li>
<li><p>If Employed, at rate <span class="math inline">\(w\)</span> <span class="math display">\[V^E(w) = U(w) +  (1-\lambda) \beta V^E(w) +  \lambda \beta E_{w'}\left[ V^U(w^{\prime}) \right] \]</span></p></li>
<li><p>We can represent value as two functions <span class="math inline">\(V^U\)</span> and <span class="math inline">\(V^E\)</span> of the states as</p>
<ul>
<li>two vectors of Floats, with three elements (recall: value-function is real valued)</li>
</ul></li>
</ul>
</section>
<section id="value-function-iteration" class="slide level2">
<h2>Value function iteration</h2>
<ul>
<li>Take a guess for value function <span class="math inline">\(\tilde{V^E}\)</span>, <span class="math inline">\(\tilde{V^U}\)</span>, <em>tomorrow</em></li>
<li>Use it to compute value function <em>today</em>: <span class="math display">\[V^U(w) = U(\underline{c}) + \max_{a(w)} \begin{cases} \beta \tilde{V}^E(w) &amp; \text{if $a(w)$ is true} \\ \beta  E_{w'}\left[ \tilde{V}^U(w^{\prime}) \right]  &amp; \text{if $a(w)$ is false} \end{cases}\]</span> <span class="math display">\[V^E(w) = U(w) +  (1-\lambda) \beta \tilde{V}^E(w) +  \lambda \beta E_{w'}\left[\tilde{V}^U(w^{\prime}) \right] \]</span></li>
<li><span class="math inline">\((\tilde{V}^E, \tilde{V}^U)\mapsto (V^E, V^U)\)</span> is one <em>value iteration</em> step</li>
<li>Note that we don’t have to keep track of policies tomorrow
<ul>
<li>all information about future decisions is contained in <span class="math inline">\(\tilde{V}^E, \tilde{V}^U\)</span></li>
<li>but we can keep track of current policy: <span class="math inline">\(a(w): \arg\max \cdots\)</span></li>
</ul></li>
</ul>
</section>
<section id="value-evaluation" class="slide level2">
<h2>Value evaluation</h2>
<ul>
<li>Suppose we take a policy <span class="math inline">\(a(w)\)</span> as given. What is the value of following this policy forever?</li>
<li>The value function <span class="math inline">\(V_a^E\)</span>, <span class="math inline">\(V_a^U\)</span> satisfies <span class="math display">\[V_a^U(w) = U(\underline{c}) + \begin{cases} \beta {V}^E_a(w) &amp; \text{if $a(w)$ is true} \\ \beta  E_{w'}\left[ {V}^U_a(w^{\prime}) \right]  &amp; \text{if $a(w)$ is false} \end{cases}\]</span> <span class="math display">\[V_a^E(w) = U(w) +  (1-\lambda) \beta {V}^E_a(w) +  \lambda \beta E_{w'}\left[{V}^U_a(w^{\prime}) \right] \]</span></li>
<li>Note the absence of the max function: we don’t reoptimize</li>
</ul>
</section>
<section id="value-evaluation-2" class="slide level2">
<h2>Value evaluation (2)</h2>
<ul>
<li>How do you compute value of policy <span class="math inline">\(a(w)\)</span> recursively?</li>
<li>Iterate: <span class="math inline">\((\tilde{V}^E_a, \tilde{V}^U)\mapsto (V^E_a, V^U_a)\)</span> <span class="math display">\[V_a^U(w) \leftarrow U(\underline{c}) + \begin{cases} \beta \tilde{V}^E_a(w) &amp; \text{if $a(w)$ is true} \\ \beta  E_{w'}\left[ \tilde{V}^U_a(w^{\prime}) \right]  &amp; \text{if $a(w)$ is false} \end{cases}\]</span> <span class="math display">\[V_a^E(w) \leftarrow U(w) +  (1-\lambda) \beta \tilde{V}^E_a(w) +  \lambda \beta E_{w'}\left[\tilde{V}^U_a(w^{\prime}) \right] \]</span></li>
<li>Note the absence of the max function:
<ul>
<li>we don’t reoptimize</li>
<li>we we keep the same policy all along</li>
</ul></li>
</ul>
</section>
<section id="policy-iteration" class="slide level2">
<h2>Policy iteration</h2>
<ul>
<li>start with policy <span class="math inline">\(a(w)\)</span></li>
<li>evaluate the value of this policy <span class="math inline">\(V^E_a, V^U_a\)</span></li>
<li>compute the optimal policy <span class="math inline">\(a(w)\)</span> in the Bellman iteration
<ul>
<li>here: <span class="math inline">\(a(w) = \arg\max_{a(w)} \begin{cases} \beta \tilde{V}^E(w)\\ \beta  E_{a'}\left[ \tilde{V}^U(a^{\prime}) \right] \end{cases}\)</span></li>
</ul></li>
<li>iterate until <span class="math inline">\(a(w)\)</span> converges</li>
</ul>

<div class="quarto-auto-generated-content">
<p><img src="logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>